
--- FILE: .\App.py ---
import sys
import os
from typing import Optional, List

from PyQt5.QtWidgets import QApplication, QMessageBox, QProgressDialog
from PyQt5.QtCore import Qt

# Import Core Logic
from core import BaseProcessor
from processors import PoreExtractionProcessor, PoreToSphereProcessor
from data import ScientificDataManager
from exporters import VTKExporter

# Import View and UI Components
from gui import MainWindow, StructureProcessingPanel, StatisticsPanel
from gui.panels import TimeSeriesControlPanel, TrackingAnalysisPanel

# Import Handlers
from gui.handlers.timeseries_handler import TimeseriesHandler
from gui.handlers.workflow_handler import WorkflowHandler


class AppController:
    """
    Application Controller (The 'C' in MVC).
    Orchestrates interaction between Data, Logic, and UI.
    Delegates specific business logic to Handler classes.
    """

    def __init__(self):
        self.app = QApplication(sys.argv)

        # 1. Initialize View
        self.visualizer = MainWindow()

        # 2. Initialize Logic
        self.data_manager = ScientificDataManager()
        
        # Connect Visualizer to DataManager for centralized data flow
        self.visualizer.set_data_manager(self.data_manager)

        # 3. Setup Processing UI
        self._setup_workflow_ui()
        
        # 4. Initialize Handlers
        self.workflow_handler = WorkflowHandler(self)
        self.timeseries_handler = TimeseriesHandler(self)
        
        # 5. Connect Handler Signals
        self._connect_signals()


    def _setup_workflow_ui(self):
        self.panel = StructureProcessingPanel()

        # Add to left side (controls)
        self.visualizer.add_custom_panel(self.panel, index=1, side='left')
        
        # Add Time Series Control to left side
        from gui.panels import TimeSeriesControlPanel, TrackingAnalysisPanel
        self.timeseries_control = TimeSeriesControlPanel()
        self.visualizer.add_custom_panel(self.timeseries_control, index=2, side='left')

        # Add Statistics Panel to right side (info)
        self.stats_panel = StatisticsPanel()
        self.visualizer.add_custom_panel(self.stats_panel, index=1, side='right')
        
        # Add Tracking Analysis Panel to right side
        self.tracking_analysis = TrackingAnalysisPanel()
        self.visualizer.add_custom_panel(self.tracking_analysis, index=2, side='right')

        # GPU Control

        from core.gpu_backend import get_gpu_backend
        self.panel.gpu_toggled.connect(lambda checked: get_gpu_backend().set_enabled(checked))
        
        self.panel.export_clicked.connect(self._export_vtk_dialog)
        self.panel.reset_clicked.connect(self._reset_to_original)


    def _connect_signals(self):
        """Connect UI signals to handlers."""
        # Workflow Signals
        self.panel.load_clicked.connect(self.workflow_handler.load_dicom_dialog)
        self.panel.fast_load_clicked.connect(self.workflow_handler.fast_load_dicom_dialog)
        self.panel.dummy_clicked.connect(self.workflow_handler.load_dummy_data)
        
        self.panel.extract_pores_clicked.connect(self._process_pores)
        self.panel.pnm_clicked.connect(self._process_spheres)
        self.panel.auto_threshold_clicked.connect(self.workflow_handler.auto_detect_threshold)
        
        # 4D CT Signals

        self.timeseries_control.timepoint_changed.connect(self.timeseries_handler.set_timepoint)



    # ==========================================
    # Helper Components (Shared by Handlers)
    # ==========================================

    def _create_progress_dialog(self, title: str) -> QProgressDialog:
        """Create a standard progress dialog."""
        progress = QProgressDialog(title, "Cancel", 0, 100, self.visualizer)
        progress.setWindowModality(Qt.WindowModal)
        progress.setMinimumDuration(0)
        progress.setValue(0)
        return progress

    def _make_progress_callback(self, progress: QProgressDialog):
        """Create a progress callback that updates dialog and checks cancellation."""
        def callback(percent, message):
            progress.setValue(percent)
            progress.setLabelText(message)
            self.app.processEvents()
            if progress.wasCanceled():
                raise InterruptedError("Operation cancelled by user.")
        return callback
    
    def _show_msg(self, title: str, msg: str):
        """Show information message box."""
        QMessageBox.information(self.visualizer, title, msg)

    def _show_err(self, title: str, e: Exception):
        """Show error message box."""
        QMessageBox.critical(self.visualizer, title, f"{title}: {str(e)}")
        print(f"Error: {e}")

    # ==========================================
    # Processing Logic (Delegated to Handlers)
    # ==========================================

    def _process_pores(self):
        """Extract pores from current data."""
        processor = PoreExtractionProcessor()
        
        def on_complete(segmented):
            self.data_manager.set_segmented_data(segmented)
            self.visualizer.set_data(segmented)
            meta = segmented.metadata
            msg = (
                f"Quantitative Analysis Results:\n\n"
                f"鈥?Porosity: {meta.get('Porosity', 'N/A')}\n"
                f"鈥?Pore Count: {meta.get('PoreCount', 0)}\n"
            )
            self._show_msg("Extraction Complete", msg)

        self.workflow_handler.run_processor_async(processor, on_complete)

    def _process_spheres(self):
        """Generate Pore Network Model from current data."""
        # Automate 4D tracking if series is loaded
        if self.timeseries_handler.has_volumes:
            self.timeseries_handler.track_pores()
            return
            
        processor = PoreToSphereProcessor()
        
        def on_complete(pnm_data):
            self.data_manager.set_pnm_data(pnm_data)
            self.visualizer.set_data(pnm_data)
            self.stats_panel.update_statistics(pnm_data.metadata)
            
            counts = pnm_data.metadata
            msg = (
                f"Model Generated Successfully\n\n"
                f"鈥?Nodes (Pores): {counts.get('PoreCount')}\n"
                f"鈥?Throats: {counts.get('ConnectionCount')}\n"
                f"鈥?Largest Pore: {counts.get('LargestPoreRatio', 'N/A')}\n"
            )
            self._show_msg("Model Generated", msg)

        self.workflow_handler.run_processor_async(processor, on_complete)


    def _reset_to_original(self):
        """Reset to original raw data."""
        if not self.data_manager.has_raw():
            return
        self.visualizer.set_data(self.data_manager.raw_ct_data)
        self.visualizer.update_status("Reset to raw data.")

    def _export_vtk_dialog(self):
        """Export current data to VTK format."""
        data_to_save = self.visualizer.data
        if not data_to_save:
            QMessageBox.warning(self.visualizer, "Export Error", "No data loaded to export.")
            return

        default_name = "output.vtk"
        file_filter = "VTK Files (*.vtk *.vtp *.vti)"

        if data_to_save.has_mesh:
            default_name = "pnm_model.vtp"
        elif data_to_save.raw_data is not None:
            default_name = "volume_data.vti"

        path, _ = QFileDialog.getSaveFileName(
            self.visualizer, "Export to VTK", default_name, file_filter
        )

        if not path:
            return

        try:
            self.visualizer.update_status(f"Exporting to {path}...")
            self.app.processEvents()
            VTKExporter.export(data_to_save, path)
            self.visualizer.update_status("Export successful.")
            self._show_msg("Export Successful", f"File saved to:\n{path}")
        except Exception as e:
            self._show_err("Export Failed", e)
            self.visualizer.update_status("Export failed.")

    def run(self, source_path: Optional[str] = None):
        """Run the application."""
        if source_path and os.path.exists(source_path):
            self.workflow_handler._load_data(source_path)
        self.visualizer.show()
        sys.exit(self.app.exec_())


if __name__ == "__main__":
    app = AppController()
    app.run()


--- FILE: .\cli.py ---
"""
Headless CLI entry point for the 4-DCT Porous Media Analysis System.

Runs the full pipeline (load 鈫?threshold 鈫?segment 鈫?PNM 鈫?export) without
any display server (X11/Wayland), PyQt5 widgets, or VTK render windows.

Usage
-----
YAML/JSON config mode (recommended for automation / HPC):
    python cli.py --config path/to/config.yaml
    python cli.py --config path/to/config.json

Inline flag mode (quick one-off runs):
    python cli.py --input  /data/scan --loader dicom
                  --output /results
                  --threshold 300

Config schema  (YAML example)
------------------------------
    input_path:      /data/DICOM_series/
    loader_type:     dicom           # dicom | dummy
    output_dir:      /results/run01/
    export_formats:  [vtk, tiff]     # any of: vtk, tiff, npy
    threshold:       300.0
    auto_threshold:  false
    min_pore_size:   100
    min_throat_dist: 3
    chunk_shape:     [128, 128, 128]
    halo_voxels:     8

Docker / headless invocation:
------------------------------
    docker run --rm -v /data:/data -v /results:/results \
        porous:latest python cli.py --config /data/config.yaml
"""

from __future__ import annotations

import argparse
import os
import sys
import time
from pathlib import Path
from typing import Callable, Optional

# ---------------------------------------------------------------------------
# Guard: ensure we never accidentally import PyQt5 in headless mode
# ---------------------------------------------------------------------------
if "PyQt5" in sys.modules:
    print(
        "[CLI] WARNING: PyQt5 was already imported before cli.py started.  "
        "Ensure cli.py is the process entry-point, not a sub-import of App.py."
    )


# ---------------------------------------------------------------------------
# Lazy / conditional VTK offscreen setup  (must happen before pyvista import)
# ---------------------------------------------------------------------------
def _configure_headless_vtk() -> None:
    """Force VTK / PyVista into offscreen mode when no display is available."""
    display = os.environ.get("DISPLAY") or os.environ.get("WAYLAND_DISPLAY")
    if not display:
        os.environ.setdefault("PYVISTA_OFF_SCREEN", "true")
        os.environ.setdefault("VTK_DEFAULT_RENDER_WINDOW_OFFSCREEN", "1")

_configure_headless_vtk()


# ---------------------------------------------------------------------------
# Core imports (no PyQt5)
# ---------------------------------------------------------------------------
from core.dto import VolumeProcessDTO, RenderParamsDTO
from core.chunker import SimpleDAGExecutor, DAGNode


# ---------------------------------------------------------------------------
# Progress helper
# ---------------------------------------------------------------------------

class _CliProgress:
    def __init__(self, label: str = "") -> None:
        self.label = label

    def __call__(self, percent: int, msg: str) -> None:
        bar_len = 30
        filled  = int(bar_len * percent / 100)
        bar     = "鈻? * filled + "鈻? * (bar_len - filled)
        print(f"\r  [{bar}] {percent:3d}%  {msg:<40}", end="", flush=True)
        if percent >= 100:
            print()


# ---------------------------------------------------------------------------
# Pipeline stages
# ---------------------------------------------------------------------------

def _stage_load(dto: VolumeProcessDTO, progress: Callable) -> "VolumeData":
    """Load raw data without touching the GUI."""
    from core import VolumeData

    progress(0, f"Loading '{dto.input_path}' via {dto.loader_type}...")
    t0 = time.perf_counter()

    if dto.loader_type == "dummy":
        from loaders.dummy import DummyLoader
        loader = DummyLoader()
        data   = loader.load(dto.input_path, callback=progress)
    elif dto.loader_type == "dicom":
        from loaders.dicom import DicomLoader
        loader = DicomLoader()
        data   = loader.load(dto.input_path, callback=progress)
    else:
        raise ValueError(f"Unknown loader_type: {dto.loader_type!r}.  Choose 'dicom' or 'dummy'.")

    elapsed = time.perf_counter() - t0
    shape   = data.raw_data.shape if data.raw_data is not None else "mesh"
    print(f"  Loaded in {elapsed:.2f}s  shape={shape}")
    return data


def _stage_threshold(
    data:     "VolumeData",
    dto:      VolumeProcessDTO,
    progress: Callable,
) -> float:
    """Compute optimal threshold (auto or use DTO value)."""
    if dto.auto_threshold:
        progress(0, "Auto-detecting threshold...")
        from processors.utils import threshold_otsu_gpu
        import numpy as np
        threshold = float(threshold_otsu_gpu(data.raw_data))
        print(f"  Auto-threshold (Otsu): {threshold:.1f}")
    else:
        threshold = dto.threshold
        print(f"  Using fixed threshold: {threshold:.1f}")
    return threshold


def _stage_segment(
    data:      "VolumeData",
    threshold: float,
    dto:       VolumeProcessDTO,
    progress:  Callable,
) -> "VolumeData":
    """Binary segmentation 鈫?void/solid."""
    progress(0, "Segmenting...")
    t0 = time.perf_counter()

    from processors import PoreExtractionProcessor
    processor = PoreExtractionProcessor()
    segmented = processor.process(
        data,
        callback=progress,
        threshold=threshold,
        min_pore_size=dto.min_pore_size,
    )
    print(f"  Segmentation done in {time.perf_counter() - t0:.2f}s")
    return segmented


def _stage_pnm(
    segmented: "VolumeData",
    dto:       VolumeProcessDTO,
    progress:  Callable,
) -> "VolumeData":
    """Build Pore Network Model from segmented volume."""
    progress(0, "Building PNM...")
    t0 = time.perf_counter()

    from processors import PoreToSphereProcessor
    processor = PoreToSphereProcessor()
    pnm = processor.process(
        segmented,
        callback=progress,
        min_throat_dist=dto.min_throat_dist,
    )
    print(f"  PNM done in {time.perf_counter() - t0:.2f}s")
    return pnm


def _stage_export(
    data_dict: dict,
    dto:       VolumeProcessDTO,
    progress:  Callable,
) -> list[str]:
    """Export results in the requested formats."""
    out_dir = Path(dto.output_dir) if dto.output_dir else Path.cwd() / "cli_output"
    out_dir.mkdir(parents=True, exist_ok=True)

    from exporters import VTKExporter
    exporter  = VTKExporter()
    exported  = []

    segmented = data_dict.get("segment")
    pnm       = data_dict.get("pnm")

    for fmt in dto.export_formats:
        fmt = fmt.lower()
        if fmt == "vtk":
            if pnm is not None:
                path = str(out_dir / "pnm_mesh.vtp")
                exporter.export(pnm, path, callback=progress)
                exported.append(path)
            if segmented is not None and segmented.raw_data is not None:
                path = str(out_dir / "segmented.vti")
                exporter.export(segmented, path, callback=progress)
                exported.append(path)
        elif fmt == "tiff":
            import numpy as np
            try:
                from tifffile import imwrite
            except ImportError:
                print("  [WARN] tifffile not installed; skipping TIFF export.")
                continue
            if segmented is not None and segmented.raw_data is not None:
                path = str(out_dir / "segmented.tif")
                imwrite(path, segmented.raw_data.astype("uint8"))
                exported.append(path)
        elif fmt == "npy":
            import numpy as np
            if segmented is not None and segmented.raw_data is not None:
                path = str(out_dir / "segmented.npy")
                np.save(path, segmented.raw_data)
                exported.append(path)
        else:
            print(f"  [WARN] Unknown export format '{fmt}' 鈥?skipped.")

    return exported


# ---------------------------------------------------------------------------
# Headless batch runner
# ---------------------------------------------------------------------------

def run_batch(dto: VolumeProcessDTO) -> dict:
    """
    Execute the full pipeline headlessly using a SimpleDAGExecutor.

    Returns a dict keyed by stage name containing each stage's output.
    """
    progress = _CliProgress()

    # ---- Build DAG --------------------------------------------------------
    dag = SimpleDAGExecutor()

    dag.add(DAGNode(
        name       = "load",
        fn         = lambda _: _stage_load(dto, _CliProgress("load")),
        depends_on = (),
    ))

    dag.add(DAGNode(
        name       = "threshold",
        fn         = lambda d: _stage_threshold(d["load"], dto, _CliProgress("threshold")),
        depends_on = ("load",),
    ))

    dag.add(DAGNode(
        name       = "segment",
        fn         = lambda d: _stage_segment(d["load"], d["threshold"], dto, _CliProgress("segment")),
        depends_on = ("load", "threshold"),
    ))

    dag.add(DAGNode(
        name       = "pnm",
        fn         = lambda d: _stage_pnm(d["segment"], dto, _CliProgress("pnm")),
        depends_on = ("segment",),
    ))

    dag.add(DAGNode(
        name       = "export",
        fn         = lambda d: _stage_export(d, dto, _CliProgress("export")),
        depends_on = ("segment", "pnm"),
    ))

    # ---- Execute ----------------------------------------------------------
    t_start  = time.perf_counter()
    results  = dag.run(progress=lambda pct, msg: print(f"  [DAG {pct:3d}%] {msg}"))
    elapsed  = time.perf_counter() - t_start

    print(f"\nPipeline complete in {elapsed:.2f}s")
    exported = results.get("export", [])
    if exported:
        print("Exported files:")
        for p in exported:
            print(f"  {p}")

    return results


# ---------------------------------------------------------------------------
# CLI argument parser
# ---------------------------------------------------------------------------

def _build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog        = "python cli.py",
        description = "Headless 4-DCT porous media batch processor",
        formatter_class = argparse.ArgumentDefaultsHelpFormatter,
    )

    # Config-file mode
    p.add_argument(
        "--config", metavar="FILE",
        help="Path to YAML or JSON config file.  All other flags are overridden by the file.",
    )

    # Inline flag mode
    p.add_argument("--input",      metavar="PATH",   default="",     help="Input path (DICOM dir or file).")
    p.add_argument("--loader",     metavar="TYPE",   default="dicom",help="Loader type: dicom | dummy.")
    p.add_argument("--output",     metavar="DIR",    default=None,   help="Output directory.")
    p.add_argument("--threshold",  metavar="VALUE",  type=float, default=300.0, help="Segmentation threshold (HU).")
    p.add_argument("--auto-threshold", action="store_true", help="Auto-detect threshold via Otsu method.")
    p.add_argument("--formats",    metavar="FMT",    nargs="+", default=["vtk"],
                   help="Export formats: vtk tiff npy (space-separated).")
    p.add_argument("--chunk",      metavar="N",      type=int, default=128,
                   help="Chunk size (single int 鈫?NxNxN).")
    p.add_argument("--halo",       metavar="N",      type=int, default=8,
                   help="Ghost-cell halo voxels for chunked EDT.")
    p.add_argument("--dry-run",    action="store_true",
                   help="Parse config and print resolved DTO without running.")

    return p


def main() -> int:
    parser = _build_parser()
    args   = parser.parse_args()

    # ---- Resolve DTO -------------------------------------------------------
    if args.config:
        cfg_path = args.config
        if cfg_path.endswith(".yaml") or cfg_path.endswith(".yml"):
            dto = VolumeProcessDTO.from_yaml(cfg_path)
        elif cfg_path.endswith(".json"):
            dto = VolumeProcessDTO.from_json(cfg_path)
        else:
            # Try YAML first, then JSON
            try:
                dto = VolumeProcessDTO.from_yaml(cfg_path)
            except Exception:
                dto = VolumeProcessDTO.from_json(cfg_path)
    else:
        if not args.input:
            parser.error("Provide --config FILE or --input PATH")
        c = int(args.chunk)
        dto = VolumeProcessDTO(
            input_path      = args.input,
            loader_type     = args.loader,
            output_dir      = args.output,
            threshold       = args.threshold,
            auto_threshold  = args.auto_threshold,
            export_formats  = tuple(args.formats),
            chunk_shape     = (c, c, c),
            halo_voxels     = args.halo,
        )

    # ---- Dry-run -----------------------------------------------------------
    if args.dry_run:
        import json
        print("Resolved VolumeProcessDTO:")
        print(json.dumps(dto.to_dict(), indent=2))
        return 0

    # ---- Execute -----------------------------------------------------------
    print("=" * 60)
    print("4-DCT Porous Media 鈥?Headless Batch Processor")
    print("=" * 60)
    try:
        run_batch(dto)
    except KeyboardInterrupt:
        print("\nAborted by user.")
        return 1
    except Exception as exc:
        import traceback
        print(f"\nPipeline failed: {type(exc).__name__}: {exc}")
        traceback.print_exc()
        return 2

    return 0


if __name__ == "__main__":
    sys.exit(main())


--- FILE: .\config.py ---
"""
Configuration constants for the Porous Media Analysis Suite.
All thresholds and configurable parameters are centralized here.
"""

# ==========================================
# Window Settings
# ==========================================
WINDOW_TITLE = "Porous Media Analysis Suite"
WINDOW_SIZE = (1400, 900)
WINDOW_POSITION = (100, 100)

# Splitter sizes (left:center:right)
SPLITTER_SIZES = [350, 900, 350]

# ==========================================
# Loader Settings
# ==========================================

# SmartDicomLoader auto-strategy thresholds (number of files)
LOADER_THRESHOLD_FAST = 200       # >200 files 鈫?use FastDicomLoader
LOADER_THRESHOLD_MMAP = 500       # >500 files 鈫?use MemoryMappedDicomLoader  
LOADER_THRESHOLD_CHUNKED = 1000   # >1000 files 鈫?use ChunkedDicomLoader

# Parallel reading settings
LOADER_MAX_WORKERS = 4            # Number of parallel threads for file reading
LOADER_DOWNSAMPLE_STEP = 2        # Downsample step for FastDicomLoader

# ChunkedDicomLoader settings
LOADER_CHUNK_SIZE = 64            # Number of slices per chunk

# ==========================================
# Rendering Settings
# ==========================================

# Memory thresholds for auto-downsampling (number of voxels)
RENDER_MAX_VOXELS_VOLUME = 100_000_000    # 100M voxels for volume rendering
RENDER_MAX_VOXELS_ISO = 150_000_000       # 150M voxels for isosurface
RENDER_MAX_MEMORY_MB = 500                # Max memory budget (MB) for render grid

# LOD (Level of Detail) settings
LOD_LEVELS = 3                    # Number of LOD levels to create
LOD_FAR_THRESHOLD = 1000          # Camera distance for lowest LOD
LOD_MID_THRESHOLD = 500           # Camera distance for medium LOD

# ==========================================
# Processing Settings (Pore Extraction)
# ==========================================

# Chunked processing threshold (bytes)
PROCESS_CHUNK_THRESHOLD = 500 * 1024 * 1024  # 500 MB - use chunked above this
PROCESS_CHUNK_SIZE = 32           # Slices per chunk for pore processing

# Otsu sampling threshold (number of voxels)
PROCESS_OTSU_SAMPLE_THRESHOLD = 100_000_000  # Sample data above this size

# ==========================================
# Colormaps
# ==========================================
DEFAULT_COLORMAPS = [
    'bone', 'viridis', 'plasma', 'gray', 
    'coolwarm', 'jet', 'magma'
]

DEFAULT_COLORMAP = 'bone'

# ==========================================
# Solid Colors
# ==========================================
SOLID_COLORS = [
    'ivory', 'red', 'gold', 'lightgray', 
    'mediumseagreen', 'dodgerblue', 'wheat'
]

DEFAULT_SOLID_COLOR = 'ivory'

# ==========================================
# Volume Rendering
# ==========================================
OPACITY_PRESETS = [
    'sigmoid', 'sigmoid_10', 'linear', 
    'linear_r', 'geom', 'geom_r'
]

DEFAULT_OPACITY = 'sigmoid'

# ==========================================
# Isosurface Rendering
# ==========================================
RENDER_STYLES = ['Surface', 'Wireframe', 'Wireframe + Surface']
COLORING_MODES = ['Solid Color', 'Depth (Z-Axis)', 'Radial (Center Dist)']

DEFAULT_THRESHOLD = 300
DEFAULT_THRESHOLD_PORE = -300

# ==========================================
# PNM Processing
# ==========================================
MIN_PEAK_DISTANCE = 6

# ==========================================
# Timer Intervals (ms)
# ==========================================
DEBOUNCE_INTERVAL = 100
CLIP_UPDATE_INTERVAL = 200

# ==========================================
# GPU Acceleration Settings
# ==========================================
GPU_ENABLED = True    # Set False to disable GPU acceleration
GPU_MIN_SIZE_MB = 10  # Minimum data size (MB) to use GPU (smaller uses CPU)

# ==========================================
# 4D CT Tracking Settings
# ==========================================
TRACKING_IOU_THRESHOLD = 0.1           # Min IoU to consider pores matched
TRACKING_COMPRESSION_THRESHOLD = 0.01  # Volume ratio below which pore is "compressed"
TRACKING_ANIMATION_FPS = 5             # Animation playback speed

# Tracking algorithm options
TRACKING_USE_GPU = True                # Use GPU acceleration for tracking (requires CuPy)
TRACKING_USE_BATCH = True              # Use batch IoU calculation (faster for many pores)
TRACKING_USE_HUNGARIAN = False         # Use Hungarian algorithm for global optimal matching
TRACKING_BATCH_SIZE = 1000             # Max pores to process in one batch (memory vs speed)
TRACKING_GPU_MIN_PORES = 100           # Min pores to use GPU (smaller uses CPU)

# TGGA matching options
TRACKING_MATCH_MODE = "temporal_global"      # temporal_global | legacy_greedy | global_iou_legacy
TRACKING_ASSIGN_SOLVER = "lapjv"             # lapjv | scipy
TRACKING_COST_WEIGHTS = (0.45, 0.30, 0.20, 0.05)  # IoU, center distance, volume ratio, dice
TRACKING_VOLUME_COST_MODE = "symdiff"        # symdiff | gaussian
TRACKING_VOLUME_COST_GAUSSIAN_SIGMA = 25.0   # Used only when TRACKING_VOLUME_COST_MODE="gaussian"
TRACKING_MAX_MISSES = 2
TRACKING_GATE_CENTER_RADIUS_FACTOR = 2.5
TRACKING_GATE_VOLUME_RATIO_MIN = 1e-4
TRACKING_GATE_VOLUME_RATIO_MAX = 5.0
TRACKING_GATE_IOU_MIN = 0.02
TRACKING_GATE_VOLUME_RATIO_MIN_FLOOR = 1e-4

# Macro pre-registration (DVC-style) before local gating
TRACKING_ENABLE_MACRO_REGISTRATION = True
TRACKING_MACRO_REG_SMOOTHING_SIGMA = 1.5
TRACKING_MACRO_REG_UPSAMPLE_FACTOR = 4
TRACKING_MACRO_REG_USE_GPU = True
TRACKING_MACRO_REG_GPU_MIN_MB = 8.0

# Constant-acceleration Kalman predictor
TRACKING_KALMAN_PROCESS_NOISE = 0.05
TRACKING_KALMAN_MEASUREMENT_NOISE = 1.0
TRACKING_KALMAN_BRAKE_VELOCITY_DECAY = 0.75
TRACKING_KALMAN_BRAKE_ACCEL_DECAY = 0.35
TRACKING_KALMAN_FREEZE_AFTER_MISSES = 3

# Pore closure event decision
TRACKING_CLOSURE_VOLUME_RATIO_THRESHOLD = 0.02
TRACKING_CLOSURE_MIN_VOLUME_VOXELS = 2.0
TRACKING_CLOSURE_STRAIN_THRESHOLD = 0.6

# Position smoothing to reduce center jitter across timepoints (0 = lock to previous, 1 = follow current)
TRACKING_CENTER_SMOOTHING = 0.35

# Topology preservation
TRACKING_PRESERVE_TOPOLOGY = True      # Keep reference connectivity (t=0) for all timepoints



--- FILE: .\test_compression_rendering.py ---
"""
Quick test to verify compression ratio rendering is connected.
Run this after loading 4DCT data and tracking PNM.
"""

import numpy as np
import pyvista as pv

def test_compression_mesh():
    """Test that compression scalars are properly added to mesh."""
    
    # Simulate mesh data
    centers = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]])
    radii = np.array([0.5, 0.6, 0.4])
    ids = np.array([1, 2, 3])
    compression_ratios = np.array([0.3, 0.8, 1.0])  # Compressed, normal, expanded
    highlight_id = 2
    
    # Create point cloud
    pore_cloud = pv.PolyData(centers)
    pore_cloud["radius"] = radii
    pore_cloud["ID"] = ids
    
    # Add compression and highlight
    highlight_flags = np.array([1 if pid == highlight_id else 0 for pid in ids], dtype=np.int32)
    pore_cloud["CompressionRatio"] = compression_ratios
    pore_cloud["Highlighted"] = highlight_flags
    
    # Glyph to spheres
    sphere_glyph = pv.Sphere(theta_resolution=10, phi_resolution=10)
    pores_mesh = pore_cloud.glyph(scale="radius", geom=sphere_glyph)
    
    n_pts_per_sphere = sphere_glyph.n_points
    
    # Repeat scalars for each sphere vertex
    compression_scalars = np.repeat(compression_ratios, n_pts_per_sphere)
    highlight_scalars = np.repeat(highlight_flags, n_pts_per_sphere)
    
    pores_mesh["CompressionRatio"] = compression_scalars
    pores_mesh["Highlighted"] = highlight_scalars
    pores_mesh["PoreRadius"] = np.repeat(radii, n_pts_per_sphere)
    
    # Verify
    print(f"鉁?Mesh has {pores_mesh.n_points} points")
    print(f"鉁?Available scalars: {list(pores_mesh.point_data.keys())}")
    print(f"鉁?CompressionRatio range: [{compression_scalars.min():.2f}, {compression_scalars.max():.2f}]")
    print(f"鉁?Highlighted pores: {highlight_scalars.sum()} points")
    
    # Test rendering logic
    highlighted = pores_mesh.point_data["Highlighted"]
    has_highlighted_pores = highlighted.max() > 0
    
    if has_highlighted_pores:
        print(f"鉁?Rendering mode: Compression + Highlight")
        compression = pores_mesh.point_data["CompressionRatio"]
        display_scalar = compression.copy()
        display_scalar[highlighted > 0] = 2.0
        print(f"  Display scalar range: [{display_scalar.min():.2f}, {display_scalar.max():.2f}]")
    else:
        print(f"鉁?Rendering mode: Compression only")
    
    # Visual test
    plotter = pv.Plotter()
    
    if has_highlighted_pores and "CompressionRatio" in pores_mesh.point_data:
        compression = pores_mesh.point_data["CompressionRatio"]
        display_scalar = compression.copy()
        display_scalar[highlighted > 0] = 2.0
        
        plotter.add_mesh(
            pores_mesh,
            scalars=display_scalar,
            cmap=['blue', 'cyan', 'green', 'yellow', 'red', 'gold'],
            clim=[0.0, 2.0],
            show_scalar_bar=True,
            scalar_bar_args={'title': 'Compression (blue=low, red=high, gold=selected)'}
        )
    else:
        plotter.add_mesh(
            pores_mesh,
            scalars="CompressionRatio",
            cmap=['blue', 'cyan', 'green', 'yellow', 'red'],
            clim=[0.0, 1.0],
            show_scalar_bar=True,
            scalar_bar_args={'title': 'Compression Ratio'}
        )
    
    print("\n鉁?All checks passed! Opening visualization...")
    print("  Expected: Blue sphere (compressed), red/yellow sphere (normal), gold sphere (highlighted)")
    plotter.show()

if __name__ == "__main__":
    test_compression_mesh()


--- FILE: .\core\base.py ---
"""
Core data structures and abstract base classes.
"""

import numpy as np
import pyvista as pv
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from typing import Tuple, Dict, Any, Optional, Callable, runtime_checkable, Protocol


@dataclass
class VolumeData:
    """
    Unified Data Transfer Object (DTO) for volumetric and mesh data.
    
    Attributes:
        raw_data (Optional[np.ndarray]): 3D Matrix (Z, Y, X) for voxel data.
        mesh (Optional[pv.PolyData]): 3D Mesh for PNM or Surface data.
        spacing (Tuple[float, float, float]): Voxel spacing (x, y, z) in mm.
        origin (Tuple[float, float, float]): Origin coordinates (x, y, z) in mm.
        metadata (Dict[str, Any]): Arbitrary metadata (SampleID, etc.).
    """
    raw_data: Optional[np.ndarray] = None
    mesh: Optional[pv.PolyData] = None
    spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0)
    origin: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def dimensions(self) -> Tuple[int, int, int]:
        """Returns the shape of the volume (Z, Y, X) if raw_data exists."""
        if self.raw_data is not None:
            return self.raw_data.shape
        return (0, 0, 0)

    @property
    def has_mesh(self) -> bool:
        """Check if mesh data is present."""
        return self.mesh is not None


class BaseLoader(ABC):
    """Abstract base class for data acquisition strategies."""

    @abstractmethod
    def load(self, source: str, callback: Optional[Callable[[int, str], None]] = None) -> VolumeData:
        """
        Load data from a source path.
        
        Args:
            source (str): Path to file or directory.
            callback: Optional progress callback (percent, message).
            
        Returns:
            VolumeData: Loaded data object.
        """
        pass


class BaseProcessor(ABC):
    """Abstract base class for volumetric processing algorithms."""

    @abstractmethod
    def process(self, data: VolumeData, callback: Optional[Callable[[int, str], None]] = None, **kwargs) -> VolumeData:
        """
        Process volume data.
        
        Args:
            data (VolumeData): Input data.
            callback (Optional[Callable]): Progress callback (percent, message).
            **kwargs: Algorithm specific parameters.
            
        Returns:
            VolumeData: Processed result.
        """
        pass


@runtime_checkable
class BaseVisualizer(Protocol):
    """
    Structural protocol for visualization controllers.

    Using ``typing.Protocol`` instead of ``abc.ABC`` eliminates the
    ``metaclass=_MainWindowMeta`` hack that was required to reconcile
    QMainWindow's meta-type with ABCMeta.  Any class that implements
    ``set_data`` and ``show`` satisfies this protocol automatically 鈥?
    no explicit inheritance required.
    """

    def set_data(self, data: "VolumeData", reset_camera: bool = False) -> None:  # type: ignore[override]
        """Set valid data to the visualizer."""
        ...

    def show(self) -> None:
        """Show the visualization window."""
        ...


--- FILE: .\core\chunker.py ---
"""
HPC-grade 3-D spatial chunker with ghost-cell (halo) support.

Design goals
------------
* Eliminate TLB-thrashing caused by operating on large numpy arrays that do
  not fit in LLC (Last-Level Cache) or physical RAM.
* Provide correct boundary handling for non-local algorithms (EDT, convolution,
  morphological operations) via configurable halo/ghost-cell overlap.
* Expose a simple DAG-ready generator interface that yields ``ChunkDescriptor``
  objects; callers compose processing stages as pure functions.

No PyQt5, no VTK, no CuPy imports 鈥?this module is completely headless.
"""

from __future__ import annotations

import math
from dataclasses import dataclass, field
from typing import (
    Callable, Generator, Iterable, Optional, Sequence, Tuple
)

import numpy as np


# ---------------------------------------------------------------------------
# Data structures
# ---------------------------------------------------------------------------

@dataclass
class ChunkDescriptor:
    """
    Describes a single spatial chunk and the surrounding halo region.

    Attributes
    ----------
    chunk_id : int
        Monotonically increasing identifier (useful for progress reporting).
    volume_shape : tuple(int, int, int)
        Full volume shape (Z, Y, X).
    core_slices : tuple of 3 slices
        Slices that select the *valid* (non-halo) output region within the
        extended (with-halo) array.
    extended_slices : tuple of 3 slices
        Slices that select the with-halo input region from the full volume.
    core_in_extended : tuple of 3 slices
        Slices that locate the core region *within* the extended array.
        Use these to strip the halo after processing.
    halo : int
        Halo width in voxels (same on all sides).
    """

    chunk_id:          int
    volume_shape:      Tuple[int, int, int]
    core_slices:       Tuple[slice, slice, slice]
    extended_slices:   Tuple[slice, slice, slice]
    core_in_extended:  Tuple[slice, slice, slice]
    halo:              int

    # ------------------------------------------------------------------
    def __repr__(self) -> str:
        cs = self.core_slices
        return (
            f"ChunkDescriptor(id={self.chunk_id}, "
            f"core=[{cs[0].start}:{cs[0].stop}, "
            f"{cs[1].start}:{cs[1].stop}, "
            f"{cs[2].start}:{cs[2].stop}], "
            f"halo={self.halo})"
        )

    @property
    def core_shape(self) -> Tuple[int, int, int]:
        return tuple(s.stop - s.start for s in self.core_slices)   # type: ignore[return-value]

    @property
    def extended_shape(self) -> Tuple[int, int, int]:
        return tuple(s.stop - s.start for s in self.extended_slices)  # type: ignore[return-value]


# ---------------------------------------------------------------------------
# SpatialChunker
# ---------------------------------------------------------------------------

class SpatialChunker:
    """
    Enumerate 3-D chunks with optional ghost-cell borders.

    Parameters
    ----------
    volume_shape : (D, H, W)
        Shape of the full volume.
    chunk_shape : (cd, ch, cw)
        Desired shape of each core chunk.  The final chunk in each dimension
        may be smaller.
    halo : int
        Number of ghost-cell voxels to add on every face of every chunk.
        Algorithms reading ``halo`` voxels beyond their core boundary will
        always receive valid data (edge voxels are clamped to the volume
        boundary 鈥?no zero-padding artefacts).
    """

    def __init__(
        self,
        volume_shape:  Tuple[int, int, int],
        chunk_shape:   Tuple[int, int, int] = (128, 128, 128),
        halo:          int                  = 0,
    ) -> None:
        assert len(volume_shape) == 3,  "volume_shape must be 3-D"
        assert len(chunk_shape)  == 3,  "chunk_shape must be 3-D"
        assert halo >= 0,               "halo must be non-negative"

        self.volume_shape = tuple(int(v) for v in volume_shape)
        self.chunk_shape  = tuple(int(c) for c in chunk_shape)
        self.halo         = int(halo)

        D, H, W   = self.volume_shape
        cd, ch, cw = self.chunk_shape

        self._starts_z = list(range(0, D, cd))
        self._starts_y = list(range(0, H, ch))
        self._starts_x = list(range(0, W, cw))

    # ------------------------------------------------------------------
    @property
    def num_chunks(self) -> int:
        return len(self._starts_z) * len(self._starts_y) * len(self._starts_x)

    # ------------------------------------------------------------------
    def __iter__(self) -> Generator[ChunkDescriptor, None, None]:
        """Yield ChunkDescriptor objects for every chunk in ZYX order."""
        D, H, W    = self.volume_shape
        cd, ch, cw = self.chunk_shape
        h          = self.halo
        chunk_id   = 0

        for z0 in self._starts_z:
            z1 = min(z0 + cd, D)
            for y0 in self._starts_y:
                y1 = min(y0 + ch, H)
                for x0 in self._starts_x:
                    x1 = min(x0 + cw, W)

                    # Core region in full-volume coordinates
                    core_slices = (slice(z0, z1), slice(y0, y1), slice(x0, x1))

                    # Extended region (with halo, clamped to array bounds)
                    ez0 = max(z0 - h, 0);  ez1 = min(z1 + h, D)
                    ey0 = max(y0 - h, 0);  ey1 = min(y1 + h, H)
                    ex0 = max(x0 - h, 0);  ex1 = min(x1 + h, W)
                    extended_slices = (slice(ez0, ez1), slice(ey0, ey1), slice(ex0, ex1))

                    # Where is the core inside the extended array?
                    cz0_in = z0 - ez0;  cz1_in = cz0_in + (z1 - z0)
                    cy0_in = y0 - ey0;  cy1_in = cy0_in + (y1 - y0)
                    cx0_in = x0 - ex0;  cx1_in = cx0_in + (x1 - x0)
                    core_in_extended = (
                        slice(cz0_in, cz1_in),
                        slice(cy0_in, cy1_in),
                        slice(cx0_in, cx1_in),
                    )

                    yield ChunkDescriptor(
                        chunk_id         = chunk_id,
                        volume_shape     = self.volume_shape,
                        core_slices      = core_slices,
                        extended_slices  = extended_slices,
                        core_in_extended = core_in_extended,
                        halo             = h,
                    )
                    chunk_id += 1

    # ------------------------------------------------------------------
    def process_inplace(
        self,
        volume:   np.ndarray,
        fn:       Callable[[np.ndarray, ChunkDescriptor], np.ndarray],
        progress: Optional[Callable[[int, str], None]] = None,
    ) -> None:
        """
        Apply *fn* to every chunk in-place, stripping halos before write-back.

        Parameters
        ----------
        volume : ndarray, shape == self.volume_shape
            The array to modify in-place.
        fn : callable(extended_chunk, desc) -> processed_chunk
            Must accept the with-halo sub-array and the descriptor, and return
            an array shaped like the *extended* input.  The core region will
            be extracted automatically using ``desc.core_in_extended``.
        progress : optional callable(percent: int, message: str)
            Called after each chunk with a completion percentage.
        """
        total = self.num_chunks
        for desc in self:
            ext_chunk = volume[desc.extended_slices].copy()
            result    = fn(ext_chunk, desc)
            # Write back only the core (halo stripped)
            volume[desc.core_slices] = result[desc.core_in_extended]

            if progress:
                pct = int(100 * (desc.chunk_id + 1) / total)
                progress(pct, f"Chunk {desc.chunk_id + 1}/{total}")

    # ------------------------------------------------------------------
    def map_reduce(
        self,
        volume:   np.ndarray,
        map_fn:   Callable[[np.ndarray, ChunkDescriptor], Any],
        reduce_fn: Callable[[Iterable], Any],
        progress: Optional[Callable[[int, str], None]] = None,
    ):
        """
        Map *map_fn* over chunks and reduce with *reduce_fn*.

        Useful for aggregation tasks (histogram, statistics) without
        loading the entire volume at once.
        """
        import builtins
        from typing import Any

        total    = self.num_chunks
        partials = []
        for desc in self:
            ext_chunk = volume[desc.extended_slices]
            partials.append(map_fn(ext_chunk, desc))
            if progress:
                pct = int(100 * (desc.chunk_id + 1) / total)
                progress(pct, f"Chunk {desc.chunk_id + 1}/{total}")
        return reduce_fn(partials)


# ---------------------------------------------------------------------------
# Convenience: EDT with ghost cells
# ---------------------------------------------------------------------------

def edt_chunked(
    binary_mask:   np.ndarray,
    chunk_shape:   Tuple[int, int, int] = (128, 128, 128),
    halo:          int                  = 16,
    sampling:      Optional[Tuple[float, ...]] = None,
    progress:      Optional[Callable[[int, str], None]] = None,
) -> np.ndarray:
    """
    Memory-efficient 3-D Euclidean Distance Transform via ghost-cell chunking.

    A halo of ``halo`` voxels on every face ensures that EDT values computed
    at chunk boundaries are accurate to within ``halo`` voxels of the true
    global EDT.  For most medical-imaging scenarios halo=16 is sufficient.

    Parameters
    ----------
    binary_mask : bool / uint8 ndarray
        Input mask (True = foreground).
    chunk_shape : (D, H, W)
        Core chunk size.  Larger = faster (less overhead), more RAM.
    halo : int
        Ghost-cell width.  Should be 鈮?max expected pore radius.
    sampling : optional tuple of floats
        Voxel spacing (z, y, x) for anisotropic EDT.
    progress : optional progress callback

    Returns
    -------
    dist : float32 ndarray, same shape as binary_mask
    """
    import scipy.ndimage as ndi

    dist = np.zeros(binary_mask.shape, dtype=np.float32)
    chunker = SpatialChunker(binary_mask.shape, chunk_shape=chunk_shape, halo=halo)  # type: ignore[arg-type]

    def _edt_chunk(ext_chunk: np.ndarray, desc: ChunkDescriptor) -> np.ndarray:
        result = ndi.distance_transform_edt(ext_chunk, sampling=sampling).astype(np.float32)
        return result

    chunker.process_inplace(
        volume   = dist,
        fn       = lambda _, desc: _run_edt_on_mask(binary_mask, desc, sampling),
        progress = progress,
    )
    return dist


def _run_edt_on_mask(mask, desc: ChunkDescriptor, sampling) -> np.ndarray:
    """Internal helper used by edt_chunked."""
    import scipy.ndimage as ndi
    ext_chunk = mask[desc.extended_slices].copy()
    return ndi.distance_transform_edt(ext_chunk, sampling=sampling).astype(np.float32)


# ---------------------------------------------------------------------------
# Lightweight DAG executor  (no external dependencies)
# ---------------------------------------------------------------------------

@dataclass
class DAGNode:
    """A single step in a processing pipeline."""
    name:        str
    fn:          Callable[[Any], Any]
    depends_on:  Tuple[str, ...] = field(default_factory=tuple)


class SimpleDAGExecutor:
    """
    Topologically-sorted pipeline runner.

    Usage::

        dag = SimpleDAGExecutor()
        dag.add(DAGNode("load",     load_fn,    depends_on=()))
        dag.add(DAGNode("segment",  segment_fn, depends_on=("load",)))
        dag.add(DAGNode("pnm",      pnm_fn,     depends_on=("segment",)))
        results = dag.run(progress_callback)

    Each ``fn`` receives a dict of ``{node_name: result}`` for all nodes it
    depends on.  The return value is stored in the results dict under its own
    name and forwarded to dependent nodes.
    """

    def __init__(self) -> None:
        self._nodes: dict[str, DAGNode] = {}

    def add(self, node: DAGNode) -> "SimpleDAGExecutor":
        self._nodes[node.name] = node
        return self

    def _topo_sort(self) -> list[str]:
        visited:    set[str]  = set()
        order:      list[str] = []

        def dfs(name: str) -> None:
            if name in visited:
                return
            visited.add(name)
            for dep in self._nodes[name].depends_on:
                if dep not in self._nodes:
                    raise KeyError(f"DAG node '{name}' depends on unknown node '{dep}'")
                dfs(dep)
            order.append(name)

        for name in self._nodes:
            dfs(name)
        return order

    def run(
        self,
        progress: Optional[Callable[[int, str], None]] = None,
    ) -> dict:
        order    = self._topo_sort()
        results  = {}
        total    = len(order)
        for i, name in enumerate(order):
            node    = self._nodes[name]
            inputs  = {dep: results[dep] for dep in node.depends_on}
            if progress:
                progress(int(100 * i / total), f"Running: {name}")
            results[name] = node.fn(inputs)
        if progress:
            progress(100, "Pipeline complete")
        return results


--- FILE: .\core\dto.py ---
"""
Data Transfer Objects (DTOs) for the 4-DCT analysis pipeline.

Design rules
------------
* All DTOs are immutable (frozen=True).  Core logic never calls into the GUI;
  the GUI builds a new DTO and *pushes* it to the engine.
* No PyQt5 imports anywhere in this module.
* ``from_dict`` / ``from_panel`` factory methods keep serialisation in one place.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Tuple, Optional, Dict, Any, List


# ---------------------------------------------------------------------------
# Render parameters DTO
# ---------------------------------------------------------------------------

@dataclass(frozen=True)
class RenderParamsDTO:
    """
    Immutable snapshot of every rendering parameter.

    Replaces direct calls to ``params_panel.get_current_values()`` inside the
    render engine, making the engine headless-capable.
    """

    threshold:      int                     = 300
    coloring_mode:  str                     = "Scientific Colormap"
    colormap:       str                     = "viridis"
    solid_color:    str                     = "ivory"
    light_angle:    int                     = 45
    render_style:   str                     = "Surface"
    opacity:        str                     = "linear"
    slice_x:        int                     = 50
    slice_y:        int                     = 50
    slice_z:        int                     = 50
    clim:           Tuple[float, float]     = (0.0, 1000.0)

    # ------------------------------------------------------------------
    # Factories
    # ------------------------------------------------------------------

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "RenderParamsDTO":
        """Build from a plain dictionary (e.g. from params_panel.get_current_values())."""
        clim_raw = d.get("clim", [0.0, 1000.0])
        return RenderParamsDTO(
            threshold     = int(d.get("threshold",     300)),
            coloring_mode = str(d.get("coloring_mode", "Scientific Colormap")),
            colormap      = str(d.get("colormap",      "viridis")),
            solid_color   = str(d.get("solid_color",   "ivory")),
            light_angle   = int(d.get("light_angle",   45)),
            render_style  = str(d.get("render_style",  "Surface")),
            opacity       = str(d.get("opacity",       "linear")),
            slice_x       = int(d.get("slice_x",       50)),
            slice_y       = int(d.get("slice_y",       50)),
            slice_z       = int(d.get("slice_z",       50)),
            clim          = (float(clim_raw[0]), float(clim_raw[1])),
        )

    @staticmethod
    def from_panel(panel) -> "RenderParamsDTO":
        """
        Build from a live RenderingParametersPanel without importing PyQt.

        This is the *only* place where GUI state is read out; after this call
        the resulting DTO can be passed deep into the rendering stack or
        serialised to YAML without touching the UI thread again.
        """
        return RenderParamsDTO.from_dict(panel.get_current_values())

    def to_dict(self) -> Dict[str, Any]:
        """Serialise back to a plain dictionary (for YAML / JSON export)."""
        return {
            "threshold":     self.threshold,
            "coloring_mode": self.coloring_mode,
            "colormap":      self.colormap,
            "solid_color":   self.solid_color,
            "light_angle":   self.light_angle,
            "render_style":  self.render_style,
            "opacity":       self.opacity,
            "slice_x":       self.slice_x,
            "slice_y":       self.slice_y,
            "slice_z":       self.slice_z,
            "clim":          list(self.clim),
        }


# ---------------------------------------------------------------------------
# Volume processing DTO
# ---------------------------------------------------------------------------

@dataclass(frozen=True)
class VolumeProcessDTO:
    """
    Immutable configuration for a headless processing run.

    Used by the CLI and by unit tests that bypass the GUI entirely.
    """

    # Input
    input_path:       str                       = ""
    loader_type:      str                       = "dicom"    # "dicom" | "dummy"

    # Segmentation
    threshold:        float                     = 300.0
    auto_threshold:   bool                      = False

    # PNM
    min_pore_size:    int                       = 100
    min_throat_dist:  int                       = 3

    # Output
    output_dir:       Optional[str]             = None
    export_formats:   Tuple[str, ...]           = ("vtk",)   # "vtk", "tiff", "npy"

    # Chunking
    chunk_shape:      Tuple[int, int, int]      = (128, 128, 128)
    halo_voxels:      int                       = 8

    # Render (headless preview / thumbnails)
    render_params:    Optional[RenderParamsDTO] = None

    # ------------------------------------------------------------------
    # Factories
    # ------------------------------------------------------------------

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "VolumeProcessDTO":
        rp_raw = d.get("render_params")
        rp     = RenderParamsDTO.from_dict(rp_raw) if rp_raw else None
        return VolumeProcessDTO(
            input_path      = str(d.get("input_path",      "")),
            loader_type     = str(d.get("loader_type",     "dicom")),
            threshold       = float(d.get("threshold",     300.0)),
            auto_threshold  = bool(d.get("auto_threshold", False)),
            min_pore_size   = int(d.get("min_pore_size",   100)),
            min_throat_dist = int(d.get("min_throat_dist", 3)),
            output_dir      = d.get("output_dir"),
            export_formats  = tuple(d.get("export_formats", ["vtk"])),
            chunk_shape     = tuple(int(x) for x in d.get("chunk_shape", [128, 128, 128])),
            halo_voxels     = int(d.get("halo_voxels",     8)),
            render_params   = rp,
        )

    @staticmethod
    def from_yaml(path: str) -> "VolumeProcessDTO":
        """Load config from a YAML file (no PyQt dependency)."""
        import yaml  # soft dependency 鈥?only needed for CLI
        with open(path, encoding="utf-8") as fh:
            d = yaml.safe_load(fh)
        return VolumeProcessDTO.from_dict(d or {})

    @staticmethod
    def from_json(path: str) -> "VolumeProcessDTO":
        """Load config from a JSON file."""
        import json
        with open(path, encoding="utf-8") as fh:
            d = json.load(fh)
        return VolumeProcessDTO.from_dict(d)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "input_path":      self.input_path,
            "loader_type":     self.loader_type,
            "threshold":       self.threshold,
            "auto_threshold":  self.auto_threshold,
            "min_pore_size":   self.min_pore_size,
            "min_throat_dist": self.min_throat_dist,
            "output_dir":      self.output_dir,
            "export_formats":  list(self.export_formats),
            "chunk_shape":     list(self.chunk_shape),
            "halo_voxels":     self.halo_voxels,
            "render_params":   self.render_params.to_dict() if self.render_params else None,
        }


--- FILE: .\core\gpu_backend.py ---
"""
GPU Backend Manager for CuPy acceleration.
Provides transparent switching between NumPy and CuPy based on availability.
"""

from typing import Optional, Any, Tuple
import numpy as np

# Try to import CuPy
try:
    import cupy as cp
    import cupyx.scipy.ndimage as gpu_ndimage
    CUPY_AVAILABLE = True
except ImportError:
    cp = None
    gpu_ndimage = None
    CUPY_AVAILABLE = False


class GPUBackend:
    """Singleton for GPU backend management."""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if GPUBackend._initialized:
            return
        GPUBackend._initialized = True
        self._gpu_enabled = True
        
        if not CUPY_AVAILABLE:
            print("[GPU] CuPy not available, using CPU backend")
            self._gpu_enabled = False
            return
        
        try:
            device = cp.cuda.Device()
            print(f"[GPU] Initialized: Device {device.id}, {device.mem_info[1] / (1024**3):.1f} GB VRAM")
            self._configure_memory_pool()
            self._warmup_kernels()
        except Exception as e:
            print(f"[GPU] Initialization failed: {e}")
            self._gpu_enabled = False
    
    def _configure_memory_pool(self):
        """Configure CuPy memory pool for optimal performance."""
        try:
            mempool = cp.get_default_memory_pool()
            free_mem = cp.cuda.Device().mem_info[0]
            mempool.set_limit(size=int(free_mem * 0.9))
            print(f"[GPU] Memory pool configured: {free_mem / (1024**3):.1f} GB limit")
        except Exception as e:
            print(f"[GPU] Memory pool config warning: {e}")
    
    def _warmup_kernels(self):
        """Pre-compile common CUDA kernels."""
        try:
            data = cp.zeros((16, 16, 16), dtype=cp.float32)
            mask = data > 0
            gpu_ndimage.binary_dilation(mask)
            gpu_ndimage.maximum_filter(data, size=3)
            gpu_ndimage.distance_transform_edt(mask.astype(cp.uint8))
            cp.argwhere(mask)
            del data, mask
            cp.get_default_memory_pool().free_all_blocks()
            print("[GPU] Kernel warmup completed")
        except Exception as e:
            print(f"[GPU] Kernel warmup failed (non-critical): {e}")
    
    @property
    def available(self) -> bool:
        """Check if GPU is available and enabled."""
        return CUPY_AVAILABLE and self._gpu_enabled
    
    def set_enabled(self, enabled: bool):
        """Enable or disable GPU acceleration."""
        self._gpu_enabled = enabled
    
    def get_free_memory_mb(self) -> float:
        """Get free GPU memory in MB."""
        if not self.available:
            return 0.0
        try:
            free = cp.cuda.Device().mem_info[0]
            return free / (1024 * 1024)
        except Exception:
            return 0.0
    
    def can_fit(self, size_bytes: int, safety_factor: float = 0.8) -> bool:
        """Check if data can fit in GPU memory."""
        if not self.available:
            return False
        free_mb = self.get_free_memory_mb()
        required_mb = size_bytes / (1024 * 1024)
        return required_mb < (free_mb * safety_factor)
    
    def to_gpu(self, array: np.ndarray) -> Any:
        """Transfer NumPy array to GPU."""
        if self.available and isinstance(array, np.ndarray):
            return cp.asarray(array)
        return array
    
    def to_cpu(self, array: Any) -> np.ndarray:
        """Transfer GPU array to CPU."""
        if CUPY_AVAILABLE and isinstance(array, cp.ndarray):
            return cp.asnumpy(array)
        return array
    
    def clear_memory(self, force: bool = False):
        """Clear GPU memory pool. Use force=True only when memory is critically low."""
        if CUPY_AVAILABLE:
            mempool = cp.get_default_memory_pool()
            if force:
                mempool.free_all_blocks()
            else:
                mempool.free_all_blocks()
    
    def create_stream(self, non_blocking: bool = True) -> Any:
        """Create a CUDA stream for async operations."""
        return cp.cuda.Stream(non_blocking=non_blocking) if self.available else None
    
    def to_gpu_async(self, array: np.ndarray, stream: Any = None) -> Any:
        """Transfer NumPy array to GPU asynchronously."""
        if not self.available or not isinstance(array, np.ndarray):
            return array
        if stream:
            with stream:
                return cp.asarray(array)
        return cp.asarray(array)
    
    def to_cpu_async(self, array: Any, stream: Any = None) -> np.ndarray:
        """Transfer GPU array to CPU asynchronously."""
        if not CUPY_AVAILABLE or not isinstance(array, cp.ndarray):
            return array
        if stream:
            with stream:
                return cp.asnumpy(array)
        return cp.asnumpy(array)
    
    def synchronize_stream(self, stream: Any):
        """Wait for stream operations to complete."""
        if stream and self.available:
            stream.synchronize()


# Global singleton
_backend: Optional[GPUBackend] = None


def get_gpu_backend() -> GPUBackend:
    """Get the global GPU backend instance."""
    global _backend
    if _backend is None:
        _backend = GPUBackend()
    return _backend


def is_gpu_available() -> bool:
    """Check if GPU acceleration is available."""
    return get_gpu_backend().available


--- FILE: .\core\time_series.py ---
"""
Time series data structures for 4D CT pore network tracking.

Provides data classes for tracking pore network changes across
multiple timepoints in a compression sequence.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Tuple, Optional, Any
import numpy as np


class PoreStatus(Enum):
    """Status of a tracked pore across timepoints."""
    ACTIVE = "active"           # Pore is present and measurable
    COMPRESSED = "compressed"   # Pore volume has dropped below threshold
    UNKNOWN = "unknown"         # Initial state before tracking


@dataclass
class PNMSnapshot:
    """
    A single timepoint snapshot of the pore network model.
    
    This captures the state of all pores at a specific time index,
    including their spatial positions, sizes, and connectivity.
    
    Attributes:
        time_index: Integer index of this timepoint (0 = reference)
        pore_centers: Coordinates of pore centroids (N, 3) in world space
        pore_radii: Equivalent sphere radii for each pore (N,)
        pore_ids: Unique identifier for each pore (N,)
        pore_volumes: Voxel count for each pore (N,)
        connections: List of (pore_id_a, pore_id_b) tuples for throats
        segmented_regions: 3D label array from watershed (optional, memory intensive)
        metadata: Additional information (threshold used, etc.)
    """
    time_index: int
    pore_centers: np.ndarray
    pore_radii: np.ndarray
    pore_ids: np.ndarray
    pore_volumes: np.ndarray
    connections: List[Tuple[int, int]]
    segmented_regions: Optional[np.ndarray] = None
    spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0)
    origin: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def num_pores(self) -> int:
        """Number of pores in this snapshot."""
        return len(self.pore_ids)
    
    @property
    def num_connections(self) -> int:
        """Number of throat connections."""
        return len(self.connections)
    
    def get_pore_data(self, pore_id: int) -> Optional[Dict[str, Any]]:
        """Get all data for a specific pore by ID."""
        mask = self.pore_ids == pore_id
        if not np.any(mask):
            return None
        idx = np.argmax(mask)
        return {
            'id': int(pore_id),
            'center': self.pore_centers[idx].tolist(),
            'radius': float(self.pore_radii[idx]),
            'volume': float(self.pore_volumes[idx])
        }


@dataclass
class PoreTrackingResult:
    """
    Results of tracking pores across all timepoints.
    
    This maintains the correspondence between pores in the reference
    (t=0) and all subsequent timepoints, tracking volume changes
    and compression events.
    
    Attributes:
        reference_ids: List of pore IDs from the reference snapshot
        volume_history: {pore_id: [v0, v1, v2, ...]} volume at each timepoint
        status_history: {pore_id: [status0, status1, ...]} status at each timepoint
        id_mapping: {time_index: {ref_id: current_id}} maps reference to current IDs
        iou_history: {pore_id: [iou0, iou1, ...]} IoU scores for matching quality
        match_confidence: {pore_id: [c0, c1, ...]} assignment confidence in [0, 1]
        miss_count: {pore_id: count} consecutive unmatched count
        unmatched_reason: {pore_id: [reason0, reason1, ...]} reason when unmatched
        evaluation: Optional structured quality report against simulation GT labels
    """
    reference_ids: List[int] = field(default_factory=list)
    volume_history: Dict[int, List[float]] = field(default_factory=dict)
    status_history: Dict[int, List[PoreStatus]] = field(default_factory=dict)
    id_mapping: Dict[int, Dict[int, int]] = field(default_factory=dict)
    iou_history: Dict[int, List[float]] = field(default_factory=dict)
    center_history: Dict[int, List[List[float]]] = field(default_factory=dict)
    match_confidence: Dict[int, List[float]] = field(default_factory=dict)
    miss_count: Dict[int, int] = field(default_factory=dict)
    unmatched_reason: Dict[int, List[str]] = field(default_factory=dict)
    evaluation: Dict[str, Any] = field(default_factory=dict)
    
    def get_volume_series(self, pore_id: int) -> List[float]:
        """Get volume history for a specific pore."""
        return self.volume_history.get(pore_id, [])
    
    def get_status_series(self, pore_id: int) -> List[PoreStatus]:
        """Get status history for a specific pore."""
        return self.status_history.get(pore_id, [])
    
    def get_compression_timepoint(self, pore_id: int) -> Optional[int]:
        """
        Get the timepoint at which a pore first became compressed.
        Returns None if the pore was never compressed.
        """
        statuses = self.status_history.get(pore_id, [])
        for i, status in enumerate(statuses):
            if status == PoreStatus.COMPRESSED:
                return i
        return None
    
    def get_compressed_pore_ids(self, at_timepoint: Optional[int] = None) -> List[int]:
        """
        Get list of pore IDs that are compressed.
        
        Args:
            at_timepoint: If specified, return pores compressed at that timepoint.
                         If None, return all pores that are currently compressed.
        """
        compressed = []
        for pore_id, statuses in self.status_history.items():
            if at_timepoint is not None:
                if at_timepoint < len(statuses) and statuses[at_timepoint] == PoreStatus.COMPRESSED:
                    compressed.append(pore_id)
            else:
                # Check if currently (last status) is compressed
                if statuses and statuses[-1] == PoreStatus.COMPRESSED:
                    compressed.append(pore_id)
        return compressed
    
    def get_active_pore_ids(self, at_timepoint: Optional[int] = None) -> List[int]:
        """Get list of pore IDs that are still active (not compressed)."""
        active = []
        for pore_id, statuses in self.status_history.items():
            if at_timepoint is not None:
                if at_timepoint < len(statuses) and statuses[at_timepoint] == PoreStatus.ACTIVE:
                    active.append(pore_id)
            else:
                if statuses and statuses[-1] == PoreStatus.ACTIVE:
                    active.append(pore_id)
        return active
    
    def get_volume_ratio(self, pore_id: int, timepoint: int) -> float:
        """
        Get the volume ratio (current/reference) for a pore at a timepoint.
        Returns 0.0 if compressed, or the ratio of current volume to t=0 volume.
        """
        volumes = self.volume_history.get(pore_id, [])
        if not volumes or timepoint >= len(volumes):
            return 0.0
        if volumes[0] == 0:
            return 0.0
        return volumes[timepoint] / volumes[0]


@dataclass
class TimeSeriesPNM:
    """
    Complete 4D CT pore network model with temporal tracking.
    
    This is the top-level data structure that holds:
    - The reference (baseline) pore network from t=0
    - All subsequent snapshots
    - The tracking results linking pores across time
    
    Attributes:
        reference_snapshot: The baseline PNM from timepoint 0
        snapshots: List of all snapshots including reference
        tracking: Results of pore tracking across timepoints
        source_folders: List of DICOM folder paths for each timepoint
    """
    reference_snapshot: Optional[PNMSnapshot] = None
    snapshots: List[PNMSnapshot] = field(default_factory=list)
    tracking: PoreTrackingResult = field(default_factory=PoreTrackingResult)
    source_folders: List[str] = field(default_factory=list)
    
    @property
    def num_timepoints(self) -> int:
        """Number of timepoints in the series."""
        return len(self.snapshots)
    
    @property
    def num_reference_pores(self) -> int:
        """Number of pores in the reference snapshot."""
        if self.reference_snapshot:
            return self.reference_snapshot.num_pores
        return 0
    
    def get_snapshot(self, time_index: int) -> Optional[PNMSnapshot]:
        """Get snapshot at a specific time index."""
        if 0 <= time_index < len(self.snapshots):
            return self.snapshots[time_index]
        return None
    
    def get_pore_volume_at_time(self, pore_id: int, time_index: int) -> float:
        """Get volume of a specific pore at a specific timepoint."""
        volumes = self.tracking.volume_history.get(pore_id, [])
        if time_index < len(volumes):
            return volumes[time_index]
        return 0.0
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of the time series analysis."""
        if not self.reference_snapshot:
            return {"status": "No reference snapshot"}
        
        num_compressed = len(self.tracking.get_compressed_pore_ids())
        num_active = len(self.tracking.get_active_pore_ids())
        
        # Calculate average volume retention
        volume_ratios = []
        for pore_id in self.tracking.reference_ids:
            ratio = self.tracking.get_volume_ratio(pore_id, self.num_timepoints - 1)
            volume_ratios.append(ratio)
        
        avg_retention = np.mean(volume_ratios) if volume_ratios else 0.0
        
        summary = {
            "num_timepoints": self.num_timepoints,
            "reference_pores": self.num_reference_pores,
            "active_pores": num_active,
            "compressed_pores": num_compressed,
            "compression_rate": num_compressed / self.num_reference_pores if self.num_reference_pores > 0 else 0,
            "avg_volume_retention": avg_retention
        }

        eval_report = self.tracking.evaluation if isinstance(self.tracking.evaluation, dict) else {}
        overall_eval = eval_report.get("overall", {}) if isinstance(eval_report, dict) else {}
        if isinstance(overall_eval, dict):
            summary["sim_eval_available"] = bool(eval_report.get("available", False))
            if "mean_tracking_accuracy" in overall_eval:
                summary["sim_eval_mean_tracking_accuracy"] = float(overall_eval.get("mean_tracking_accuracy", 0.0))
            if "mean_instance_f1" in overall_eval:
                summary["sim_eval_mean_instance_f1"] = float(overall_eval.get("mean_instance_f1", 0.0))

        return summary


--- FILE: .\core\__init__.py ---
"""
Core module containing base classes and data structures.
"""

from core.base import VolumeData, BaseLoader, BaseProcessor, BaseVisualizer
from core.gpu_backend import get_gpu_backend, is_gpu_available
from core.time_series import PNMSnapshot, PoreTrackingResult, TimeSeriesPNM, PoreStatus
from core.dto import RenderParamsDTO, VolumeProcessDTO
from core.chunker import SpatialChunker, ChunkDescriptor, DAGNode, SimpleDAGExecutor, edt_chunked

__all__ = [
    'VolumeData', 'BaseLoader', 'BaseProcessor', 'BaseVisualizer',
    'get_gpu_backend', 'is_gpu_available',
    'PNMSnapshot', 'PoreTrackingResult', 'TimeSeriesPNM', 'PoreStatus',
    'RenderParamsDTO', 'VolumeProcessDTO',
    'SpatialChunker', 'ChunkDescriptor', 'DAGNode', 'SimpleDAGExecutor', 'edt_chunked',
]


--- FILE: .\data\disk_cache.py ---
"""
Disk cache manager for large volume data.
Provides memory-mapped arrays for out-of-core processing of large datasets.
"""

import os
import tempfile
import logging
import numpy as np
from typing import Optional, Tuple, Dict, Any, List
import gc

logger = logging.getLogger(__name__)


class DiskCacheManager:
    """
    Manages disk-based caching for large numpy arrays.
    Uses memory-mapped files to avoid RAM limitations.
    """
    
    def __init__(self, cache_dir: Optional[str] = None, prefix: str = "porous_cache"):
        """
        Args:
            cache_dir: Directory for cache files. Uses system temp if None.
            prefix: Prefix for cache file names.
        """
        self.cache_dir = cache_dir or tempfile.gettempdir()
        self.prefix = prefix
        self._cache_files: Dict[str, str] = {}
        self._cache_arrays: Dict[str, np.memmap] = {}
    
    def create_array(self, name: str, shape: Tuple[int, ...], dtype=np.float32) -> np.memmap:
        """
        Create a memory-mapped array on disk.
        
        Args:
            name: Unique identifier for this array.
            shape: Shape of the array.
            dtype: Data type.
            
        Returns:
            Memory-mapped numpy array.
        """
        # Clean up existing array with same name
        self.release(name)
        
        # Create memory-mapped file
        filename = os.path.join(self.cache_dir, f"{self.prefix}_{name}_{id(self)}.dat")
        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=shape)
        
        self._cache_files[name] = filename
        self._cache_arrays[name] = arr
        
        print(f"[DiskCache] Created '{name}': {shape}, {dtype}, {self._format_size(arr.nbytes)}")
        return arr
    
    def get_array(self, name: str) -> Optional[np.memmap]:
        """Get a cached array by name."""
        return self._cache_arrays.get(name)
    
    def release(self, name: str):
        """Release a specific cached array and delete its file."""
        if name in self._cache_arrays:
            del self._cache_arrays[name]
            gc.collect()
        
        if name in self._cache_files:
            try:
                os.remove(self._cache_files[name])
            except OSError as e:
                logger.warning(f"Failed to remove cache file {self._cache_files[name]}: {e}")
            del self._cache_files[name]
    
    def release_all(self):
        """Release all cached arrays and delete all cache files."""
        names = list(self._cache_files.keys())
        for name in names:
            self.release(name)
        gc.collect()
    
    def _format_size(self, size_bytes: int) -> str:
        """Format byte size to human readable string."""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"
    
    def __del__(self):
        """Clean up on deletion."""
        try:
            self.release_all()
        except Exception as e:
            # Log but don't raise in destructor - may be called during interpreter shutdown
            logger.debug(f"Cache cleanup during destruction failed: {e}")


class ChunkedProcessor:
    """
    Processes large volumes in chunks to avoid memory exhaustion.
    """
    
    def __init__(self, chunk_size: int = 64):
        """
        Args:
            chunk_size: Number of slices to process at a time.
        """
        self.chunk_size = chunk_size
        self.cache = DiskCacheManager()
    
    def process_chunked(self, volume: np.ndarray, func, 
                        output_dtype=np.float32,
                        callback=None) -> np.memmap:
        """
        Process a volume in chunks, storing results on disk.
        
        Args:
            volume: Input volume array.
            func: Function to apply to each chunk. Signature: func(chunk) -> result.
            output_dtype: Data type for output array.
            callback: Optional progress callback.
            
        Returns:
            Memory-mapped result array.
        """
        shape = volume.shape
        n_slices = shape[0]
        n_chunks = (n_slices + self.chunk_size - 1) // self.chunk_size
        
        # Create output array on disk
        output = self.cache.create_array("output", shape, dtype=output_dtype)
        
        for i in range(n_chunks):
            start = i * self.chunk_size
            end = min(start + self.chunk_size, n_slices)
            
            # Process chunk
            chunk = volume[start:end]
            result = func(chunk)
            output[start:end] = result
            
            # Flush to disk
            output.flush()
            
            if callback:
                progress = int(100 * (i + 1) / n_chunks)
                callback(progress, f"Processing chunk {i+1}/{n_chunks}...")
            
            # Free chunk memory
            del chunk, result
            gc.collect()
        
        return output
    
    def cleanup(self):
        """Release all cached data."""
        self.cache.release_all()


# Global cache instance for shared use
_global_cache: Optional[DiskCacheManager] = None


def get_disk_cache() -> DiskCacheManager:
    """Get the global disk cache manager."""
    global _global_cache
    if _global_cache is None:
        _global_cache = DiskCacheManager()
    return _global_cache


def clear_disk_cache():
    """Clear the global disk cache."""
    global _global_cache
    if _global_cache is not None:
        _global_cache.release_all()
        _global_cache = None
    gc.collect()


# ==========================================
# Segmentation Cache (for PNM processing)
# ==========================================

class SegmentationCache:
    """
    Shared cache for segmentation results between PoreExtractionProcessor and PoreToSphereProcessor.
    Uses disk-backed storage to avoid memory issues with large volumes.
    
    Usage:
        cache = get_segmentation_cache()
        
        # Store result
        cache.store_pores_mask(volume_id, pores_mask, metadata)
        
        # Retrieve result
        result = cache.get_pores_mask(volume_id)
    """
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or tempfile.gettempdir()
        self._prefix = "porous_seg"
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._memmap_files: Dict[str, str] = {}
    
    def _get_volume_id(self, raw_data: np.ndarray, threshold: int) -> str:
        """Generate unique ID for a volume + threshold combination."""
        return f"{id(raw_data)}_{threshold}"
    
    def has_cache(self, volume_id: str) -> bool:
        """Check if cache exists for given volume ID."""
        return volume_id in self._cache
    
    def store_pores_mask(self, volume_id: str, pores_mask: np.ndarray, 
                         metadata: Optional[Dict] = None, use_disk: bool = True) -> None:
        """
        Store pores mask in cache.
        
        Args:
            volume_id: Unique identifier for this volume
            pores_mask: Boolean mask of pore voxels
            metadata: Optional metadata (porosity, pore count, etc.)
            use_disk: If True, store on disk as memory-mapped file
        """
        if use_disk and pores_mask.nbytes > 100 * 1024 * 1024:  # > 100MB
            # Store on disk
            filename = os.path.join(self.cache_dir, f"{self._prefix}_{volume_id}.dat")
            mmap = np.memmap(filename, dtype=pores_mask.dtype, mode='w+', shape=pores_mask.shape)
            mmap[:] = pores_mask[:]
            mmap.flush()
            
            self._memmap_files[volume_id] = filename
            self._cache[volume_id] = {
                'shape': pores_mask.shape,
                'dtype': pores_mask.dtype,
                'disk': True,
                'filename': filename,
                'metadata': metadata or {}
            }
            print(f"[SegCache] Stored pores_mask to disk: {filename}")
        else:
            # Store in memory
            self._cache[volume_id] = {
                'pores_mask': pores_mask.copy(),
                'disk': False,
                'metadata': metadata or {}
            }
            print(f"[SegCache] Stored pores_mask in memory: {volume_id}")
    
    def get_pores_mask(self, volume_id: str) -> Optional[np.ndarray]:
        """
        Retrieve pores mask from cache.
        
        Returns:
            pores_mask array or None if not cached
        """
        if volume_id not in self._cache:
            return None
        
        entry = self._cache[volume_id]
        
        if entry.get('disk'):
            # Load from disk as memmap (read-only)
            filename = entry['filename']
            if os.path.exists(filename):
                return np.memmap(filename, dtype=entry['dtype'], mode='r', shape=entry['shape'])
            return None
        else:
            return entry.get('pores_mask')
    
    def get_metadata(self, volume_id: str) -> Optional[Dict]:
        """Get cached metadata for a volume."""
        if volume_id not in self._cache:
            return None
        return self._cache[volume_id].get('metadata')
    
    def clear(self, volume_id: Optional[str] = None):
        """
        Clear cache entries.
        
        Args:
            volume_id: Specific entry to clear. If None, clears all.
        """
        if volume_id:
            if volume_id in self._cache:
                entry = self._cache[volume_id]
                if entry.get('disk') and 'filename' in entry:
                    try:
                        os.remove(entry['filename'])
                    except OSError as e:
                        logger.warning(f"Failed to remove segmentation cache file: {e}")
                del self._cache[volume_id]
        else:
            # Clear all
            for vid, entry in list(self._cache.items()):
                if entry.get('disk') and 'filename' in entry:
                    try:
                        os.remove(entry['filename'])
                    except OSError as e:
                        logger.warning(f"Failed to remove segmentation cache file {entry['filename']}: {e}")
            self._cache.clear()
            self._memmap_files.clear()
        gc.collect()
        print(f"[SegCache] Cleared cache")
    
    def __del__(self):
        try:
            self.clear()
        except Exception as e:
            # Log but don't raise in destructor
            logger.debug(f"SegmentationCache cleanup during destruction failed: {e}")


# Global singleton instance for segmentation cache
_global_seg_cache: Optional[SegmentationCache] = None


def get_segmentation_cache() -> SegmentationCache:
    """Get the global segmentation cache."""
    global _global_seg_cache
    if _global_seg_cache is None:
        _global_seg_cache = SegmentationCache()
    return _global_seg_cache


def clear_segmentation_cache():
    """Clear the global segmentation cache."""
    global _global_seg_cache
    if _global_seg_cache is not None:
        _global_seg_cache.clear()
    gc.collect()


# ==========================================
# Time Series PNM Cache (for 4DCT tracking)
# ==========================================

class TimeSeriesPNMCache:
    """
    Cache for 4DCT time series PNM tracking results.
    Stores TimeSeriesPNM objects on disk using pickle serialization
    to avoid regenerating PNM when switching between views.
    
    Usage:
        cache = get_timeseries_pnm_cache()
        
        # Store time series PNM after tracking
        cache_key = cache.generate_key(volumes, threshold)
        cache.store(cache_key, time_series_pnm, reference_mesh)
        
        # Retrieve when switching back to PNM view
        cached_data = cache.get(cache_key)
        if cached_data:
            time_series_pnm, reference_mesh = cached_data
    """
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or tempfile.gettempdir()
        self._prefix = "porous_4dct_pnm"
        self._cache: Dict[str, Dict[str, Any]] = {}
        
    def generate_key(self, volumes: List, threshold: int) -> str:
        """
        Generate unique cache key from volumes and threshold.
        Uses volume IDs and folder names to create a stable identifier.
        
        Args:
            volumes: List of VolumeData objects
            threshold: HU threshold value
            
        Returns:
            Unique cache key string
        """
        # Create key from volume IDs and folder names
        volume_ids = []
        for vol in volumes:
            folder_name = vol.metadata.get('folder_name', '')
            vol_id = id(vol.raw_data) if vol.raw_data is not None else 0
            volume_ids.append(f"{folder_name}_{vol_id}")
        
        key_str = "_".join(volume_ids) + f"_t{threshold}"
        # Use hash to keep key length manageable
        import hashlib
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def has_cache(self, cache_key: str) -> bool:
        """Check if cache exists for given key."""
        return cache_key in self._cache
    
    def store(self, cache_key: str, time_series_pnm, reference_mesh, 
              reference_snapshot) -> None:
        """
        Store time series PNM data to cache.
        
        Args:
            cache_key: Unique cache identifier
            time_series_pnm: TimeSeriesPNM object with tracking results
            reference_mesh: Reference mesh (VolumeData) from t=0
            reference_snapshot: Reference PNMSnapshot from t=0
        """
        import pickle
        
        cache_data = {
            'time_series_pnm': time_series_pnm,
            'reference_mesh': reference_mesh,
            'reference_snapshot': reference_snapshot
        }
        
        filename = os.path.join(self.cache_dir, f"{self._prefix}_{cache_key}.pkl")
        
        try:
            with open(filename, 'wb') as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            self._cache[cache_key] = {
                'filename': filename,
                'num_timepoints': time_series_pnm.num_timepoints,
                'num_pores': time_series_pnm.num_reference_pores
            }
            
            size_mb = os.path.getsize(filename) / (1024 * 1024)
            print(f"[4DCT PNM Cache] Stored: {cache_key[:8]}... "
                  f"({time_series_pnm.num_timepoints} timepoints, "
                  f"{time_series_pnm.num_reference_pores} pores, "
                  f"{size_mb:.1f} MB)")
        except Exception as e:
            logger.error(f"Failed to store 4DCT PNM cache: {e}")
    
    def get(self, cache_key: str) -> Optional[Tuple]:
        """
        Retrieve time series PNM data from cache.
        
        Returns:
            Tuple of (time_series_pnm, reference_mesh, reference_snapshot) or None
        """
        if cache_key not in self._cache:
            return None
        
        import pickle
        
        entry = self._cache[cache_key]
        filename = entry['filename']
        
        if not os.path.exists(filename):
            # Cache file was deleted, remove entry
            del self._cache[cache_key]
            return None
        
        try:
            with open(filename, 'rb') as f:
                cache_data = pickle.load(f)
            
            print(f"[4DCT PNM Cache] Loaded: {cache_key[:8]}... "
                  f"({entry['num_timepoints']} timepoints, "
                  f"{entry['num_pores']} pores)")
            
            return (
                cache_data['time_series_pnm'],
                cache_data['reference_mesh'],
                cache_data['reference_snapshot']
            )
        except Exception as e:
            logger.error(f"Failed to load 4DCT PNM cache: {e}")
            return None
    
    def clear(self, cache_key: Optional[str] = None):
        """
        Clear cache entries.
        
        Args:
            cache_key: Specific entry to clear. If None, clears all.
        """
        if cache_key:
            if cache_key in self._cache:
                entry = self._cache[cache_key]
                filename = entry['filename']
                try:
                    if os.path.exists(filename):
                        os.remove(filename)
                except OSError as e:
                    logger.warning(f"Failed to remove 4DCT PNM cache file: {e}")
                del self._cache[cache_key]
        else:
            # Clear all
            for key, entry in list(self._cache.items()):
                filename = entry['filename']
                try:
                    if os.path.exists(filename):
                        os.remove(filename)
                except OSError as e:
                    logger.warning(f"Failed to remove 4DCT PNM cache file {filename}: {e}")
            self._cache.clear()
            print(f"[4DCT PNM Cache] Cleared all cache")
        
        gc.collect()
    
    def __del__(self):
        try:
            self.clear()
        except Exception as e:
            logger.debug(f"TimeSeriesPNMCache cleanup during destruction failed: {e}")


# Global singleton instance for time series PNM cache
_global_ts_pnm_cache: Optional[TimeSeriesPNMCache] = None


def get_timeseries_pnm_cache() -> TimeSeriesPNMCache:
    """Get the global time series PNM cache."""
    global _global_ts_pnm_cache
    if _global_ts_pnm_cache is None:
        _global_ts_pnm_cache = TimeSeriesPNMCache()
    return _global_ts_pnm_cache


def clear_timeseries_pnm_cache():
    """Clear the global time series PNM cache."""
    global _global_ts_pnm_cache
    if _global_ts_pnm_cache is not None:
        _global_ts_pnm_cache.clear()
    gc.collect()



--- FILE: .\data\manager.py ---
"""
Scientific data manager for porous media analysis workflow.

Responsibilities:
- Manages runtime data state (raw, segmented, PNM, ROI)
- Provides data manipulation operations (clip, invert, histogram)
- Coordinates cache lifecycle
- Emits signals on data changes for UI synchronization
"""

from typing import Optional, Tuple, Callable
import numpy as np

from PyQt5.QtCore import QObject, pyqtSignal

from core import VolumeData


class ScientificDataManager(QObject):
    """
    Central Data Management Class for Scientific Computing.

    Manages the critical stages of the Porous Media Analysis workflow:
    1. Raw CT Data (Original Scan)
    2. Segmented Volume (Distinguishing Air/Void from Solid)
    3. Pore Network Model (PNM) Mesh (Distinguishing Pores vs Throats)
    4. ROI Data (Region of Interest extracted sub-volume)
    
    Also provides data manipulation operations with proper cache coordination.
    """
    
    # Signals
    data_changed = pyqtSignal()  # Emitted when active data is modified
    data_loaded = pyqtSignal(object)  # Emitted when new data is loaded (VolumeData)

    def __init__(self):
        super().__init__()
        self.raw_ct_data: Optional[VolumeData] = None
        self.segmented_volume: Optional[VolumeData] = None
        self.pnm_model: Optional[VolumeData] = None
        self.roi_data: Optional[VolumeData] = None

    @property
    def active_data(self) -> Optional[VolumeData]:
        """
        Returns the most relevant data for processing.
        Priority: ROI > Raw (we process on extracted region if available)
        """
        if self.roi_data is not None:
            return self.roi_data
        return self.raw_ct_data

    def load_raw_data(self, data: VolumeData):
        """Sets the raw input data. Clears previous data first."""
        # Clear all previous data to free memory.
        # Explicit reference drops trigger CPython's ref-count destructor
        # immediately 鈥?no gc.collect() required.
        self.raw_ct_data       = None
        self.segmented_volume  = None
        self.pnm_model         = None
        self.roi_data          = None

        # Clear segmentation cache
        self._clear_segmentation_cache()

        # Now set new data
        self.raw_ct_data = data
        self.data_loaded.emit(data)

    def set_segmented_data(self, data: VolumeData):
        """Stores the intermediate segmented void space."""
        self.segmented_volume = data

    def set_pnm_data(self, data: VolumeData):
        """Stores the generated Pore Network Model mesh."""
        self.pnm_model = data

    def set_roi_data(self, data: VolumeData):
        """Stores ROI-extracted sub-volume for focused analysis."""
        self.roi_data = data
        self.segmented_volume = None
        self.pnm_model = None
        self._clear_segmentation_cache()

    def clear_roi(self):
        """Clears ROI data, reverting to full raw volume."""
        self.roi_data = None
        self.segmented_volume = None
        self.pnm_model = None
        self._clear_segmentation_cache()

    # ==========================================
    # Data Manipulation Methods
    # ==========================================
    
    def clip_data(self, min_val: float, max_val: float,
                  chunk_size: int = 32,
                  progress_callback: Optional[Callable[[int, str], None]] = None) -> None:
        """
        Permanently clip active data to [min_val, max_val] in-place.

        Uses ``np.clip(out=raw)`` for a fully vectorised, zero-extra-copy
        operation.  The ``chunk_size`` parameter is kept for API compatibility
        but is no longer used (vectorised numpy is both faster and creates no
        additional peak-memory spike for a simple clip).

        Args:
            min_val: Minimum value to clip to
            max_val: Maximum value to clip to
            chunk_size: Retained for backward-compatibility; ignored.
            progress_callback: Optional callback for progress updates (percent, message)

        Raises:
            ValueError: If no active data available
        """
        data = self.active_data
        if data is None or data.raw_data is None:
            raise ValueError("No active data to clip")

        if progress_callback:
            progress_callback(0, "Clipping data...")

        # In-place, single-pass, zero extra allocation
        np.clip(data.raw_data, min_val, max_val, out=data.raw_data)

        # Update metadata
        data.metadata["ClipRange"] = f"[{min_val:.0f}, {max_val:.0f}]"
        if "(Clipped)" not in data.metadata.get("Type", ""):
            data.metadata["Type"] = data.metadata.get("Type", "CT") + " (Clipped)"

        # Clear related caches
        self._clear_segmentation_cache()

        if progress_callback:
            progress_callback(100, "Clip complete")

        # Emit data changed signal
        self.data_changed.emit()
    
    def invert_data(self, chunk_size: int = 32,
                    progress_callback: Optional[Callable[[int, str], None]] = None) -> Tuple[float, float, float]:
        """
        Invert volume values in-place: ``new_val = (max + min) - val``.

        Uses a fully vectorised numpy operation (``np.subtract(offset, raw,
        out=raw)``) instead of a chunked loop with gc.collect().  This avoids
        Stop-The-World pauses while being 2鈥?0脳 faster than the chunked path.

        Args:
            chunk_size: Retained for backward-compatibility; ignored.
            progress_callback: Optional callback for progress updates

        Returns:
            Tuple of (data_min, data_max, invert_offset)

        Raises:
            ValueError: If no active data available
        """
        data = self.active_data
        if data is None or data.raw_data is None:
            raise ValueError("No active data to invert")

        raw = data.raw_data
        data_min      = float(raw.min())
        data_max      = float(raw.max())
        invert_offset = data_max + data_min

        if progress_callback:
            progress_callback(0, "Inverting volume...")

        # Single vectorised in-place pass 鈥?no temporary copy, no gc pauses
        np.subtract(invert_offset, raw, out=raw)

        # Update metadata
        if "(Inverted)" not in data.metadata.get("Type", ""):
            data.metadata["Type"] = data.metadata.get("Type", "CT") + " (Inverted)"

        # Clear related caches
        self._clear_segmentation_cache()

        if progress_callback:
            progress_callback(100, "Invert complete")

        # Emit data changed signal
        self.data_changed.emit()

        return data_min, data_max, invert_offset
        
        return data_min, data_max, invert_offset
    
    def calculate_histogram(self, bins: int = 100, 
                            sample_step: int = 4) -> Tuple[np.ndarray, np.ndarray]:
        """
        Calculate histogram of active data with optional downsampling for performance.
        
        Args:
            bins: Number of histogram bins
            sample_step: Downsampling step for large volumes (applied in each dimension)
            
        Returns:
            Tuple of (histogram counts, bin edges)
        """
        data = self.active_data
        if data is None or data.raw_data is None:
            return np.array([]), np.array([])
        
        raw = data.raw_data
        
        # Downsample for performance if needed
        if raw.size > 10**6:
            sample = raw[::sample_step, ::sample_step, ::sample_step].flatten()
        else:
            sample = raw.flatten()
        
        hist, bin_edges = np.histogram(sample, bins=bins)
        return hist, bin_edges
    
    def get_data_range(self) -> Tuple[float, float]:
        """Get min/max values of active data."""
        data = self.active_data
        if data is None or data.raw_data is None:
            return (0.0, 1.0)
        
        return float(np.nanmin(data.raw_data)), float(np.nanmax(data.raw_data))

    # ==========================================
    # Cache Management
    # ==========================================
    
    def _clear_segmentation_cache(self):
        """Clear segmentation cache when data changes."""
        try:
            from data.disk_cache import clear_segmentation_cache
            clear_segmentation_cache()
        except Exception as e:
            print(f"[DataManager] Failed to clear segmentation cache: {e}")

    # ==========================================
    # Status Methods
    # ==========================================

    def get_current_state_info(self) -> str:
        """Returns a status string describing what data is available."""
        status = []
        if self.raw_ct_data:
            status.append("鉁?Raw CT Data")
        else:
            status.append("鉁?Raw CT Data")
            
        if self.roi_data:
            status.append("鉁?ROI Extracted")

        if self.segmented_volume:
            status.append("鉁?Segmented Void Volume")
        else:
            status.append("鉁?Segmented Void Volume")

        if self.pnm_model:
            status.append("鉁?PNM Mesh (Pores & Throats)")
        else:
            status.append("鉁?PNM Mesh")

        return " | ".join(status)

    def has_raw(self) -> bool:
        return self.raw_ct_data is not None
    
    def has_active_data(self) -> bool:
        return self.active_data is not None

    def has_segmented(self) -> bool:
        return self.segmented_volume is not None


--- FILE: .\data\__init__.py ---
"""
Data management package.
"""

from data.manager import ScientificDataManager
from data.disk_cache import (
    DiskCacheManager,
    ChunkedProcessor,
    SegmentationCache,
    TimeSeriesPNMCache,
    get_disk_cache,
    clear_disk_cache,
    get_segmentation_cache,
    clear_segmentation_cache,
    get_timeseries_pnm_cache,
    clear_timeseries_pnm_cache
)

__all__ = [
    'ScientificDataManager',
    'DiskCacheManager',
    'ChunkedProcessor',
    'SegmentationCache',
    'TimeSeriesPNMCache',
    'get_disk_cache',
    'clear_disk_cache',
    'get_segmentation_cache',
    'clear_segmentation_cache',
    'get_timeseries_pnm_cache',
    'clear_timeseries_pnm_cache'
]



--- FILE: .\exporters\vtk.py ---
"""
VTK format exporter for volumetric and mesh data.
"""

import numpy as np
import pyvista as pv

from core import VolumeData


class VTKExporter:
    """
    Responsible for exporting VolumeData (voxel or mesh) to VTK standard format files.
    Handles logic for .vti (Image Data) and .vtp (Poly Data).
    """

    @staticmethod
    def export(data: VolumeData, filepath: str) -> bool:
        """
        Automatically selects export strategy based on data type.

        Args:
            data: contains raw_data or VolumeData of mesh.
            filepath: target path.

        Returns:
            bool: return True once success.
        """
        if data is None:
            raise ValueError("No data to export.")

        if data.has_mesh:
            return VTKExporter._export_mesh(data.mesh, filepath)
        elif data.raw_data is not None:
            return VTKExporter._export_volume(data, filepath)
        else:
            raise ValueError("No exportable data found (neither Mesh nor Volume).")

    @staticmethod
    def _export_mesh(mesh: pv.PolyData, filepath: str) -> bool:
        """Export PNM mesh data (.vtp, .vtk)."""
        if "IsPore" in mesh.array_names:
            print(f"[Exporter] Detected 'IsPore' attribute. Pore=1, Throat=0.")
        else:
            print(f"[Exporter] Warning: 'IsPore' attribute missing in mesh.")

        mesh.save(filepath)
        print(f"[Exporter] Mesh saved to {filepath}")
        return True

    @staticmethod
    def _export_volume(data: VolumeData, filepath: str) -> bool:
        """Export voxel data (.vti)."""
        grid = pv.ImageData()
        grid.dimensions = np.array(data.raw_data.shape) + 1
        grid.origin = data.origin
        grid.spacing = data.spacing

        grid.cell_data["values"] = data.raw_data.flatten(order="F")

        grid.save(filepath)
        print(f"[Exporter] Volume saved to {filepath}")
        return True


--- FILE: .\exporters\__init__.py ---
"""
Data exporters package.
"""

from exporters.vtk import VTKExporter

__all__ = ['VTKExporter']


--- FILE: .\gui\main_window.py ---
"""
Main application window combining PyQt5 UI with PyVista 3D rendering.
Delegates rendering logic to RenderEngine for separation of concerns.
"""

import math
import weakref
from typing import Optional

import numpy as np
import pyvista as pv
from pyvistaqt import BackgroundPlotter

from PyQt5.QtWidgets import (QMainWindow, QWidget, QVBoxLayout,
                             QLabel, QFrame, QStatusBar, QScrollArea, QSplitter)
from PyQt5.QtCore import Qt, QTimer, QSignalBlocker
from PyQt5.QtGui import QFont

from core import VolumeData
from gui.panels import (
    VisualizationModePanel,
    RenderingParametersPanel,
    InfoPanel,
    ClipPlanePanel,
    ROIPanel
)
from gui.ui_constants import PANEL_MARGIN, PANEL_SPACING
from gui.widgets import CollapsiblePanel
from rendering import RenderEngine
from rendering.roi_handler import ROIHandler
from rendering.clip_handler import ClipHandler


class MainWindow(QMainWindow):
    """
    Main application window.
    Integrates PyQt5 UI controls with a PyVista 3D rendering canvas.
    Delegates rendering to RenderEngine for separation of concerns.

    Note: previously inherited from both QMainWindow and BaseVisualizer (ABC),
    which required a _MainWindowMeta metaclass hack.  BaseVisualizer is now a
    typing.Protocol so no metaclass resolution is needed.
    """

    def __init__(self):
        super().__init__()
        self._data_manager = None

        self.setWindowTitle("Porous CT Analysis Suite")
        self.setGeometry(100, 100, 1400, 900)
        self._init_ui()

        # Initialize RenderEngine after UI is ready
        self.render_engine = RenderEngine(
            plotter=self.plotter,
            params_panel=self.params_panel,
            info_panel=self.info_panel,
            clip_panel=self.clip_panel,
            status_callback=self.update_status
        )
        self._sync_render_style_ui()
        
        # Initialize ROIHandler
        self.roi_handler = ROIHandler(
            plotter=self.plotter,
            roi_panel=self.roi_panel,
            data_manager=self._data_manager,
            status_callback=self.update_status
        )
        
        # Initialize ClipHandler
        self.clip_handler = ClipHandler(
            plotter=self.plotter,
            clip_panel=self.clip_panel,
            render_engine=self.render_engine
        )
        
        # Connect signals to handlers
        self._connect_roi_signals()
        self._connect_clip_signals()

        # Debounce timer for expensive renders
        self.update_timer = QTimer()
        self.update_timer.setInterval(100)
        self.update_timer.setSingleShot(True)
        self.update_timer.timeout.connect(self._perform_delayed_render)

        self._setup_mouse_probe()
    
    def _connect_roi_signals(self):
        """Connect ROI panel signals to ROIHandler."""
        self.roi_panel.roi_toggled.connect(self.roi_handler.on_toggled)
        self.roi_panel.apply_roi.connect(lambda: self.roi_handler.on_apply(self.set_data))
        self.roi_panel.reset_roi.connect(lambda: self.roi_handler.on_reset(self.set_data))
        self.roi_panel.shape_changed.connect(self.roi_handler.on_shape_changed)
    
    def _connect_clip_signals(self):
        """Connect Clip panel signals to ClipHandler."""
        self.clip_panel.clip_toggled.connect(self.clip_handler.on_clip_toggled)
        self.clip_update_timer = QTimer()
        self.clip_update_timer.setInterval(200)
        self.clip_update_timer.setSingleShot(True)
        self.clip_update_timer.timeout.connect(self.clip_handler.apply_clip_planes)
        self.clip_panel.clip_changed.connect(lambda: self.clip_update_timer.start())

    def _init_ui(self):
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        main_layout = QVBoxLayout(main_widget)
        main_layout.setContentsMargins(0, 0, 0, 0)

        self.main_splitter = QSplitter(Qt.Horizontal)

        # Left Panel (Controls)
        left_scroll = self._create_control_panel()
        self.main_splitter.addWidget(left_scroll)

        # Center Panel (3D Canvas)
        self.plotter = BackgroundPlotter(
            window_size=(1000, 900),
            show=False,
            title="3D Structure Viewer"
        )
        self.main_splitter.addWidget(self.plotter.app_window)

        # Right Panel (Info)
        right_scroll = self._create_info_panel()
        self.main_splitter.addWidget(right_scroll)

        self.main_splitter.setSizes([350, 900, 350])
        self.main_splitter.setCollapsible(0, False)
        self.main_splitter.setCollapsible(2, False)

        main_layout.addWidget(self.main_splitter)

        self.status_bar = QStatusBar()
        self.setStatusBar(self.status_bar)
        self.update_status("Ready. Please load a sample scan.")

    def _create_control_panel(self) -> QWidget:
        scroll_area = QScrollArea()
        scroll_area.setWidgetResizable(True)
        scroll_area.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)

        panel = QWidget()
        self.control_panel_layout = QVBoxLayout(panel)
        layout = self.control_panel_layout
        layout.setContentsMargins(*PANEL_MARGIN)
        layout.setSpacing(PANEL_SPACING)

        title = QLabel("Control Panel")
        title.setFont(QFont("Arial", 16, QFont.Bold))
        title.setAlignment(Qt.AlignCenter)
        layout.addWidget(title)
        layout.addWidget(self._create_separator())

        # Mode Panel
        self.mode_panel = VisualizationModePanel()
        self.mode_panel.volume_clicked.connect(lambda: self.render_volume(reset_view=True))
        self.mode_panel.slices_clicked.connect(lambda: self.render_slices(reset_view=True))
        self.mode_panel.iso_clicked.connect(self.render_isosurface_auto)
        self.mode_panel.clear_clicked.connect(self.clear_view)
        self.mode_panel.reset_camera_clicked.connect(self.reset_camera)
        layout.addWidget(self.mode_panel)

        # Parameters Panel
        self.params_panel = RenderingParametersPanel()
        for signal in [self.params_panel.solid_color_changed,
                       self.params_panel.light_angle_changed,
                       self.params_panel.coloring_mode_changed,
                       self.params_panel.threshold_changed,
                       self.params_panel.slice_position_changed]:
            signal.connect(self.trigger_render)
        self.params_panel.render_style_changed.connect(self._on_render_style_changed)

        self.params_panel.opacity_changed.connect(lambda: self.render_volume(reset_view=False))
        self.params_panel.clim_changed.connect(self._on_clim_changed_fast)
        self.params_panel.colormap_changed.connect(self._on_colormap_changed)
        self.params_panel.apply_clim_clip.connect(self._on_apply_clim_clip)
        self.params_panel.invert_volume.connect(self._on_invert_volume)
        layout.addWidget(self.params_panel)

        # Clip Panel - signals connected later in __init__ after clip_handler is created
        self.clip_panel = ClipPlanePanel()
        layout.addWidget(self.clip_panel)

        # ROI Panel - signals connected later in __init__ after roi_handler is created
        self.roi_panel = ROIPanel()
        layout.addWidget(self.roi_panel)

        layout.addStretch(1)
        scroll_area.setWidget(panel)
        return scroll_area

    def _create_info_panel(self) -> QWidget:
        """Create right sidebar with global scroll area."""
        scroll_area = QScrollArea()
        scroll_area.setWidgetResizable(True)
        scroll_area.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll_area.setVerticalScrollBarPolicy(Qt.ScrollBarAsNeeded)

        panel = QWidget()
        self.info_panel_layout = QVBoxLayout(panel)
        layout = self.info_panel_layout
        layout.setContentsMargins(*PANEL_MARGIN)
        layout.setSpacing(PANEL_SPACING)
        
        # Title
        title = QLabel("Information")
        title.setFont(QFont("Arial", 16, QFont.Bold))
        title.setAlignment(Qt.AlignCenter)
        layout.addWidget(title)
        layout.addWidget(self._create_separator())
        
        # Default Info Panel
        self.info_panel = InfoPanel()
        info_wrapper = CollapsiblePanel(
            title=self._resolve_panel_title(self.info_panel),
            content=self.info_panel,
            expanded=True,
        )
        layout.addWidget(info_wrapper)
        
        layout.addStretch(1)
        scroll_area.setWidget(panel)
        
        return scroll_area

    @staticmethod
    def _resolve_panel_title(panel: QWidget) -> str:
        for attr in ("custom_title", "windowTitle", "title"):
            candidate = getattr(panel, attr, None)
            if callable(candidate):
                try:
                    value = candidate()
                except Exception:
                    value = None
            else:
                value = candidate
            if isinstance(value, str) and value.strip():
                return value.strip()
        return panel.__class__.__name__

    def add_custom_panel(self, panel: QWidget, index: int = 2, side: str = 'left'):
        """Add a custom panel to either the left or right sidebar."""
        if side == 'left' and hasattr(self, 'control_panel_layout'):
            self.control_panel_layout.insertWidget(index, panel)
        elif side == 'right' and hasattr(self, 'info_panel_layout'):
            wrapped_panel = panel
            if not isinstance(panel, CollapsiblePanel):
                title = self._resolve_panel_title(panel)
                wrapped_panel = CollapsiblePanel(title=title, content=panel, expanded=True)

            # Insert before the stretch (last item)
            count = self.info_panel_layout.count()
            # If there is a stretch, insert before it. 
            # The stretch is usually the last item.
            if count > 0:
                self.info_panel_layout.insertWidget(count - 1, wrapped_panel)
            else:
                self.info_panel_layout.addWidget(wrapped_panel)

    def _create_separator(self):
        line = QFrame()
        line.setFrameShape(QFrame.HLine)
        line.setFrameShadow(QFrame.Sunken)
        return line

    def trigger_render(self):
        self.update_timer.start()

    def _perform_delayed_render(self):
        mode = self.render_engine.active_view_mode
        if mode == 'volume':
            self.render_volume(reset_view=False)
        elif mode == 'slices':
            self.render_slices(reset_view=False)
        elif mode == 'iso':
            self.render_isosurface_auto(reset_view=False)
        elif mode == 'mesh':
            self.render_mesh(reset_view=False)

    # ==========================================
    # BaseVisualizer Interface
    # ==========================================

    def set_data(self, data: VolumeData, reset_camera: bool = False):
        """Set data and delegate to RenderEngine."""
        previous_mode = self.active_view_mode
        self.render_engine.set_data(data)
        
        # Sync with ROIHandler
        if hasattr(self, 'roi_handler'):
            self.roi_handler.set_data(data, self.render_engine.grid)
        
        d_type = data.metadata.get('Type', 'Unknown')

        if data.has_mesh:
            self.update_status(f"Loaded Mesh: {d_type}")
            self.render_mesh(reset_view=reset_camera)
            self.info_panel.update_info(d_type, (0, 0, 0), data.spacing, data.metadata)
        elif data.raw_data is not None:
            self.update_status(f"Loaded Volume: {d_type}")

            is_processed = ("Processed" in d_type)
            default_color = "red" if is_processed else "ivory"
            idx = self.params_panel.solid_color_combo.findText(default_color)
            if idx >= 0:
                with QSignalBlocker(self.params_panel.solid_color_combo):
                    self.params_panel.solid_color_combo.setCurrentIndex(idx)
            
            # Update Histogram
            self._update_histogram()

            min_val = np.nanmin(data.raw_data)
            max_val = np.nanmax(data.raw_data)
            self.params_panel.set_data_range(min_val, max_val)

            dims = self.render_engine.grid.dimensions
            self.params_panel.set_slice_limits(dims[0] - 1, dims[1] - 1, dims[2] - 1)
            self.params_panel.set_slice_defaults(dims[0] // 2, dims[1] // 2, dims[2] // 2)

            # Keep user's current volumetric view mode during time-step updates.
            target_mode = 'volume'
            if not reset_camera and previous_mode in {'volume', 'slices', 'iso'}:
                target_mode = previous_mode

            if target_mode == 'slices':
                self.render_slices(reset_view=False)
            elif target_mode == 'iso':
                self.render_isosurface_auto(reset_view=False)
            else:
                self.render_volume(reset_view=reset_camera)
            self.info_panel.update_info(d_type, data.dimensions, data.spacing, data.metadata)

    def _update_histogram(self):
        """Calculate and update histogram in params panel using DataManager."""
        if not self._data_manager:
            return
            
        try:
            hist, bins = self._data_manager.calculate_histogram(bins=100, sample_step=4)
            if hist.size > 0:
                self.params_panel.set_histogram_data(hist, bins)
        except Exception as e:
            print(f"Histogram error: {e}")

    @property
    def data(self) -> Optional[VolumeData]:
        return self.render_engine.data if hasattr(self, 'render_engine') else None

    @property
    def grid(self):
        return self.render_engine.grid if hasattr(self, 'render_engine') else None

    @property
    def mesh(self):
        return self.render_engine.mesh if hasattr(self, 'render_engine') else None

    @property
    def active_view_mode(self):
        return self.render_engine.active_view_mode if hasattr(self, 'render_engine') else None

    def show(self):
        super().show()
        self.plotter.app_window.show()

    def update_status(self, message: str):
        self.status_bar.showMessage(message)

    def set_data_manager(self, data_manager):
        self._data_manager = data_manager
        # Update ROIHandler's data manager reference
        if hasattr(self, 'roi_handler'):
            self.roi_handler._data_manager = data_manager

    # ==========================================
    # Helper Methods for Data Operations
    # ==========================================
    
    def _clear_render_caches(self):
        """Clear all render engine caches and explicitly release VTK resources."""
        self.plotter.clear()
        if hasattr(self, 'render_engine'):
            # --- Explicit VTK C++ resource release ---
            # Drop Python-side actor reference so the ref-count hits zero and
            # VTK's C++ destructor can free GPU/CPU memory without waiting for gc.
            actor = getattr(self.render_engine, 'volume_actor', None)
            if actor is not None:
                try:
                    mapper = actor.GetMapper()
                    if mapper:
                        mapper.RemoveAllInputs()
                except Exception:
                    pass

            self.render_engine.data              = None
            self.render_engine.grid              = None
            self.render_engine._cached_vol_grid  = None
            self.render_engine._lod_pyramid      = None
            self.render_engine._iso_cache        = {}
            self.render_engine.volume_actor      = None
            self.render_engine.iso_actor         = None
            self.render_engine._current_iso_threshold = None
    
    def _refresh_view(self):
        """Re-render current view mode."""
        if self.active_view_mode == 'volume':
            self.render_volume(reset_view=True)
        elif self.active_view_mode == 'slices':
            self.render_slices(reset_view=True)
        elif self.active_view_mode == 'iso':
            self.render_isosurface_auto(reset_view=True)
        elif self.active_view_mode == 'mesh':
            self.render_mesh(reset_view=True)
        else:
            self.render_volume(reset_view=True)

    # ==========================================
    # Rendering Delegation
    # ==========================================

    def clear_view(self):
        self.render_engine.clear_view()

    def reset_camera(self):
        self.render_engine.reset_camera()

    def render_mesh(self, reset_view=True):
        self.render_engine.render_mesh(reset_view=reset_view)

    def render_volume(self, reset_view=True):
        self.render_engine.render_volume(reset_view=reset_view)

    def render_slices(self, reset_view=True):
        self.render_engine.render_slices(reset_view=reset_view)

    def render_isosurface_auto(self, reset_view=True):
        self.render_engine.render_isosurface_auto(reset_view=reset_view)

    def render_isosurface(self, threshold=300, reset_view=True):
        self.render_engine.render_isosurface(threshold=threshold, reset_view=reset_view)

    def _sync_render_style_ui(self):
        """Sync render-style combo to engine state without emitting UI signals."""
        if not hasattr(self, 'params_panel') or not hasattr(self, 'render_engine'):
            return
        combo = self.params_panel.render_style_combo
        desired = self.render_engine.get_current_render_mode_label()
        index = combo.findText(desired)
        if index >= 0 and combo.currentIndex() != index:
            with QSignalBlocker(combo):
                combo.setCurrentIndex(index)

    def _on_render_style_changed(self, render_style: str):
        """
        Handle render-style requests with central state + guard clauses.
        Falls back to full render only when no live actor is available.
        """
        result = self.render_engine.request_render_mode_change(render_style)
        if result == 'unchanged':
            return
        if self.active_view_mode == 'iso' and result != 'applied':
            self.render_isosurface_auto(reset_view=False)

    # ==========================================
    # Clip Plane Methods
    # ==========================================

    def _on_colormap_changed(self, text):
        if self.active_view_mode == 'volume':
            self.render_volume(reset_view=False)
        else:
            self.trigger_render()

    def _on_clim_changed_fast(self):
        """Fast colormap range update without full re-render."""
        if self.active_view_mode == 'volume':
            clim = self.params_panel.get_current_values().get('clim', [0, 1000])
            # Try fast update first, fall back to full render if needed
            if not self.render_engine.update_clim_fast(clim):
                self.render_volume(reset_view=False)
        elif self.active_view_mode == 'slices':
            # Slices also use clim, trigger delayed render
            self.trigger_render()

    def _on_apply_clim_clip(self, clim: list):
        """Apply colormap range as permanent data clip (delegates to DataManager)."""
        if not hasattr(self, '_data_manager') or not self._data_manager:
            self.update_status("No data to clip")
            return

        import traceback

        min_val, max_val = float(clim[0]), float(clim[1])

        try:
            # Step 1: Clear render caches
            self.update_status("Clipping: Clearing caches...")
            self._clear_render_caches()

            # Step 2: Delegate data manipulation to DataManager.
            # Use a weakref so the callback does not keep the window alive if
            # the UI is closed while a long clip operation is running.
            _self_ref = weakref.ref(self)

            def progress_cb(percent, msg):
                win = _self_ref()
                if win is not None:
                    win.update_status(f"Clipping: {msg}")

            self._data_manager.clip_data(min_val, max_val, progress_callback=progress_cb)

            # Step 3: Update UI
            self.update_status("Clipping: Updating UI...")
            self.params_panel.set_data_range(int(min_val), int(max_val))

            # Step 4: Recreate rendering grid
            self.update_status("Clipping: Recreating grid...")
            self.render_engine.set_data(self._data_manager.active_data)

            # Step 5: Update histogram
            self._update_histogram()

            # Step 6: Re-render
            self._refresh_view()

            self.update_status(f"Applied clip: [{min_val:.0f}, {max_val:.0f}]")

        except ValueError as e:
            self.update_status(str(e))
        except MemoryError as e:
            print(f"[RangeClip] Memory Error: {e}")
            print(traceback.format_exc())
            self.update_status("Memory Error during clip")
        except Exception as e:
            print(f"[RangeClip] Error: {type(e).__name__}: {e}")
            print(traceback.format_exc())
            self.update_status(f"Clip error: {type(e).__name__}")

    def _on_invert_volume(self):
        """Invert volume values (delegates to DataManager)."""
        if not hasattr(self, '_data_manager') or not self._data_manager:
            self.update_status("No data to invert")
            return

        import traceback

        try:
            # Step 1: Clear render caches
            self.update_status("Inverting: Clearing caches...")
            self._clear_render_caches()

            # Step 2: Get current clim before inversion (for UI update)
            data = self._data_manager.active_data
            current_clim = self.params_panel.get_current_values().get('clim', [0, 1000])
            old_min_clim, old_max_clim = current_clim[0], current_clim[1]

            # Step 3: Delegate data inversion to DataManager.
            # Use weakref to avoid keeping the window alive through the closure.
            _self_ref = weakref.ref(self)

            def progress_cb(percent, msg):
                win = _self_ref()
                if win is not None:
                    win.update_status(f"Inverting: {msg}")

            data_min, data_max, invert_offset = self._data_manager.invert_data(progress_callback=progress_cb)

            # Step 4: Calculate inverted clim values for UI
            new_min_clim = int(invert_offset - old_max_clim)
            new_max_clim = int(invert_offset - old_min_clim)

            # Step 5: Update params panel
            self.update_status("Inverting: Updating UI...")
            self.params_panel.set_data_range(int(data_min), int(data_max))
            self.params_panel.block_signals(True)
            self.params_panel.slider_clim_min.setValue(new_min_clim)
            self.params_panel.slider_clim_max.setValue(new_max_clim)
            self.params_panel.spinbox_clim_min.setValue(new_min_clim)
            self.params_panel.spinbox_clim_max.setValue(new_max_clim)
            self.params_panel.block_signals(False)

            # Step 6: Recreate rendering grid
            self.update_status("Inverting: Recreating grid...")
            self.render_engine.set_data(self._data_manager.active_data)

            # Step 7: Update histogram
            self._update_histogram()

            # Step 8: Re-render
            self._refresh_view()

            self.update_status(f"Volume inverted. Clim: [{new_min_clim}, {new_max_clim}]")

        except ValueError as e:
            self.update_status(str(e))
        except Exception as e:
            print(f"[InvertVolume] Error: {type(e).__name__}: {e}")
            print(traceback.format_exc())
            self.update_status(f"Invert error: {type(e).__name__}")


    # ==========================================
    # Mouse Probe
    # ==========================================

    def _setup_mouse_probe(self):
        self.plotter.iren.add_observer("MouseMoveEvent", self._on_mouse_move)

    def _on_mouse_move(self, obj, event):
        if self.data is None or self.active_view_mode != 'slices':
            return

        try:
            if hasattr(obj, "GetEventPosition"):
                pos = obj.GetEventPosition()
            elif hasattr(obj, "get_event_position"):
                pos = obj.get_event_position()
            else:
                pos = self.plotter.iren.GetEventPosition()

            import vtk
            picker = vtk.vtkPointPicker()
            picker.Pick(pos[0], pos[1], 0, self.plotter.renderer)

            if picker.GetViewProp() is None or self.data.raw_data is None:
                return

            world_pos = picker.GetPickPosition()
            ox, oy, oz = self.data.origin
            sx, sy, sz = self.data.spacing

            raw_z = int(round((world_pos[0] - ox) / sx))
            raw_y = int(round((world_pos[1] - oy) / sy))
            raw_x = int(round((world_pos[2] - oz) / sz))

            shape = self.data.raw_data.shape
            if 0 <= raw_z < shape[0] and 0 <= raw_y < shape[1] and 0 <= raw_x < shape[2]:
                val = self.data.raw_data[raw_z, raw_y, raw_x]
                self.status_bar.showMessage(
                    f"馃搷 Pos: ({world_pos[0]:.1f}, {world_pos[1]:.1f}, {world_pos[2]:.1f}) | "
                    f"Indices: [{raw_z}, {raw_y}, {raw_x}] | 馃挕 HU: {val:.1f}"
                )
        except Exception:
            pass


--- FILE: .\gui\styles.py ---
"""
Shared styles and CSS constants for the GUI components.
"""

# ==========================================
# Panel Title Styles
# ==========================================
PANEL_TITLE_STYLE = """
    font-size: 20px; 
    font-weight: bold; 
    margin-top: 15px; 
    margin-bottom: 5px;
    padding: 5px;
"""

# ==========================================
# Table Styles
# ==========================================
TABLE_STYLESHEET = """
    QTableWidget {
        background-color: transparent;
        gridline-color: #666666;
        font-size: 15px;
        border: 1px solid #555555;
    }
    QTableWidget::item {
        padding: 10px;
        border: 1px solid #444444;
    }
    QHeaderView::section {
        background-color: transparent;
        font-weight: bold;
        padding: 10px;
        border: 1px solid #555555;
        font-size: 15px;
    }
"""

# ==========================================
# Button Styles
# ==========================================
PRIMARY_BUTTON_STYLE = """
    QPushButton {
        background-color: #4a90d9;
        color: white;
        border: none;
        padding: 8px 16px;
        border-radius: 4px;
        font-weight: bold;
    }
    QPushButton:hover {
        background-color: #5a9fe9;
    }
    QPushButton:pressed {
        background-color: #3a80c9;
    }
"""

SECONDARY_BUTTON_STYLE = """
    QPushButton {
        background-color: #555555;
        color: white;
        border: none;
        padding: 8px 16px;
        border-radius: 4px;
    }
    QPushButton:hover {
        background-color: #666666;
    }
"""

# ==========================================
# Slider Styles
# ==========================================
SLIDER_STYLESHEET = """
    QSlider::groove:horizontal {
        height: 8px;
        background: #444444;
        border-radius: 4px;
    }
    QSlider::handle:horizontal {
        background: #4a90d9;
        width: 16px;
        margin: -4px 0;
        border-radius: 8px;
    }
"""


--- FILE: .\gui\ui_constants.py ---
"""
Shared UI spacing and sizing constants for consistent panel layout.
"""

from __future__ import annotations

from PyQt5.QtWidgets import QLabel, QLayout, QSizePolicy


# Panel-level layout defaults
PANEL_MARGIN = (10, 10, 10, 10)
PANEL_SPACING = 8

# GroupBox internal layout defaults
GROUP_MARGIN = (8, 16, 8, 8)
GROUP_SPACING = 8

# Table behavior defaults
TABLE_MIN_HEIGHT = 150


def apply_panel_layout(layout: QLayout) -> None:
    layout.setContentsMargins(*PANEL_MARGIN)
    layout.setSpacing(PANEL_SPACING)


def apply_group_layout(layout: QLayout) -> None:
    layout.setContentsMargins(*GROUP_MARGIN)
    layout.setSpacing(GROUP_SPACING)


def make_description_label(label: QLabel) -> QLabel:
    label.setWordWrap(True)
    label.setMinimumWidth(100)
    return label


def set_primary_button_policy(button) -> None:
    button.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)


--- FILE: .\gui\__init__.py ---
"""
GUI Package for Porous Media Analysis Suite.
Exports all panel classes and MainWindow.
"""

from gui.styles import (
    PANEL_TITLE_STYLE, 
    TABLE_STYLESHEET,
    PRIMARY_BUTTON_STYLE,
    SECONDARY_BUTTON_STYLE
)

from gui.panels.info_panel import InfoPanel, StatisticsPanel
from gui.panels.mode_panel import VisualizationModePanel
from gui.panels.params_panel import RenderingParametersPanel
from gui.panels.clip_panel import ClipPlanePanel
from gui.panels.roi_panel import ROIPanel
from gui.panels.processing_panel import StructureProcessingPanel
from gui.panels.timeseries_panel import TimeSeriesControlPanel, TrackingAnalysisPanel
from gui.main_window import MainWindow

__all__ = [
    # Styles
    'PANEL_TITLE_STYLE',
    'TABLE_STYLESHEET',
    'PRIMARY_BUTTON_STYLE',
    'SECONDARY_BUTTON_STYLE',
    # Panels
    'InfoPanel',
    'StatisticsPanel',
    'VisualizationModePanel',
    'RenderingParametersPanel',
    'ClipPlanePanel',
    'ROIPanel',
    'StructureProcessingPanel',
    'TimeSeriesControlPanel',
    'TrackingAnalysisPanel',
    # Window
    'MainWindow',
]


--- FILE: .\gui\handlers\timeseries_handler.py ---
from typing import List, Optional, Dict
from PyQt5.QtWidgets import QFileDialog, QMessageBox
from PyQt5.QtCore import QObject

from core import VolumeData
from loaders import TimeSeriesDicomLoader
from loaders.time_series import TimeSeriesOrderDialog
from processors import PNMTracker, PoreToSphereProcessor, PoreExtractionProcessor
from data import get_timeseries_pnm_cache

class TimeseriesHandler(QObject):
    """
    Handler for 4D CT Time Series operations.
    Manages loading, tracking, and visualization of time-series data.
    """
    

    def __init__(self, main_controller):
        super().__init__()
        self.controller = main_controller
        self.visualizer = main_controller.visualizer
        self.panel = main_controller.panel
        self.control_panel = main_controller.timeseries_control
        self.analysis_panel = main_controller.tracking_analysis
        self.stats_panel = main_controller.stats_panel
        self.data_manager = main_controller.data_manager

        
        # State
        self._volumes: List[VolumeData] = []
        self._pnm_result = None
        self._reference_mesh = None    # t=0 PNM mesh
        self._reference_snapshot = None # t=0 PNM snapshot
        self._current_cache_key = None  # Current cache key for PNM data
        self._pores_cache: Dict[int, VolumeData] = {}  # Cache for extracted pores at each timepoint
        self._current_timepoint = 0     # Current timepoint index
        
        # Helper processor for on-demand mesh generation
        self._sphere_processor = PoreToSphereProcessor()
        self._pore_processor = PoreExtractionProcessor()
        
        # Cache manager
        self._pnm_cache = get_timeseries_pnm_cache()
        
        # Connect analysis panel pore selection signal
        if hasattr(self.analysis_panel, 'pore_selected'):
            self.analysis_panel.pore_selected.connect(self._on_pore_selected)

    @property
    def has_volumes(self) -> bool:
        return bool(self._volumes)

    def load_series(self, strategy=None):
        """Load 4D CT series workflow."""
        from loaders import SmartDicomLoader
        folder = QFileDialog.getExistingDirectory(
            self.visualizer, 
            "Select 4D CT Parent Folder (containing t0, t1, t2... subfolders)"
        )
        if not folder:
            return
        
        # Clear old PNM cache when loading new series
        if self._current_cache_key:
            print("[TimeseriesHandler] Clearing old PNM cache before loading new series")
            # Keep the cache for potential reuse if user switches back
            # Only clear if it's a different dataset
        
        sort_mode = self.panel.sort_combo.currentText().lower()
        
        # Use existing loaders by passing a SmartDicomLoader with desired strategy
        inner_loader = SmartDicomLoader(strategy=strategy)
        loader = TimeSeriesDicomLoader(loader=inner_loader)
        
        manual_order = None
        
        # Manual sorting dialog
        if sort_mode == 'manual':
            folder_list = loader.get_folder_list(folder, sort_mode='alphabetical')
            if not folder_list:
                QMessageBox.warning(self.visualizer, "No Data", 
                                   "No DICOM subfolders found in selected folder.")
                return
            
            manual_order = TimeSeriesOrderDialog.get_order(self.visualizer, folder_list)
            if manual_order is None:
                return
        
        # Execute loading
        progress = self.controller._create_progress_dialog("Loading 4D CT time series...")
        callback = self.controller._make_progress_callback(progress)
        
        try:
            self.visualizer.update_status("Loading 4D CT time series...")
            
            self._volumes = loader.load_series(
                folder, 
                sort_mode=sort_mode,
                manual_order=manual_order,
                callback=callback
            )
            
            if not self._volumes:
                raise ValueError("No timepoints loaded")
            
            self._initialize_series()

            annotation_msg = ""
            annotation_report = self._summarize_sim_annotation_validation()
            if annotation_report is not None:
                annotation_msg = (
                    f"\n\nAnnotation checks:\n"
                    f"- Errors: {annotation_report['error_count']}\n"
                    f"- Warnings: {annotation_report['warning_count']}"
                )
            
            self.controller._show_msg(
                "4D CT Loaded", 
                f"Loaded {len(self._volumes)} timepoints.\n"
                f"Sorting: {sort_mode.title()}\n"
                f"Use timeline slider (left) to navigate volumes."
                f"{annotation_msg}"
            )
            self.visualizer.update_status(f"4D CT: {len(self._volumes)} timepoints loaded.")
            
        except Exception as e:
            self.controller._show_err("4D CT Loading Error", e)
        finally:
            progress.close()

    def _initialize_series(self):
        """Initialize UI and state after loading."""
        # Load t=0
        first_vol = self._volumes[0]
        self.data_manager.load_raw_data(first_vol)
        self.visualizer.set_data(first_vol)
        self.panel.set_threshold(-300)
        self.stats_panel.update_statistics(first_vol.metadata)

        
        # Setup timeline (left side)
        names = [v.metadata.get('folder_name', f't={i}') for i, v in enumerate(self._volumes)]
        self.control_panel.set_range(len(self._volumes), folder_names=names)
        
        # Reset analysis (right side)
        self.analysis_panel.reset()
        
        # Reset tracking state
        self._pnm_result = None
        self._reference_mesh = None
        self._reference_snapshot = None
        self._current_cache_key = None
        self._pores_cache.clear()  # Clear pores cache when loading new series

    def _summarize_sim_annotation_validation(self):
        """Aggregate annotation validation results from loader metadata."""
        if not self._volumes:
            return None

        series_report = self._volumes[0].metadata.get("sim_annotation_series")
        if not isinstance(series_report, dict):
            return None
        validation = series_report.get("validation", {})
        return {
            "ok": bool(validation.get("ok", True)),
            "error_count": int(validation.get("error_count", 0)),
            "warning_count": int(validation.get("warning_count", 0)),
            "errors": list(validation.get("errors", [])),
            "warnings": list(validation.get("warnings", [])),
        }

    def track_pores(self):
        """Execute pore tracking workflow."""
        if not self._volumes:
            QMessageBox.warning(self.visualizer, "No 4D CT Data", 
                               "Please load a 4D CT series first.")
            return
        
        thresh = self.panel.get_threshold()
        
        # Generate cache key for this configuration
        cache_key = self._pnm_cache.generate_key(self._volumes, thresh)
        
        # Check if we have cached results
        cached_data = self._pnm_cache.get(cache_key)
        if cached_data:
            self._pnm_result, self._reference_mesh, self._reference_snapshot = cached_data
            self._current_cache_key = cache_key
            
            # Update UI
            self.analysis_panel.set_time_series(self._pnm_result)
            self.visualizer.set_data(self._reference_mesh)
            self.stats_panel.update_statistics(self._reference_mesh.metadata)
            
            self._show_tracking_summary()
            self.visualizer.update_status("4D CT PNM loaded from cache.")
            return
        
        # No cache, proceed with tracking
        progress = self.controller._create_progress_dialog("Tracking pores across timepoints...")
        
        try:
            tracker = PNMTracker()
            total = len(self._volumes)
            
            # Phase 1: Tracking
            for i, volume in enumerate(self._volumes):
                progress.setValue(int(80 * i / total))
                progress.setLabelText(f"Tracking timepoint {i+1}/{total}...")
                self.controller.app.processEvents()
                
                if progress.wasCanceled(): raise InterruptedError()
                
                # Only compute connectivity for t=0 (reference frame)
                # Subsequent frames inherit connectivity from reference
                compute_connections = (i == 0)
                snapshot = self._sphere_processor.extract_snapshot(
                    volume, threshold=thresh, time_index=i, 
                    compute_connectivity=compute_connections
                )
                
                if i == 0:
                    tracker.set_reference(snapshot)
                    self._reference_snapshot = snapshot
                else:
                    # Inherit connections from reference
                    snapshot.connections = self._reference_snapshot.connections
                    tracker.track_snapshot(snapshot)
            
            # Phase 2: Reference Mesh
            progress.setValue(85)
            progress.setLabelText("Generating reference PNM mesh...")
            self.controller.app.processEvents()
            
            self._reference_mesh = self._sphere_processor.process(self._volumes[0], threshold=thresh)
            self._pnm_result = tracker.get_results()

            progress.setValue(88)
            progress.setLabelText("Evaluating against simulation labels...")
            self.controller.app.processEvents()
            try:
                tracker.evaluate_against_sim_annotations(self._volumes)
                self._pnm_result = tracker.get_results()
            except Exception as eval_exc:
                print(f"[TimeseriesHandler] Simulation evaluation skipped: {eval_exc}")
            
            # Store in cache
            progress.setValue(90)
            progress.setLabelText("Caching PNM results...")
            self.controller.app.processEvents()
            
            self._pnm_cache.store(cache_key, self._pnm_result, 
                                 self._reference_mesh, self._reference_snapshot)
            self._current_cache_key = cache_key
            
            # Update UI
            progress.setValue(95)
            self.analysis_panel.set_time_series(self._pnm_result)
            self.visualizer.set_data(self._reference_mesh)
            self.stats_panel.update_statistics(self._reference_mesh.metadata)
            
            self._show_tracking_summary()

            
        except Exception as e:
            self.controller._show_err("4D CT Tracking Error", e)
        finally:
            progress.close()

    def set_timepoint(self, index: int):
        """Handle timepoint change request."""
        if not self._volumes or index >= len(self._volumes):
            return
        
        self._current_timepoint = index
        
        # Update analysis panel view
        self.analysis_panel.set_timepoint(index)
        
        # Check current view mode to maintain user's preference
        current_mode = self.visualizer.active_view_mode
        
        # If user is currently viewing PNM mesh and we have tracking data, update PNM view
        if current_mode == 'mesh' and self._reference_mesh and self._pnm_result:
            # Find current snapshot if available
            current_snapshot = None
            if index < len(self._pnm_result.snapshots):
                current_snapshot = self._pnm_result.snapshots[index]
                
            mesh = self._sphere_processor.create_time_varying_mesh(
                self._reference_mesh,
                self._reference_snapshot,
                self._pnm_result.tracking,
                index,
                current_snapshot=current_snapshot
            )
            # set_data will automatically call render_mesh, no need to call it again
            self.visualizer.set_data(mesh, reset_camera=False)
            self.stats_panel.update_statistics(mesh.metadata)
            self.visualizer.update_status(f"Viewing PNM at t={index} (connectivity from t=0)")
        
        # If user is viewing extracted pores (segmented data), show pores for this timepoint
        elif current_mode in ['volume', 'slices', 'iso'] and self.data_manager.has_segmented():
            # Check if we have cached pores for this timepoint
            if index in self._pores_cache:
                pores_data = self._pores_cache[index]
                self.visualizer.set_data(pores_data, reset_camera=False)
                self.stats_panel.update_statistics(pores_data.metadata)
                self.visualizer.update_status(f"Viewing pores at t={index} (cached)")
            else:
                # Extract pores for this timepoint
                thresh = self.panel.get_threshold()
                try:
                    pores_data = self._pore_processor.process(
                        self._volumes[index], 
                        threshold=thresh
                    )
                    # Cache the result
                    self._pores_cache[index] = pores_data
                    
                    self.visualizer.set_data(pores_data, reset_camera=False)
                    self.stats_panel.update_statistics(pores_data.metadata)
                    self.visualizer.update_status(f"Viewing pores at t={index}")
                except Exception as e:
                    print(f"[TimeseriesHandler] Failed to extract pores for t={index}: {e}")
                    # Fallback to raw volume
                    self.visualizer.set_data(self._volumes[index], reset_camera=False)
                    self.stats_panel.update_statistics(self._volumes[index].metadata)
                    self.visualizer.update_status(f"Viewing volume at t={index}")
        
        else:
            # Otherwise show raw volume (user is in volume/slices/iso mode without pores data)
            self.visualizer.set_data(self._volumes[index], reset_camera=False)
            self.stats_panel.update_statistics(self._volumes[index].metadata)
            self.visualizer.update_status(f"Viewing volume at t={index}")


    def _show_tracking_summary(self):
        summary = self._pnm_result.get_summary()
        eval_msg = ""
        eval_report = getattr(self._pnm_result.tracking, "evaluation", {})
        if isinstance(eval_report, dict) and eval_report.get("available"):
            overall = eval_report.get("overall", {})
            mean_voxel_iou = float(overall.get("mean_voxel_iou", 0.0))
            mean_instance_f1 = float(overall.get("mean_instance_f1", 0.0))
            mean_tracking_acc = float(overall.get("mean_tracking_accuracy", 0.0))
            eval_msg = (
                f"\n\nSimulation-label evaluation:\n"
                f"- Mean voxel IoU: {mean_voxel_iou:.1%}\n"
                f"- Mean instance F1: {mean_instance_f1:.1%}\n"
                f"- Mean tracking accuracy: {mean_tracking_acc:.1%}"
            )

        self.controller._show_msg(
            "4D CT Tracking Complete",
            f"Tracked {summary['reference_pores']} pores across {summary['num_timepoints']} timepoints.\n\n"
            f"鈥?Active pores: {summary['active_pores']}\n"
            f"鈥?Compressed pores: {summary['compressed_pores']}\n"
            f"鈥?Avg. volume retention: {summary['avg_volume_retention']:.1%}\n\n"
            f"Use the timeline (left) to navigate time steps.\n"
            f"Use the analysis table (right) to see pore details."
            f"{eval_msg}"
        )

    def _on_pore_selected(self, pore_id: int):
        """Handle pore selection from analysis panel for highlighting."""
        # Toggle highlight: if same pore is clicked, clear highlight
        current_highlight = getattr(self.visualizer, '_highlight_pore_id', None)
        
        if current_highlight == pore_id:
            # Clear highlight
            self.visualizer.set_highlight_pore(None)
            self.visualizer.update_status(f"Cleared pore highlight")
        else:
            # Set new highlight
            self.visualizer.set_highlight_pore(pore_id)
            self.visualizer.update_status(f"Highlighting pore {pore_id}")
        
        # Re-render to show/hide highlight
        if self.visualizer.active_view_mode == 'mesh':
            self.visualizer.render_mesh(reset_view=False)

    def cleanup(self):
        """Clean up resources and cache when closing or switching datasets."""
        # Note: We don't clear all cache here, as it might be useful for re-opening
        # Only clear the current session's state
        self._volumes.clear()
        self._pnm_result = None
        self._reference_mesh = None
        self._reference_snapshot = None
        self._current_cache_key = None
        self._pores_cache.clear()
        self._current_timepoint = 0
        
        # Clear PNM color cache in visualizer
        if hasattr(self.visualizer, 'clear_pnm_color_cache'):
            self.visualizer.clear_pnm_color_cache()
        
        # Optionally clear all PNM cache on explicit cleanup
        # Uncomment if you want to clear cache on every cleanup:
        # self._pnm_cache.clear()


--- FILE: .\gui\handlers\workflow_handler.py ---
from typing import Optional
from PyQt5.QtWidgets import QFileDialog, QMessageBox, QProgressDialog
from PyQt5.QtCore import QObject, Qt, QThread, pyqtSignal

from loaders import DummyLoader, SmartDicomLoader, LoadStrategy
from core import BaseProcessor
from processors import PoreExtractionProcessor

class ProcessorWorker(QThread):
    """Background worker for running processors without blocking UI."""
    finished = pyqtSignal(object)  # Emits VolumeData result
    error = pyqtSignal(str)        # Emits error message
    progress = pyqtSignal(int, str)  # Emits (percent, message)
    
    def __init__(self, processor: BaseProcessor, data, threshold: int):
        super().__init__()
        self.processor = processor
        self.data = data
        self.threshold = threshold
        self._cancelled = False
    
    def run(self):
        try:
            def progress_callback(percent, message):
                if not self._cancelled:
                    self.progress.emit(percent, message)
            
            result = self.processor.process(
                self.data, 
                callback=progress_callback, 
                threshold=self.threshold
            )
            
            if not self._cancelled:
                self.finished.emit(result)
        except Exception as e:
            self.error.emit(str(e))
    
    def cancel(self):
        self._cancelled = True


class WorkflowHandler(QObject):
    """
    Handler for Standard Workflow operations (Load, Process, Analyze).
    """
    
    def __init__(self, main_controller):
        super().__init__()
        self.controller = main_controller
        self.visualizer = main_controller.visualizer
        self.panel = main_controller.panel
        self.data_manager = main_controller.data_manager
        
        self.worker = None
        self.progress_dialog = None


    def load_dicom_dialog(self):
        """Load DICOM with full resolution."""
        if self.panel.load_4d_check.isChecked():
            self.controller.timeseries_handler.load_series(strategy=LoadStrategy.FULL)
        else:
            folder = QFileDialog.getExistingDirectory(self.visualizer, "Select Scan Series Folder")
            if folder:
                self._load_data(folder, strategy=None)

    def fast_load_dicom_dialog(self):
        """Load DICOM with fast/preview mode."""
        if self.panel.load_4d_check.isChecked():
            self.controller.timeseries_handler.load_series(strategy=LoadStrategy.FAST)
        else:
            folder = QFileDialog.getExistingDirectory(self.visualizer, "Select Scan Series Folder (Fast)")
            if folder:
                self._load_data(folder, strategy=LoadStrategy.FAST)


    def load_dummy_data(self):
        """Generate and load synthetic sample data."""
        progress = self.controller._create_progress_dialog("Generating synthetic sample...")
        callback = self.controller._make_progress_callback(progress)

        try:
            self.visualizer.update_status("Generating synthetic sample...")
            
            loader = DummyLoader()
            data = loader.load(128, callback=callback)

            self.data_manager.load_raw_data(data)
            self.visualizer.set_data(data)
            self.panel.set_threshold(500)
            
            self.controller._show_msg("Sample Loaded", "Synthetic sample ready.")
            self.visualizer.update_status("Ready.")
        except Exception as e:
            self.controller._show_err("Loading Error", e)
        finally:
            progress.close()

    def _load_data(self, folder_path: str, strategy: Optional[LoadStrategy] = None):
        """Executes data loading."""
        progress = self.controller._create_progress_dialog("Loading scan data...")
        callback = self.controller._make_progress_callback(progress)

        try:
            self.visualizer.update_status("Loading scan data...")
            
            loader = SmartDicomLoader(strategy=strategy)
            data = loader.load(folder_path, callback=callback)

            self.data_manager.load_raw_data(data)
            self.visualizer.set_data(data)
            self.panel.set_threshold(-300)
            
            mode = data.metadata.get('LoadStrategy', 'Auto')
            self.controller._show_msg("Scan Loaded", f"Loaded successfully.\nMode: {mode}")
            self.visualizer.update_status("Ready.")
        except Exception as e:
            self.controller._show_err("Loading Error", e)
        finally:
            progress.close()

    def auto_detect_threshold(self):
        """Run auto-threshold detection."""
        current_data = self.data_manager.active_data
        if not current_data or current_data.raw_data is None:
            QMessageBox.warning(self.visualizer, "No Data", "Please load a sample first.")
            return
        
        algorithm = self.panel.get_algorithm()
        self.visualizer.update_status(f"Calculating threshold ({algorithm})...")
        self.controller.app.processEvents()
        
        try:
            suggested = PoreExtractionProcessor.suggest_threshold(current_data, algorithm)
            self.panel.set_threshold(suggested)
            
            algo_display = algorithm.capitalize() if algorithm != 'auto' else 'Auto'
            self.visualizer.update_status(f"Threshold set to {suggested} HU ({algo_display})")
            self.controller._show_msg("Threshold Results", f"Threshold: {suggested} HU\nAlgorithm: {algo_display}")
        except Exception as e:
            self.controller._show_err("Threshold Detection Failed", e)

    def run_processor_async(self, processor, success_callback):
        """Run a processor in background."""
        current_data = self.data_manager.active_data
        if not current_data or current_data.raw_data is None:
            QMessageBox.warning(self.visualizer, "No Data", "Please load a sample first.")
            return

        thresh = self.panel.get_threshold()
        
        self.progress_dialog = self.controller._create_progress_dialog("Processing...")
        
        # Create and configure worker
        self.worker = ProcessorWorker(processor, current_data, thresh)
        
        def on_progress(percent, message):
            self.progress_dialog.setValue(percent)
            self.progress_dialog.setLabelText(message)
            self.visualizer.update_status(message)
        
        def on_finished(result):
            self.progress_dialog.close()
            self.visualizer.update_status("Ready.")
            success_callback(result)
        
        def on_error(msg):
            self.progress_dialog.close()
            self.controller._show_err("Processing Failed", Exception(msg))
        
        def on_cancel():
            self.worker.cancel()
            self.worker.wait()
            self.visualizer.update_status("Processing cancelled.")
        
        self.worker.progress.connect(on_progress)
        self.worker.finished.connect(on_finished)
        self.worker.error.connect(on_error)
        self.progress_dialog.canceled.connect(on_cancel)
        
        self.worker.start()


--- FILE: .\gui\panels\clip_panel.py ---
"""
Clip Plane Panel for interactive volume clipping.
"""

from PyQt5.QtWidgets import (
    QWidget, QVBoxLayout, QHBoxLayout, QLabel, 
    QGroupBox, QSlider, QCheckBox
)
from PyQt5.QtCore import Qt, pyqtSignal
from typing import Dict, Any

from gui.styles import PANEL_TITLE_STYLE
from gui.ui_constants import apply_group_layout, make_description_label


class ClipPlanePanel(QGroupBox):
    """
    Panel for interactive clip plane controls.
    """
    clip_changed = pyqtSignal()
    clip_toggled = pyqtSignal(bool)

    def __init__(self, title: str = "鉁傦笍 Clip Planes"):
        super().__init__()
        self.custom_title = title
        self._enabled = False
        self._init_ui()

    def _init_ui(self):
        layout = QVBoxLayout()
        apply_group_layout(layout)
        
        title_lbl = QLabel(self.custom_title)
        title_lbl.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(title_lbl)
        layout.addWidget(title_lbl)

        # Enable checkbox
        self.enable_checkbox = QCheckBox("Enable Clipping")
        self.enable_checkbox.stateChanged.connect(self._on_toggle)
        layout.addWidget(self.enable_checkbox)

        # X Clip
        x_layout = QHBoxLayout()
        x_layout.addWidget(QLabel("X:"))
        self.slider_x = QSlider(Qt.Horizontal)
        self.slider_x.setRange(0, 100)
        self.slider_x.setValue(50)
        self.slider_x.valueChanged.connect(self.clip_changed.emit)
        x_layout.addWidget(self.slider_x)
        self.invert_x = QCheckBox("Invert")
        self.invert_x.stateChanged.connect(self.clip_changed.emit)
        x_layout.addWidget(self.invert_x)
        layout.addLayout(x_layout)

        # Y Clip
        y_layout = QHBoxLayout()
        y_layout.addWidget(QLabel("Y:"))
        self.slider_y = QSlider(Qt.Horizontal)
        self.slider_y.setRange(0, 100)
        self.slider_y.setValue(50)
        self.slider_y.valueChanged.connect(self.clip_changed.emit)
        y_layout.addWidget(self.slider_y)
        self.invert_y = QCheckBox("Invert")
        self.invert_y.stateChanged.connect(self.clip_changed.emit)
        y_layout.addWidget(self.invert_y)
        layout.addLayout(y_layout)

        # Z Clip
        z_layout = QHBoxLayout()
        z_layout.addWidget(QLabel("Z:"))
        self.slider_z = QSlider(Qt.Horizontal)
        self.slider_z.setRange(0, 100)
        self.slider_z.setValue(50)
        self.slider_z.valueChanged.connect(self.clip_changed.emit)
        z_layout.addWidget(self.slider_z)
        self.invert_z = QCheckBox("Invert")
        self.invert_z.stateChanged.connect(self.clip_changed.emit)
        z_layout.addWidget(self.invert_z)
        layout.addLayout(z_layout)

        self.setLayout(layout)
        self._update_slider_state()

    def _on_toggle(self, state):
        self._enabled = (state == Qt.Checked)
        self._update_slider_state()
        self.clip_toggled.emit(self._enabled)

    def _update_slider_state(self):
        enabled = self._enabled
        self.slider_x.setEnabled(enabled)
        self.slider_y.setEnabled(enabled)
        self.slider_z.setEnabled(enabled)
        self.invert_x.setEnabled(enabled)
        self.invert_y.setEnabled(enabled)
        self.invert_z.setEnabled(enabled)

    def get_clip_values(self) -> Dict[str, Any]:
        return {
            'enabled': self._enabled,
            'x': self.slider_x.value() / 100.0,
            'y': self.slider_y.value() / 100.0,
            'z': self.slider_z.value() / 100.0,
            'invert_x': self.invert_x.isChecked(),
            'invert_y': self.invert_y.isChecked(),
            'invert_z': self.invert_z.isChecked(),
        }


--- FILE: .\gui\panels\info_panel.py ---
"""
Information panels for metadata and statistics display.
"""

from __future__ import annotations

import numpy as np
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
from PyQt5.QtCore import Qt
from PyQt5.QtWidgets import (
    QGroupBox,
    QHeaderView,
    QLabel,
    QSizePolicy,
    QTableWidget,
    QTableWidgetItem,
    QTabWidget,
    QVBoxLayout,
    QWidget,
)

from gui.styles import PANEL_TITLE_STYLE, TABLE_STYLESHEET
from gui.ui_constants import TABLE_MIN_HEIGHT, apply_group_layout, apply_panel_layout, make_description_label


class StatisticsPanel(QWidget):
    """
    Panel to display derived statistics and histograms for PNM analysis.
    """

    def __init__(self):
        super().__init__()
        self._init_ui()

    def _init_ui(self) -> None:
        layout = QVBoxLayout(self)
        apply_panel_layout(layout)

        title = QLabel("Statistics & Analysis")
        title.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(title)
        layout.addWidget(title)

        self.tabs = QTabWidget()
        # Keep chart area compact by default to avoid overly long bars/plots.
        self.tabs.setMinimumHeight(210)
        self.tabs.setMaximumHeight(280)
        self.tabs.setSizePolicy(QSizePolicy.Preferred, QSizePolicy.Fixed)

        self.fig_pore = Figure(figsize=(5, 3), dpi=80)
        self.canvas_pore = FigureCanvas(self.fig_pore)
        self.tabs.addTab(self.canvas_pore, "Pore Sizes")

        self.fig_throat = Figure(figsize=(5, 3), dpi=80)
        self.canvas_throat = FigureCanvas(self.fig_throat)
        self.tabs.addTab(self.canvas_throat, "Throat Sizes")
        layout.addWidget(self.tabs)

        self.stats_table = QTableWidget()
        self.stats_table.setColumnCount(2)
        self.stats_table.setHorizontalHeaderLabels(["Metric", "Value"])
        self.stats_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        self.stats_table.verticalHeader().setVisible(False)
        self.stats_table.setStyleSheet(TABLE_STYLESHEET)
        self.stats_table.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        self.stats_table.setMinimumHeight(TABLE_MIN_HEIGHT)
        layout.addWidget(self.stats_table)
        layout.addStretch(1)

    def _adjust_table_height(self, table: QTableWidget, max_height: int = 400) -> None:
        table.doItemsLayout()
        height = table.horizontalHeader().height()
        for i in range(table.rowCount()):
            height += table.rowHeight(i)
        height += 6
        clamped_height = max(TABLE_MIN_HEIGHT, min(height, max_height))
        table.setMinimumHeight(clamped_height)
        table.setMaximumHeight(max_height)
        table.setVerticalScrollBarPolicy(
            Qt.ScrollBarAsNeeded if height > max_height else Qt.ScrollBarAlwaysOff
        )

    def update_statistics(self, metadata) -> None:
        self.fig_pore.clear()
        self.fig_throat.clear()
        self.stats_table.setRowCount(0)

        pore_dist = metadata.get("PoreSizeDistribution", {})
        throat_dist = metadata.get("ThroatSizeDistribution", {})
        largest_pore = metadata.get("LargestPoreRatio", "N/A")
        throat_stats = metadata.get("ThroatStats", {})
        pore_count = metadata.get("PoreCount", 0)
        connection_count = metadata.get("ConnectionCount", 0)

        if pore_dist.get("bins") and pore_dist.get("counts"):
            ax = self.fig_pore.add_subplot(111)
            bins = np.array(pore_dist["bins"])
            counts = pore_dist["counts"]
            bin_centers = (bins[:-1] + bins[1:]) / 2
            ax.bar(
                bin_centers,
                counts,
                width=np.diff(bins) * 0.85,
                alpha=0.7,
                color="steelblue",
                edgecolor="black",
            )
            ax.set_xlabel("Pore Radius (mm)")
            ax.set_ylabel("Count")
            ax.set_title("Pore Size Distribution")
            ax.grid(True, alpha=0.3)
            ax.margins(x=0.02)
            self.fig_pore.tight_layout()

        if throat_dist.get("bins") and throat_dist.get("counts"):
            ax = self.fig_throat.add_subplot(111)
            bins = np.array(throat_dist["bins"])
            counts = throat_dist["counts"]
            bin_centers = (bins[:-1] + bins[1:]) / 2
            ax.bar(
                bin_centers,
                counts,
                width=np.diff(bins) * 0.85,
                alpha=0.7,
                color="indianred",
                edgecolor="black",
            )
            ax.set_xlabel("Throat Radius (mm)")
            ax.set_ylabel("Count")
            ax.set_title("Throat Size Distribution")
            ax.grid(True, alpha=0.3)
            ax.margins(x=0.02)
            self.fig_throat.tight_layout()

        self.canvas_pore.draw()
        self.canvas_throat.draw()

        stats_data = [
            ("Total Pores", str(pore_count)),
            ("Connections", str(connection_count)),
            ("Largest Pore %", str(largest_pore)),
        ]
        if throat_stats:
            stats_data.extend(
                [
                    ("Throat Min", f"{throat_stats.get('min', 0):.3f} mm"),
                    ("Throat Max", f"{throat_stats.get('max', 0):.3f} mm"),
                    ("Throat Mean", f"{throat_stats.get('mean', 0):.3f} mm"),
                ]
            )

        self.stats_table.setRowCount(len(stats_data))
        for row, (metric, value) in enumerate(stats_data):
            self.stats_table.setItem(row, 0, QTableWidgetItem(metric))
            self.stats_table.setItem(row, 1, QTableWidgetItem(value))
        self._adjust_table_height(self.stats_table, max_height=350)

    def clear(self) -> None:
        self.fig_pore.clear()
        self.fig_throat.clear()
        self.canvas_pore.draw()
        self.canvas_throat.draw()
        self.stats_table.setRowCount(0)


class InfoPanel(QGroupBox):
    """
    Panel to display source metadata and computed metrics.
    """

    def __init__(self, title: str = "Sample Information"):
        super().__init__()
        self.custom_title = title
        self._init_ui()

    def _init_ui(self) -> None:
        layout = QVBoxLayout()
        apply_group_layout(layout)

        title_lbl = QLabel(self.custom_title)
        title_lbl.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(title_lbl)
        layout.addWidget(title_lbl)

        self.info_table = QTableWidget()
        self.info_table.setColumnCount(2)
        self.info_table.setHorizontalHeaderLabels(["Property", "Value"])
        self.info_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        self.info_table.verticalHeader().setVisible(False)
        self.info_table.setAlternatingRowColors(True)
        self.info_table.setEditTriggers(QTableWidget.NoEditTriggers)
        self.info_table.setSelectionMode(QTableWidget.ExtendedSelection)
        self.info_table.setStyleSheet(TABLE_STYLESHEET)
        self.info_table.setShowGrid(True)
        self.info_table.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        self.info_table.setMinimumHeight(TABLE_MIN_HEIGHT)

        self.computed_label = QLabel("<b>Computed Metrics</b>")
        self.computed_label.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(self.computed_label)

        self.computed_table = QTableWidget()
        self.computed_table.setColumnCount(2)
        self.computed_table.setHorizontalHeaderLabels(["Metric", "Value"])
        self.computed_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        self.computed_table.verticalHeader().setVisible(False)
        self.computed_table.setAlternatingRowColors(True)
        self.computed_table.setEditTriggers(QTableWidget.NoEditTriggers)
        self.computed_table.setSelectionMode(QTableWidget.ExtendedSelection)
        self.computed_table.setStyleSheet(TABLE_STYLESHEET)
        self.computed_table.setShowGrid(True)
        self.computed_table.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        self.computed_table.setMinimumHeight(TABLE_MIN_HEIGHT)

        layout.addWidget(self.info_table)
        layout.addWidget(self.computed_label)
        layout.addWidget(self.computed_table)
        self.setLayout(layout)

    def _adjust_table_height(self, table: QTableWidget, max_height: int = 400) -> None:
        table.doItemsLayout()
        height = table.horizontalHeader().height()
        for i in range(table.rowCount()):
            height += table.rowHeight(i)
        height += 6
        clamped_height = max(TABLE_MIN_HEIGHT, min(height, max_height))
        table.setMinimumHeight(clamped_height)
        table.setMaximumHeight(max_height)
        table.setVerticalScrollBarPolicy(
            Qt.ScrollBarAsNeeded if height > max_height else Qt.ScrollBarAlwaysOff
        )

    def update_info(self, type_str: str, dim: tuple, spacing: tuple, metadata: dict) -> None:
        self.info_table.setRowCount(0)
        self.computed_table.setRowCount(0)

        basic_data = [("Data Type", type_str)]
        if dim != (0, 0, 0):
            basic_data.append(("Dimensions", f"{dim[0]} x {dim[1]} x {dim[2]}"))
        elif "MeshPoints" in metadata:
            basic_data.append(("Mesh Points", str(metadata["MeshPoints"])))
        basic_data.append(("Voxel Size", f"{spacing[0]:.3f} x {spacing[1]:.3f} x {spacing[2]:.3f} mm"))

        for key in ("Porosity", "PoreCount", "ConnectionCount"):
            if key in metadata:
                val = metadata[key]
                if isinstance(val, float) and key == "Porosity":
                    val = f"{val:.2f}%"
                basic_data.append((key, str(val)))

        self.info_table.setRowCount(len(basic_data))
        for row, (prop, val) in enumerate(basic_data):
            self.info_table.setItem(row, 0, QTableWidgetItem(prop))
            self.info_table.setItem(row, 1, QTableWidgetItem(val))
        self._adjust_table_height(self.info_table, max_height=300)

        computed = []
        if dim != (0, 0, 0):
            total_voxels = dim[0] * dim[1] * dim[2]
            computed.append(("Total Voxels", f"{total_voxels:,}"))
            phys_volume = dim[0] * spacing[0] * dim[1] * spacing[1] * dim[2] * spacing[2]
            computed.append(("Physical Volume", f"{phys_volume / 1000:.2f} cm3" if phys_volume > 1000 else f"{phys_volume:.2f} mm3"))
            phys_dim = (dim[0] * spacing[0], dim[1] * spacing[1], dim[2] * spacing[2])
            computed.append(("Physical Size", f"{phys_dim[0]:.1f} x {phys_dim[1]:.1f} x {phys_dim[2]:.1f} mm"))
            mem_mb = (total_voxels * 4) / (1024 * 1024)
            computed.append(("Memory (Est.)", f"{mem_mb:.1f} MB"))

        if "ThroatStats" in metadata:
            ts = metadata["ThroatStats"]
            if "mean" in ts:
                computed.append(("Avg Throat Radius", f"{ts['mean']:.3f} mm"))
            if "min" in ts and "max" in ts:
                computed.append(("Throat Range", f"{ts['min']:.3f} - {ts['max']:.3f} mm"))

        for key, label in (
            ("LargestPoreRatio", "Largest Pore %"),
            ("Permeability_mD", "Permeability"),
            ("Tortuosity", "Tortuosity"),
            ("CoordinationNumber", "Coord. Number"),
            ("ConnectedPoreFraction", "Connected Pores"),
        ):
            if key in metadata:
                value = metadata[key]
                if key == "Permeability_mD":
                    value = f"{value} mD"
                computed.append((label, str(value)))

        self.computed_table.setRowCount(len(computed))
        for row, (metric, val) in enumerate(computed):
            self.computed_table.setItem(row, 0, QTableWidgetItem(metric))
            self.computed_table.setItem(row, 1, QTableWidgetItem(val))
        self._adjust_table_height(self.computed_table, max_height=400)


--- FILE: .\gui\panels\mode_panel.py ---
"""
Visualization Mode Panel for selecting rendering modes.
"""

from PyQt5.QtWidgets import QWidget, QVBoxLayout, QPushButton, QLabel, QGroupBox, QFrame
from PyQt5.QtCore import pyqtSignal

from gui.styles import PANEL_TITLE_STYLE
from gui.ui_constants import apply_group_layout, make_description_label, set_primary_button_policy


class Separator(QFrame):
    """Simple horizontal line separator"""
    def __init__(self):
        super().__init__()
        self.setFrameShape(QFrame.HLine)
        self.setFrameShadow(QFrame.Sunken)


class VisualizationModePanel(QGroupBox):
    """
    Panel for selecting the Visualization Mode (Volume, Slices, Iso).
    """
    volume_clicked = pyqtSignal()
    slices_clicked = pyqtSignal()
    iso_clicked = pyqtSignal()
    clear_clicked = pyqtSignal()
    reset_camera_clicked = pyqtSignal()

    def __init__(self, title: str = "馃柤锔?Analysis Modes"):
        super().__init__()
        self.custom_title = title
        self._init_ui()

    def _init_ui(self):
        layout = QVBoxLayout()
        apply_group_layout(layout)
        
        title_lbl = QLabel(self.custom_title)
        title_lbl.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(title_lbl)
        layout.addWidget(title_lbl)
        
        self._add_button(layout, "馃搳 Volume Rendering", self.volume_clicked)
        self._add_button(layout, "馃敵 Orthogonal Slices", self.slices_clicked)
        self._add_button(layout, "馃彅锔?Isosurface (Solid/Pore)", self.iso_clicked)
        layout.addWidget(Separator())
        self._add_button(layout, "馃棏锔?Clear View", self.clear_clicked, min_height=35)
        self._add_button(layout, "馃帴 Reset Camera", self.reset_camera_clicked, min_height=35)
        self.setLayout(layout)

    def _add_button(self, layout, text, signal, min_height=40):
        btn = QPushButton(text)
        btn.setMinimumHeight(min_height)
        set_primary_button_policy(btn)
        btn.clicked.connect(signal.emit)
        layout.addWidget(btn)


--- FILE: .\gui\panels\params_panel.py ---
"""
Rendering Parameters Panel for controlling visualization settings.
"""

from PyQt5.QtWidgets import (
    QWidget, QVBoxLayout, QHBoxLayout, QLabel, 
    QGroupBox, QSlider, QComboBox, QSpinBox, QPushButton
)
from PyQt5.QtCore import Qt, pyqtSignal, QTimer
from typing import Dict, Any
import numpy as np
import pyqtgraph as pg

from gui.styles import PANEL_TITLE_STYLE
from gui.ui_constants import apply_group_layout, make_description_label, set_primary_button_policy
from config import (
    DEFAULT_COLORMAPS, 
    SOLID_COLORS, 
    OPACITY_PRESETS, 
    RENDER_STYLES, 
    COLORING_MODES
)


class RenderingParametersPanel(QGroupBox):
    """
    Panel for controlling fine-grained rendering parameters.
    """
    # Signals
    threshold_changed = pyqtSignal(int)
    coloring_mode_changed = pyqtSignal(str)
    colormap_changed = pyqtSignal(str)
    solid_color_changed = pyqtSignal(str)
    light_angle_changed = pyqtSignal(int)
    render_style_changed = pyqtSignal(str)
    opacity_changed = pyqtSignal(str)
    slice_position_changed = pyqtSignal()
    clim_changed = pyqtSignal()
    apply_clim_clip = pyqtSignal(list)  # Emits [min, max] for permanent clipping
    invert_volume = pyqtSignal()        # Emits signal to invert volume values

    def __init__(self, title: str = "馃帹 Rendering Parameters"):
        super().__init__()
        self.custom_title = title
        self.active_mode = None
        self._is_updating_programmatically = False
        self._init_ui()

    def _init_ui(self):
        layout = QVBoxLayout()
        apply_group_layout(layout)
        
        title_lbl = QLabel(self.custom_title)
        title_lbl.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(title_lbl)
        layout.addWidget(title_lbl)

        # 1. Threshold (Iso)
        self.lbl_threshold, self.threshold_slider, self.threshold_value_label = self._create_slider_group(
            layout, "Iso-Threshold (Intensity):", -1000, 2000, 300, self._on_threshold_change
        )

        # 2. Coloring Mode (Iso)
        self.lbl_coloring_mode = QLabel("Isosurface Coloring Mode:")
        layout.addWidget(self.lbl_coloring_mode)
        self.coloring_mode_combo = self._create_combo(layout,
                                                      COLORING_MODES,
                                                      [self.coloring_mode_changed.emit, self._update_visibility]
                                                      )

        # 3. Slice Sliders (Slices)
        self.lbl_slice_x, self.slider_slice_x, _ = self._create_slider_group(
            layout, "Slice X Position:", 0, 100, 50, lambda v: self.slice_position_changed.emit())
        self.lbl_slice_y, self.slider_slice_y, _ = self._create_slider_group(
            layout, "Slice Y Position:", 0, 100, 50, lambda v: self.slice_position_changed.emit())
        self.lbl_slice_z, self.slider_slice_z, _ = self._create_slider_group(
            layout, "Slice Z Position:", 0, 100, 50, lambda v: self.slice_position_changed.emit())

        # 4. Colormap Selection
        self.lbl_colormap = QLabel("Colormap:")
        layout.addWidget(self.lbl_colormap)
        self.colormap_combo = self._create_combo(layout,
                                                 DEFAULT_COLORMAPS,
                                                 [self.colormap_changed.emit]
                                                 )

        # 5. Colormap Range (CLim)
        self.lbl_clim = QLabel("Colormap Range (Min/Max):")
        layout.addWidget(self.lbl_clim)

        # Histogram Widget
        self.hist_widget = pg.PlotWidget()
        self.hist_widget.setBackground('w')
        self.hist_widget.setFixedHeight(120)
        self.hist_widget.setMouseEnabled(x=False, y=False)
        self.hist_widget.hideAxis('left')
        self.hist_widget.hideAxis('bottom')
        
        # Region Item (draggable area)
        self.hist_region = pg.LinearRegionItem()
        self.hist_region.setZValue(10)
        self.hist_region.sigRegionChanged.connect(self._on_histogram_region_changed)
        self.hist_widget.addItem(self.hist_region)
        
        # Mapping Curve (Ramp)
        self.mapping_curve = pg.PlotCurveItem(pen=pg.mkPen('r', width=3))
        self.hist_widget.addItem(self.mapping_curve)
        
        self.hist_data_bounds = (0, 100)
        self.hist_peak = 1.0
        self.hist_data_bounds = (0, 100)
        self.hist_peak = 1.0
        
        layout.addWidget(self.hist_widget)
        


        # Min row

        # Min row
        min_container = QWidget()
        min_layout = QHBoxLayout(min_container)
        min_layout.setContentsMargins(0, 0, 0, 0)
        self.lbl_clim_min = QLabel("Min:")
        min_layout.addWidget(self.lbl_clim_min)
        self.slider_clim_min = QSlider(Qt.Horizontal)
        self.slider_clim_min.setRange(0, 100)
        self.slider_clim_min.setValue(0)
        self.slider_clim_min.valueChanged.connect(self._on_clim_slider_change)
        min_layout.addWidget(self.slider_clim_min, stretch=1)
        self.spinbox_clim_min = QSpinBox()
        self.spinbox_clim_min.setRange(0, 100)
        self.spinbox_clim_min.setValue(0)
        self.spinbox_clim_min.valueChanged.connect(self._on_clim_spinbox_change)
        min_layout.addWidget(self.spinbox_clim_min)
        layout.addWidget(min_container)

        # Max row
        max_container = QWidget()
        max_layout = QHBoxLayout(max_container)
        max_layout.setContentsMargins(0, 0, 0, 0)
        self.lbl_clim_max = QLabel("Max:")
        max_layout.addWidget(self.lbl_clim_max)
        self.slider_clim_max = QSlider(Qt.Horizontal)
        self.slider_clim_max.setRange(0, 100)
        self.slider_clim_max.setValue(100)
        self.slider_clim_max.valueChanged.connect(self._on_clim_slider_change)
        max_layout.addWidget(self.slider_clim_max, stretch=1)
        self.spinbox_clim_max = QSpinBox()
        self.spinbox_clim_max.setRange(0, 100)
        self.spinbox_clim_max.setValue(100)
        self.spinbox_clim_max.valueChanged.connect(self._on_clim_spinbox_change)
        max_layout.addWidget(self.spinbox_clim_max)
        layout.addWidget(max_container)

        # Apply Clip button
        self.btn_apply_clim_clip = QPushButton("鉁傦笍 Apply Range Clip")
        self.btn_apply_clim_clip.setToolTip("Permanently clip data to current min/max range")
        set_primary_button_policy(self.btn_apply_clim_clip)
        self.btn_apply_clim_clip.clicked.connect(self._on_apply_clim_clip)
        layout.addWidget(self.btn_apply_clim_clip)

        # Invert Volume button
        self.btn_invert_volume = QPushButton("馃攧 Invert Volume")
        self.btn_invert_volume.setToolTip("Invert volume values (extract pore surfaces instead of object surfaces)")
        set_primary_button_policy(self.btn_invert_volume)
        self.btn_invert_volume.clicked.connect(self.invert_volume.emit)
        layout.addWidget(self.btn_invert_volume)

        # 6. Solid Color
        self.lbl_solid_color = QLabel("Solid Color:")
        layout.addWidget(self.lbl_solid_color)
        self.solid_color_combo = self._create_combo(layout, SOLID_COLORS, [self.solid_color_changed.emit])

        # 7. Light Angle
        self.lbl_light_angle, self.light_azimuth_slider, _ = self._create_slider_group(
            layout, "Light Source Angle (0-360掳):", 0, 360, 45, self.light_angle_changed.emit
        )

        # 8. Render Style
        self.lbl_render_style = QLabel("Render Style:")
        layout.addWidget(self.lbl_render_style)
        self.render_style_combo = self._create_combo(layout, RENDER_STYLES, [self.render_style_changed.emit])

        # 9. Opacity
        self.lbl_opacity = QLabel("Opacity Preset:")
        layout.addWidget(self.lbl_opacity)
        self.opacity_combo = self._create_combo(layout, OPACITY_PRESETS, [self.opacity_changed.emit])

        self.setLayout(layout)
        self._update_visibility()

    def _create_slider_group(self, layout, label_text, min_val, max_val, default, callback):
        container = QWidget()
        h_layout = QVBoxLayout(container)
        h_layout.setContentsMargins(0, 0, 0, 0)
        h_layout.setSpacing(2)

        label = QLabel(label_text)
        h_layout.addWidget(label)

        slider = QSlider(Qt.Horizontal)
        slider.setRange(min_val, max_val)
        slider.setValue(default)
        slider.valueChanged.connect(callback)
        h_layout.addWidget(slider)

        val_label = QLabel(f"{default}")
        val_label.setAlignment(Qt.AlignRight)
        h_layout.addWidget(val_label)

        layout.addWidget(container)
        return label, slider, val_label

    def _create_combo(self, layout, items, callbacks):
        combo = QComboBox()
        combo.addItems(items)
        for cb in callbacks:
            combo.currentTextChanged.connect(cb)
        layout.addWidget(combo)
        return combo

    def set_mode(self, mode: str):
        self.active_mode = mode
        self._update_visibility()

    def set_slice_limits(self, x_max: int, y_max: int, z_max: int):
        self.slider_slice_x.setMaximum(x_max)
        self.slider_slice_y.setMaximum(y_max)
        self.slider_slice_z.setMaximum(z_max)

    def set_slice_defaults(self, x: int, y: int, z: int):
        self.block_signals(True)
        self.slider_slice_x.setValue(x)
        self.slider_slice_y.setValue(y)
        self.slider_slice_z.setValue(z)
        self.block_signals(False)

    def set_data_range(self, min_val: int, max_val: int):
        self.block_signals(True)
        self.slider_clim_min.setRange(int(min_val), int(max_val))
        self.slider_clim_max.setRange(int(min_val), int(max_val))
        self.spinbox_clim_min.setRange(int(min_val), int(max_val))
        self.spinbox_clim_max.setRange(int(min_val), int(max_val))
        self.slider_clim_min.setValue(int(min_val))
        self.slider_clim_max.setValue(int(max_val))
        self.spinbox_clim_min.setValue(int(min_val))
        self.spinbox_clim_max.setValue(int(max_val))
        self.spinbox_clim_max.setValue(int(max_val))
        
        # Update histogram region if visible
        if hasattr(self, 'hist_region'):
            self.hist_region.setBounds([min_val, max_val])
            self.hist_region.setRegion([min_val, max_val])
            
        self.block_signals(False)

    def set_histogram_data(self, hist: np.ndarray, bins: np.ndarray):
        """Update histogram plot with new data."""
        self.hist_widget.clear()
        self.hist_widget.addItem(self.hist_region)
        self.hist_widget.addItem(self.mapping_curve)
        
        # Store bounds for mapping curve (ensure float)
        mn, mx = float(bins[0]), float(bins[-1])
        self.hist_data_bounds = (mn, mx)
        if hist.size > 0:
            self.hist_peak = float(hist.max())
        
        # Use stepMode=True which expects len(x) == len(y) + 1
        # bins has length N+1, hist has length N
        curve = pg.PlotCurveItem(bins, hist, stepMode=True, fillLevel=0, brush=(0, 0, 255, 100), pen='k')
        self.hist_widget.addItem(curve)
        
        # Set range (use float values to avoid pyqtgraph casting overflow warnings)
        self.hist_widget.setXRange(mn, mx, padding=0)
        self.hist_region.setBounds([mn, mx])
        
        # Update mapping curve
        self._update_mapping_curve()

    def _update_mapping_curve(self):
        """Update the visual ramp curve based on current region."""
        if not hasattr(self, 'mapping_curve'):
            return
            
        mn, mx = self.hist_region.getRegion()
        d_min, d_max = self.hist_data_bounds
        peak = self.hist_peak
        
        # Simple linear ramp from mn to mx
        x_vals = [d_min, mn, mx, d_max]
        y_vals = [0, 0, peak, peak]
        
        self.mapping_curve.setData(x_vals, y_vals)
    


    def block_signals(self, block: bool):
        self.slider_slice_x.blockSignals(block)
        self.slider_slice_y.blockSignals(block)
        self.slider_slice_z.blockSignals(block)
        self.slider_clim_min.blockSignals(block)
        self.slider_clim_max.blockSignals(block)
        self.spinbox_clim_min.blockSignals(block)
        self.spinbox_clim_max.blockSignals(block)

    def get_current_values(self) -> Dict[str, Any]:
        return {
            'threshold': self.threshold_slider.value(),
            'coloring_mode': self.coloring_mode_combo.currentText(),
            'colormap': self.colormap_combo.currentText(),
            'solid_color': self.solid_color_combo.currentText(),
            'light_angle': self.light_azimuth_slider.value(),
            'render_style': self.render_style_combo.currentText(),
            'opacity': self.opacity_combo.currentText(),
            'slice_x': self.slider_slice_x.value(),
            'slice_y': self.slider_slice_y.value(),
            'slice_z': self.slider_slice_z.value(),
            'clim': [self.slider_clim_min.value(), self.slider_clim_max.value()]
        }

    def _on_threshold_change(self, value):
        self.threshold_value_label.setText(f"Value: {value}")
        self.threshold_changed.emit(value)

    def _on_clim_slider_change(self, value):
        self.spinbox_clim_min.blockSignals(True)
        self.spinbox_clim_max.blockSignals(True)
        self.spinbox_clim_min.setValue(self.slider_clim_min.value())
        self.spinbox_clim_max.setValue(self.slider_clim_max.value())
        self.spinbox_clim_min.blockSignals(False)
        self.spinbox_clim_max.blockSignals(False)
        if self.slider_clim_min.value() > self.slider_clim_max.value():
            self.slider_clim_max.setValue(self.slider_clim_min.value())
            
        # Sync histogram region
        if not self._is_updating_programmatically:
            self.hist_region.setRegion([self.slider_clim_min.value(), self.slider_clim_max.value()])
            self._update_mapping_curve()
            
        self.clim_changed.emit()

    def _on_clim_spinbox_change(self, value):
        self.slider_clim_min.blockSignals(True)
        self.slider_clim_max.blockSignals(True)
        self.slider_clim_min.setValue(self.spinbox_clim_min.value())
        self.slider_clim_max.setValue(self.spinbox_clim_max.value())
        self.slider_clim_min.blockSignals(False)
        self.slider_clim_max.blockSignals(False)
        if self.spinbox_clim_min.value() > self.spinbox_clim_max.value():
            self.spinbox_clim_max.setValue(self.spinbox_clim_min.value())
            
        # Sync histogram region
        if not self._is_updating_programmatically:
            self.hist_region.setRegion([self.spinbox_clim_min.value(), self.spinbox_clim_max.value()])
            self._update_mapping_curve()
            
        self.clim_changed.emit()

    def _on_histogram_region_changed(self):
        """Sync histogram region to sliders/spinboxes."""
        self._is_updating_programmatically = True
        min_val, max_val = self.hist_region.getRegion()
        
        self.slider_clim_min.setValue(int(min_val))
        self.slider_clim_max.setValue(int(max_val))
        self.spinbox_clim_min.setValue(int(min_val))
        self.spinbox_clim_max.setValue(int(max_val))
        
        self._update_mapping_curve()
        
        self._is_updating_programmatically = False
        self.clim_changed.emit()

    def _on_apply_clim_clip(self):
        """Emit signal to permanently clip data to current clim range."""
        clim = [self.slider_clim_min.value(), self.slider_clim_max.value()]
        self.apply_clim_clip.emit(clim)

    def _update_visibility(self):
        mode = self.active_mode
        is_iso = (mode == 'iso')
        is_vol = (mode == 'volume')
        is_slice = (mode == 'slices')
        is_mesh = (mode == 'mesh')

        show_solid_col = is_iso and (self.coloring_mode_combo.currentText() == 'Solid Color')
        show_cmap = is_vol or is_slice or is_mesh or (is_iso and not show_solid_col)
        show_clim = is_vol or is_slice

        def visible(widgets, status):
            for w in widgets:
                if w.parentWidget() != self:
                    w.parentWidget().setVisible(status)
                else:
                    w.setVisible(status)

        visible([self.lbl_threshold, self.threshold_slider, self.threshold_value_label], is_iso)
        visible([self.lbl_coloring_mode, self.coloring_mode_combo], is_iso)
        visible([self.lbl_light_angle, self.light_azimuth_slider], is_iso)
        visible([self.lbl_render_style, self.render_style_combo], is_iso)
        visible([self.lbl_solid_color, self.solid_color_combo], show_solid_col)

        visible([self.lbl_opacity, self.opacity_combo], is_vol)
        visible([self.lbl_colormap, self.colormap_combo], show_cmap)

        visible([self.lbl_clim, self.hist_widget, 
                 self.lbl_clim_min, self.slider_clim_min, self.spinbox_clim_min,
                 self.lbl_clim_max, self.slider_clim_max, self.spinbox_clim_max,
                 self.btn_apply_clim_clip, self.btn_invert_volume], show_clim)

        visible([self.lbl_slice_x, self.slider_slice_x], is_slice)
        visible([self.lbl_slice_y, self.slider_slice_y], is_slice)
        visible([self.lbl_slice_z, self.slider_slice_z], is_slice)


--- FILE: .\gui\panels\processing_panel.py ---
"""
Structure Processing Panel for workflow actions.
"""

from __future__ import annotations

from PyQt5.QtCore import Qt, pyqtSignal
from PyQt5.QtWidgets import (
    QCheckBox,
    QComboBox,
    QFormLayout,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QPushButton,
    QSpinBox,
    QVBoxLayout,
    QWidget,
)

from gui.styles import PANEL_TITLE_STYLE
from gui.ui_constants import apply_group_layout, make_description_label, set_primary_button_policy


class StructureProcessingPanel(QGroupBox):
    """
    Panel for Workflow Actions (Load, Process, Model, Export).
    """

    load_clicked = pyqtSignal()
    fast_load_clicked = pyqtSignal()
    dummy_clicked = pyqtSignal()
    extract_pores_clicked = pyqtSignal()
    auto_threshold_clicked = pyqtSignal()
    pnm_clicked = pyqtSignal()
    reset_clicked = pyqtSignal()
    export_clicked = pyqtSignal()
    gpu_toggled = pyqtSignal(bool)

    load_4dct_clicked = pyqtSignal()
    track_4dct_clicked = pyqtSignal()

    def __init__(self, title: str = "Structure Processing"):
        super().__init__()
        self.custom_title = title
        self._init_ui()

    def _init_ui(self) -> None:
        layout = QVBoxLayout()
        apply_group_layout(layout)

        title_lbl = QLabel(self.custom_title)
        title_lbl.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(title_lbl)
        layout.addWidget(title_lbl)

        section_load = make_description_label(QLabel("--- Data Loading ---"))
        layout.addWidget(section_load)

        load_opt_layout = QHBoxLayout()
        self.load_4d_check = QCheckBox("Load as 4D Series")
        self.load_4d_check.setToolTip(
            "If checked, load buttons will ask for a folder containing timepoint subfolders."
        )
        self.sort_combo = QComboBox()
        self.sort_combo.addItems(["Alphabetical", "Numeric", "Date Modified", "Manual"])
        self.sort_combo.setToolTip("Sorting order for 4D timepoints")
        self.sort_combo.setEnabled(False)
        self.load_4d_check.toggled.connect(self.sort_combo.setEnabled)
        load_opt_layout.addWidget(self.load_4d_check)
        load_opt_layout.addWidget(self.sort_combo, stretch=1)
        layout.addLayout(load_opt_layout)

        self._add_button(layout, "Load DICOM Series", self.load_clicked)
        self._add_button(layout, "Fast Load (2x Downsample)", self.fast_load_clicked)
        self._add_button(layout, "Generate Synthetic Sample", self.dummy_clicked)

        layout.addSpacing(8)
        section_process = make_description_label(QLabel("--- Segmentation & Modeling ---"))
        layout.addWidget(section_process)

        form = QFormLayout()
        form.setLabelAlignment(Qt.AlignRight | Qt.AlignVCenter)
        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)
        form.setHorizontalSpacing(10)
        form.setVerticalSpacing(8)

        self.threshold_spin = QSpinBox()
        self.threshold_spin.setRange(-10000, 30000)
        self.threshold_spin.setValue(-300)
        self.threshold_spin.setSingleStep(50)
        form.addRow("Threshold (HU):", self.threshold_spin)

        algo_row_widget = QWidget()
        algo_row_layout = QHBoxLayout(algo_row_widget)
        algo_row_layout.setContentsMargins(0, 0, 0, 0)
        algo_row_layout.setSpacing(8)

        self.algo_combo = QComboBox()
        self.algo_combo.addItems(["Auto", "Otsu", "Li", "Yen", "Triangle", "Minimum"])
        self.algo_combo.setToolTip(
            "Auto: Smart selection based on histogram\n"
            "Otsu: Classic bimodal thresholding\n"
            "Li: Minimum cross-entropy (noisy data)\n"
            "Yen: Maximum correlation\n"
            "Triangle: Good for CT peak+tail\n"
            "Minimum: Valley between peaks"
        )
        auto_btn = QPushButton("Detect")
        auto_btn.setToolTip("Calculate threshold using selected algorithm")
        auto_btn.clicked.connect(self.auto_threshold_clicked.emit)
        set_primary_button_policy(auto_btn)
        auto_btn.setMinimumWidth(80)
        auto_btn.setMaximumWidth(120)

        algo_row_layout.addWidget(self.algo_combo, stretch=1)
        algo_row_layout.addWidget(auto_btn)
        form.addRow("Algorithm:", algo_row_widget)
        layout.addLayout(form)

        from core.gpu_backend import is_gpu_available

        self.gpu_check = QCheckBox("Enable GPU Acceleration")
        self.gpu_check.setToolTip("Use CuPy/CUDA for accelerated processing")
        gpu_available = is_gpu_available()
        self.gpu_check.setChecked(gpu_available)
        self.gpu_check.setEnabled(gpu_available)
        self.gpu_check.toggled.connect(self.gpu_toggled.emit)
        layout.addWidget(self.gpu_check)

        layout.addSpacing(4)
        self._add_button(layout, "Extract Pores", self.extract_pores_clicked)
        self._add_button(layout, "Generate PNM Model", self.pnm_clicked)

        layout.addSpacing(8)
        self._add_button(layout, "Reset to Original", self.reset_clicked, min_height=35)
        self._add_button(layout, "Export VTK", self.export_clicked, min_height=35)

        self.setLayout(layout)

    def get_sort_mode(self) -> str:
        return self.sort_combo.currentText().lower()

    def _add_button(self, layout, text, signal, min_height: int = 40) -> None:
        btn = QPushButton(text)
        btn.setMinimumHeight(min_height)
        set_primary_button_policy(btn)
        btn.clicked.connect(signal.emit)
        layout.addWidget(btn)

    def get_threshold(self) -> int:
        return self.threshold_spin.value()

    def set_threshold(self, value: int) -> None:
        self.threshold_spin.setValue(value)

    def get_algorithm(self) -> str:
        return self.algo_combo.currentText().lower()


--- FILE: .\gui\panels\roi_panel.py ---
"""
ROI Panel for interactive Region of Interest selection.
"""

from __future__ import annotations

from PyQt5.QtCore import Qt, pyqtSignal
from PyQt5.QtWidgets import (
    QCheckBox,
    QComboBox,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QPushButton,
    QVBoxLayout,
)

from gui.styles import PANEL_TITLE_STYLE
from gui.ui_constants import apply_group_layout, make_description_label, set_primary_button_policy


class ROIPanel(QGroupBox):
    """
    Panel for interactive ROI selection.
    All shapes are controlled by one box widget. Cylinder and sphere are inscribed.
    """

    roi_toggled = pyqtSignal(bool)
    apply_roi = pyqtSignal()
    reset_roi = pyqtSignal()
    shape_changed = pyqtSignal(str)

    def __init__(self, title: str = "ROI Selection"):
        super().__init__()
        self.custom_title = title
        self._enabled = False
        self._bounds = None
        self._shape = "box"
        self._init_ui()

    def _init_ui(self) -> None:
        layout = QVBoxLayout()
        apply_group_layout(layout)

        title_lbl = QLabel(self.custom_title)
        title_lbl.setStyleSheet(PANEL_TITLE_STYLE)
        make_description_label(title_lbl)
        layout.addWidget(title_lbl)

        shape_layout = QHBoxLayout()
        shape_layout.setSpacing(8)
        shape_layout.addWidget(QLabel("Shape:"))
        self.shape_combo = QComboBox()
        self.shape_combo.addItems(["Box", "Cylinder", "Sphere"])
        self.shape_combo.currentTextChanged.connect(self._on_shape_changed)
        shape_layout.addWidget(self.shape_combo)
        layout.addLayout(shape_layout)

        self.shape_desc = QLabel("Extract rectangular region.")
        self.shape_desc.setStyleSheet("color: gray; font-size: 10px;")
        make_description_label(self.shape_desc)
        layout.addWidget(self.shape_desc)

        self.enable_checkbox = QCheckBox("Enable ROI Selection")
        self.enable_checkbox.stateChanged.connect(self._on_toggle)
        layout.addWidget(self.enable_checkbox)

        self.bounds_label = QLabel("Bounds: Not selected")
        make_description_label(self.bounds_label)
        layout.addWidget(self.bounds_label)

        btn_layout = QHBoxLayout()
        btn_layout.setSpacing(8)
        self.apply_btn = QPushButton("Apply ROI")
        self.apply_btn.setEnabled(False)
        set_primary_button_policy(self.apply_btn)
        self.apply_btn.clicked.connect(self.apply_roi.emit)
        btn_layout.addWidget(self.apply_btn)

        self.reset_btn = QPushButton("Reset")
        set_primary_button_policy(self.reset_btn)
        self.reset_btn.clicked.connect(self._on_reset)
        btn_layout.addWidget(self.reset_btn)
        layout.addLayout(btn_layout)

        self.setLayout(layout)

    def _on_shape_changed(self, text: str) -> None:
        self._shape = text.lower()
        descriptions = {
            "box": "Extract rectangular region.",
            "cylinder": "Extract cylinder inscribed in the box (YZ plane).",
            "sphere": "Extract sphere inscribed in the box.",
        }
        self.shape_desc.setText(descriptions.get(self._shape, ""))
        self._bounds = None
        self.bounds_label.setText("Bounds: Not selected")
        self.apply_btn.setEnabled(False)
        self.shape_changed.emit(self._shape)

    def _on_toggle(self, state: int) -> None:
        self._enabled = (state == Qt.Checked)
        self.roi_toggled.emit(self._enabled)
        if not self._enabled:
            self._bounds = None
            self.bounds_label.setText("Bounds: Not selected")
            self.apply_btn.setEnabled(False)

    def _on_reset(self) -> None:
        self.enable_checkbox.setChecked(False)
        self._bounds = None
        self.bounds_label.setText("Bounds: Not selected")
        self.apply_btn.setEnabled(False)
        self.reset_roi.emit()

    def update_bounds(self, bounds: tuple) -> None:
        self._bounds = bounds
        if not bounds:
            self.bounds_label.setText("Bounds: Not selected")
            self.apply_btn.setEnabled(False)
            return

        size_x = bounds[1] - bounds[0]
        size_y = bounds[3] - bounds[2]
        size_z = bounds[5] - bounds[4]
        shape_info = ""
        if self._shape == "sphere":
            rx, ry, rz = size_x / 2, size_y / 2, size_z / 2
            shape_info = f"\nEllipsoid Rx:{rx:.1f} Ry:{ry:.1f} Rz:{rz:.1f}"
        elif self._shape == "cylinder":
            ry, rz = size_y / 2, size_z / 2
            shape_info = f"\nCylinder Ry:{ry:.1f} Rz:{rz:.1f} H:{size_x:.1f}"

        self.bounds_label.setText(
            f"X: {bounds[0]:.1f} - {bounds[1]:.1f}\n"
            f"Y: {bounds[2]:.1f} - {bounds[3]:.1f}\n"
            f"Z: {bounds[4]:.1f} - {bounds[5]:.1f}{shape_info}"
        )
        self.apply_btn.setEnabled(True)

    def get_shape(self) -> str:
        return self._shape

    def get_bounds(self) -> tuple:
        return self._bounds

    def is_enabled(self) -> bool:
        return self._enabled


--- FILE: .\gui\panels\timeseries_panel.py ---
"""
Time series panels for 4D CT tracking workflow.
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional, Tuple

from PyQt5.QtCore import Qt, QTimer, pyqtSignal
from PyQt5.QtGui import QColor
from PyQt5.QtWidgets import (
    QComboBox,
    QGroupBox,
    QHBoxLayout,
    QHeaderView,
    QLabel,
    QPushButton,
    QSlider,
    QTableWidget,
    QTableWidgetItem,
    QVBoxLayout,
    QWidget,
)

from config import TRACKING_ANIMATION_FPS
from gui.ui_constants import (
    TABLE_MIN_HEIGHT,
    apply_group_layout,
    apply_panel_layout,
    make_description_label,
    set_primary_button_policy,
)


class TimeSeriesControlPanel(QGroupBox):
    """
    Left-side timeline controls for 4DCT navigation.
    """

    timepoint_changed = pyqtSignal(int)
    animation_toggled = pyqtSignal(bool)

    def __init__(self, parent=None):
        super().__init__("Time Navigation", parent)
        self._num_timepoints = 0
        self._current_timepoint = 0
        self._is_animating = False
        self._folder_names: List[str] = []

        self._animation_timer = QTimer()
        self._animation_timer.timeout.connect(self._advance_frame)

        self._setup_ui()
        self.setEnabled(False)

    def _setup_ui(self) -> None:
        layout = QVBoxLayout(self)
        apply_group_layout(layout)

        self._timepoint_label = QLabel("Timepoint: 0 / 0")
        self._timepoint_label.setStyleSheet("font-weight: bold; color: #3498db;")
        make_description_label(self._timepoint_label)
        layout.addWidget(self._timepoint_label)

        slider_layout = QHBoxLayout()
        self._timeline_slider = QSlider(Qt.Horizontal)
        self._timeline_slider.setMinimum(0)
        self._timeline_slider.setMaximum(0)
        self._timeline_slider.valueChanged.connect(self._on_slider_changed)

        slider_layout.addWidget(QLabel("t=0"))
        slider_layout.addWidget(self._timeline_slider, stretch=1)
        self._max_label = QLabel("t=0")
        slider_layout.addWidget(self._max_label)
        layout.addLayout(slider_layout)

        playback_layout = QHBoxLayout()
        self._prev_btn = QPushButton("Prev")
        self._prev_btn.setMinimumHeight(32)
        set_primary_button_policy(self._prev_btn)
        self._prev_btn.clicked.connect(self._prev_timepoint)
        playback_layout.addWidget(self._prev_btn)

        self._play_btn = QPushButton("Play")
        self._play_btn.setMinimumHeight(32)
        set_primary_button_policy(self._play_btn)
        self._play_btn.clicked.connect(self._toggle_animation)
        playback_layout.addWidget(self._play_btn)

        self._next_btn = QPushButton("Next")
        self._next_btn.setMinimumHeight(32)
        set_primary_button_policy(self._next_btn)
        self._next_btn.clicked.connect(self._next_timepoint)
        playback_layout.addWidget(self._next_btn)

        layout.addLayout(playback_layout)

    def set_range(self, num_timepoints: int, folder_names: Optional[List[str]] = None) -> None:
        self._num_timepoints = int(max(num_timepoints, 0))
        self._folder_names = folder_names or [f"t={i}" for i in range(self._num_timepoints)]
        self._timeline_slider.setMaximum(max(0, self._num_timepoints - 1))
        self._timeline_slider.setValue(0)
        self._max_label.setText(f"t={max(0, self._num_timepoints - 1)}")
        self._update_timepoint_label()
        self.setEnabled(True)

    def _update_timepoint_label(self) -> None:
        if self._folder_names and self._current_timepoint < len(self._folder_names):
            name = self._folder_names[self._current_timepoint]
            self._timepoint_label.setText(
                f"Timepoint: {self._current_timepoint} / {max(0, self._num_timepoints - 1)}\n({name})"
            )
        else:
            self._timepoint_label.setText(
                f"Timepoint: {self._current_timepoint} / {max(0, self._num_timepoints - 1)}"
            )

    def _on_slider_changed(self, value: int) -> None:
        self._current_timepoint = int(value)
        self._update_timepoint_label()
        self.timepoint_changed.emit(int(value))

    def _prev_timepoint(self) -> None:
        if self._current_timepoint > 0:
            self._timeline_slider.setValue(self._current_timepoint - 1)

    def _next_timepoint(self) -> None:
        if self._current_timepoint < self._num_timepoints - 1:
            self._timeline_slider.setValue(self._current_timepoint + 1)

    def _toggle_animation(self) -> None:
        self._is_animating = not self._is_animating
        if self._is_animating:
            self._play_btn.setText("Pause")
            interval_ms = int(1000 / max(TRACKING_ANIMATION_FPS, 1))
            self._animation_timer.start(interval_ms)
        else:
            self._play_btn.setText("Play")
            self._animation_timer.stop()
        self.animation_toggled.emit(self._is_animating)

    def _advance_frame(self) -> None:
        next_frame = self._current_timepoint + 1
        if next_frame >= self._num_timepoints:
            next_frame = 0
        self._timeline_slider.setValue(next_frame)

    def reset(self) -> None:
        self._animation_timer.stop()
        self._is_animating = False
        self._play_btn.setText("Play")
        self._timeline_slider.setMaximum(0)
        self._timeline_slider.setValue(0)
        self._timepoint_label.setText("Timepoint: 0 / 0")
        self.setEnabled(False)


class TrackingAnalysisPanel(QWidget):
    """
    Right-side tracking analysis panel.
    """

    pore_selected = pyqtSignal(int)

    def __init__(self, parent=None):
        super().__init__(parent)
        self._time_series_pnm = None
        self._current_timepoint = 0
        self._eval_by_time_and_pore: Dict[int, Dict[int, Dict[str, Any]]] = {}
        self._has_eval = False
        self._setup_ui()
        self.setEnabled(False)

    def _setup_ui(self) -> None:
        layout = QVBoxLayout(self)
        apply_panel_layout(layout)

        self._summary_group = QGroupBox("Tracking Summary")
        summary_layout = QVBoxLayout(self._summary_group)
        apply_group_layout(summary_layout)

        self._summary_table = QTableWidget(4, 2)
        self._summary_table.setHorizontalHeaderLabels(["Metric", "Value"])
        self._summary_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        self._summary_table.verticalHeader().setVisible(False)
        self._summary_table.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        self._summary_table.setMinimumHeight(TABLE_MIN_HEIGHT)
        self._summary_table.setMaximumHeight(260)

        self._set_summary_row(0, "Total Pores", "--")
        self._set_summary_row(1, "Active", "--")
        self._set_summary_row(2, "Compressed", "--")
        self._set_summary_row(3, "Avg. Retention", "--")
        summary_layout.addWidget(self._summary_table)
        layout.addWidget(self._summary_group)

        pore_group = QGroupBox("Pore Detailed Tracking")
        pore_layout = QVBoxLayout(pore_group)
        apply_group_layout(pore_layout)

        filter_layout = QHBoxLayout()
        filter_layout.addWidget(QLabel("Filter:"))
        self._filter_combo = QComboBox()
        self._filter_combo.addItems(["All Pores", "Active Only", "Compressed Only"])
        self._filter_combo.currentIndexChanged.connect(self.update_view)
        filter_layout.addWidget(self._filter_combo, stretch=1)
        pore_layout.addLayout(filter_layout)

        self._pore_table = QTableWidget()
        self._pore_table.setColumnCount(5)
        self._pore_table.setHorizontalHeaderLabels(["ID", "Status", "Vol (t=0)", "Vol (now)", "Eval"])
        self._pore_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        self._pore_table.verticalHeader().setVisible(False)
        self._pore_table.setSelectionBehavior(QTableWidget.SelectRows)
        self._pore_table.setSelectionMode(QTableWidget.SingleSelection)
        self._pore_table.setMinimumHeight(TABLE_MIN_HEIGHT)
        self._pore_table.cellClicked.connect(self._on_pore_clicked)
        pore_layout.addWidget(self._pore_table)

        layout.addWidget(pore_group)
        layout.addStretch()

        self._export_btn = QPushButton("Export Volume History (CSV)")
        self._export_btn.setMinimumHeight(32)
        set_primary_button_policy(self._export_btn)
        self._export_btn.clicked.connect(self._export_csv)
        self._export_btn.setEnabled(False)
        layout.addWidget(self._export_btn)

    def _adjust_table_height(self, table: QTableWidget, max_height: int = 400) -> None:
        table.doItemsLayout()
        height = table.horizontalHeader().height()
        for i in range(table.rowCount()):
            height += table.rowHeight(i)
        height += 6
        clamped_height = max(TABLE_MIN_HEIGHT, min(height, max_height))
        table.setMinimumHeight(clamped_height)
        table.setMaximumHeight(max_height)
        table.setVerticalScrollBarPolicy(
            Qt.ScrollBarAsNeeded if height > max_height else Qt.ScrollBarAlwaysOff
        )

    def _set_summary_row(self, row: int, name: str, value: str) -> None:
        self._summary_table.setItem(row, 0, QTableWidgetItem(name))
        self._summary_table.setItem(row, 1, QTableWidgetItem(value))

    def _index_evaluation(self) -> None:
        self._eval_by_time_and_pore = {}
        self._has_eval = False
        if self._time_series_pnm is None:
            return

        tracking = getattr(self._time_series_pnm, "tracking", None)
        eval_report = getattr(tracking, "evaluation", {}) if tracking is not None else {}
        if not isinstance(eval_report, dict) or not bool(eval_report.get("available", False)):
            return

        steps = eval_report.get("steps", [])
        if not isinstance(steps, list):
            return

        for step in steps:
            if not isinstance(step, dict):
                continue
            t = step.get("time_index")
            if not isinstance(t, int):
                continue
            tracking_eval = step.get("tracking", {})
            one_step: Dict[int, Dict[str, Any]] = {}

            if isinstance(tracking_eval, dict):
                per_ref = tracking_eval.get("per_reference", {})
                if isinstance(per_ref, dict):
                    for key, detail in per_ref.items():
                        if not isinstance(detail, dict):
                            continue
                        try:
                            ref_id = int(key)
                        except Exception:
                            ref_id = int(detail.get("ref_pred_id", -1))
                        if ref_id <= 0:
                            continue
                        one_step[ref_id] = detail

            # Fallback for t=0: only instance mapping is available.
            if not one_step:
                mapping = step.get("mapping", {})
                if isinstance(mapping, dict):
                    pred_to_gt = mapping.get("pred_to_gt", {})
                    if isinstance(pred_to_gt, dict):
                        for key, gt_id in pred_to_gt.items():
                            try:
                                ref_id = int(key)
                                gt_int = int(gt_id)
                            except Exception:
                                continue
                            one_step[ref_id] = {
                                "ref_pred_id": ref_id,
                                "ref_gt_id": gt_int,
                                "present_in_gt": True,
                                "matched_pred_id": ref_id,
                                "mapped_gt_id": gt_int,
                                "outcome": "mapped_t0",
                                "correct": True,
                            }

            if one_step:
                self._eval_by_time_and_pore[t] = one_step

        self._has_eval = len(self._eval_by_time_and_pore) > 0

    @staticmethod
    def _eval_label_and_color(eval_detail: Optional[Dict[str, Any]]) -> Tuple[str, Optional[QColor]]:
        if not isinstance(eval_detail, dict):
            return ("n/a", None)

        outcome = str(eval_detail.get("outcome", "unknown"))
        is_correct = bool(eval_detail.get("correct", False))
        if is_correct:
            if outcome == "correct_absent":
                return ("closure ok", QColor(212, 242, 220))
            return ("match ok", QColor(212, 242, 220))

        mapping: Dict[str, Tuple[str, QColor]] = {
            "mapped_t0": ("mapped", QColor(212, 242, 220)),
            "missed": ("missed", QColor(255, 224, 224)),
            "id_switched": ("id switch", QColor(255, 210, 200)),
            "unmatched_to_gt": ("no gt match", QColor(255, 235, 205)),
            "false_positive_alive": ("false alive", QColor(255, 224, 224)),
            "unknown": ("unknown", QColor(235, 235, 235)),
        }
        return mapping.get(outcome, (outcome, QColor(235, 235, 235)))

    def _colorize_row(self, row: int, color: Optional[QColor]) -> None:
        if color is None:
            return
        for col in range(self._pore_table.columnCount()):
            item = self._pore_table.item(row, col)
            if item is not None:
                item.setBackground(color)

    def set_time_series(self, time_series_pnm) -> None:
        self._time_series_pnm = time_series_pnm
        self._current_timepoint = 0
        self._index_evaluation()

        summary = time_series_pnm.get_summary()
        self._set_summary_row(0, "Total Pores", str(summary.get("reference_pores", 0)))
        self._set_summary_row(1, "Active", str(summary.get("active_pores", 0)))
        self._set_summary_row(2, "Compressed", str(summary.get("compressed_pores", 0)))
        self._set_summary_row(3, "Avg. Retention", f"{summary.get('avg_volume_retention', 0):.1%}")
        self._adjust_table_height(self._summary_table, max_height=260)

        self.update_view()
        self.setEnabled(True)
        self._export_btn.setEnabled(True)

    def set_timepoint(self, index: int) -> None:
        self._current_timepoint = int(index)
        self.update_view()

    def update_view(self) -> None:
        if self._time_series_pnm is None:
            return

        tracking = self._time_series_pnm.tracking
        filter_mode = self._filter_combo.currentIndex()
        if filter_mode == 0:
            pore_ids = tracking.reference_ids
        elif filter_mode == 1:
            pore_ids = tracking.get_active_pore_ids(self._current_timepoint)
        else:
            pore_ids = tracking.get_compressed_pore_ids(self._current_timepoint)

        eval_for_time = self._eval_by_time_and_pore.get(self._current_timepoint, {}) if self._has_eval else {}

        self._pore_table.setRowCount(len(pore_ids))
        for row, pore_id in enumerate(pore_ids):
            volumes = tracking.volume_history.get(pore_id, [])
            statuses = tracking.status_history.get(pore_id, [])

            vol_t0 = float(volumes[0]) if volumes else 0.0
            vol_now = float(volumes[self._current_timepoint]) if self._current_timepoint < len(volumes) else 0.0
            status = statuses[self._current_timepoint].value if self._current_timepoint < len(statuses) else "unknown"

            self._pore_table.setItem(row, 0, QTableWidgetItem(str(pore_id)))
            status_item = QTableWidgetItem(status)
            self._pore_table.setItem(row, 1, status_item)
            self._pore_table.setItem(row, 2, QTableWidgetItem(f"{vol_t0:.0f}"))
            self._pore_table.setItem(row, 3, QTableWidgetItem(f"{vol_now:.0f}"))

            eval_detail = eval_for_time.get(int(pore_id)) if isinstance(eval_for_time, dict) else None
            eval_label, eval_color = self._eval_label_and_color(eval_detail)
            self._pore_table.setItem(row, 4, QTableWidgetItem(eval_label))

            if self._has_eval and eval_detail is not None:
                self._colorize_row(row, eval_color)
            else:
                fallback = QColor(255, 225, 225) if status == "compressed" else QColor(220, 245, 220)
                status_item.setBackground(fallback)

        self._adjust_table_height(self._pore_table, max_height=520)

    def _on_pore_clicked(self, row: int, _col: int) -> None:
        item = self._pore_table.item(row, 0)
        if item:
            self.pore_selected.emit(int(item.text()))

    def _export_csv(self) -> None:
        from PyQt5.QtWidgets import QFileDialog
        from processors.pnm_tracker import PNMTracker

        filepath, _ = QFileDialog.getSaveFileName(
            self,
            "Export Volume History",
            "pore_volumes.csv",
            "CSV Files (*.csv)",
        )
        if not filepath or self._time_series_pnm is None:
            return

        tracker = PNMTracker()
        tracker.time_series = self._time_series_pnm
        tracker.export_volume_csv(filepath)

    def reset(self) -> None:
        self._time_series_pnm = None
        self._current_timepoint = 0
        self._eval_by_time_and_pore = {}
        self._has_eval = False
        self._pore_table.setRowCount(0)
        self._set_summary_row(0, "Total Pores", "--")
        self._set_summary_row(1, "Active", "--")
        self._set_summary_row(2, "Compressed", "--")
        self._set_summary_row(3, "Avg. Retention", "--")
        self.setEnabled(False)
        self._export_btn.setEnabled(False)


--- FILE: .\gui\panels\__init__.py ---
"""
Panels sub-package.
"""

from gui.panels.info_panel import InfoPanel, StatisticsPanel
from gui.panels.mode_panel import VisualizationModePanel
from gui.panels.params_panel import RenderingParametersPanel
from gui.panels.clip_panel import ClipPlanePanel
from gui.panels.roi_panel import ROIPanel
from gui.panels.processing_panel import StructureProcessingPanel
from gui.panels.timeseries_panel import TimeSeriesControlPanel, TrackingAnalysisPanel

__all__ = [
    'InfoPanel',
    'StatisticsPanel',
    'VisualizationModePanel',
    'RenderingParametersPanel',
    'ClipPlanePanel',
    'ROIPanel',
    'StructureProcessingPanel',
    'TimeSeriesControlPanel',
    'TrackingAnalysisPanel',
]


--- FILE: .\gui\widgets\collapsible_panel.py ---
"""
Simple collapsible container for dense sidebars.
"""

from __future__ import annotations

from PyQt5.QtCore import Qt
from PyQt5.QtWidgets import QToolButton, QVBoxLayout, QWidget

from gui.ui_constants import GROUP_MARGIN, GROUP_SPACING


class CollapsiblePanel(QWidget):
    def __init__(self, title: str, content: QWidget, expanded: bool = True, parent=None):
        super().__init__(parent)
        self._content = content

        layout = QVBoxLayout(self)
        layout.setContentsMargins(*GROUP_MARGIN)
        layout.setSpacing(GROUP_SPACING)

        self._toggle = QToolButton(self)
        self._toggle.setText(title)
        self._toggle.setToolButtonStyle(Qt.ToolButtonTextBesideIcon)
        self._toggle.setCheckable(True)
        self._toggle.setChecked(bool(expanded))
        self._toggle.setArrowType(Qt.DownArrow if expanded else Qt.RightArrow)
        self._toggle.toggled.connect(self._on_toggled)
        layout.addWidget(self._toggle)
        layout.addWidget(content)

        self._content.setVisible(bool(expanded))

    def _on_toggled(self, checked: bool) -> None:
        self._toggle.setArrowType(Qt.DownArrow if checked else Qt.RightArrow)
        self._content.setVisible(bool(checked))


--- FILE: .\gui\widgets\__init__.py ---
"""
Reusable GUI widgets.
"""

from gui.widgets.collapsible_panel import CollapsiblePanel

__all__ = ["CollapsiblePanel"]


--- FILE: .\loaders\dicom.py ---
"""
DICOM series loaders for CT/Micro-CT data.
Optimized for uncompressed files with parallel reading and filename-based sorting.
Uses shared utilities from dicom_utils module.
"""

import os
import gc
import numpy as np
import pydicom
import concurrent.futures
from glob import glob
from typing import List, Tuple, Optional, Callable
from enum import Enum

from core import BaseLoader, VolumeData
from config import (
    LOADER_THRESHOLD_FAST,
    LOADER_THRESHOLD_MMAP,
    LOADER_THRESHOLD_CHUNKED,
    LOADER_MAX_WORKERS,
    LOADER_DOWNSAMPLE_STEP
)

# Import shared utilities from dedicated module
from loaders.dicom_utils import (
    NUMBA_AVAILABLE,
    rescale_volume_numba as _rescale_volume_numba,
    rescale_volume_gpu as _rescale_volume_gpu,
    natural_sort_key as _natural_sort_key,
    validate_path as _validate_path,
    find_dicom_files as _find_dicom_files,
    get_spacing_and_origin as _get_spacing_and_origin,
    apply_rescale as _apply_rescale,
    sort_slices_by_position as _sort_slices_by_position,
    extract_metadata as _extract_metadata
)
from config import GPU_ENABLED
from core.gpu_backend import get_gpu_backend, CUPY_AVAILABLE




class DicomSeriesLoader(BaseLoader):
    """Concrete DICOM series loader for Industrial CT/Micro-CT scans.
    
    Optimized for performance with:
    - Filename-based natural sorting (default, fast and reliable)
    - Optional header-based sorting verification
    - Parallel file reading using ThreadPoolExecutor
    """

    def __init__(self, use_header_sort: bool = True, max_workers: int = 4):
        """
        Args:
            use_header_sort: If True, sort files using ImagePositionPatient headers (default).
                            This ensures correct Z-axis ordering regardless of filename.
                            Falls back to filename sort if headers are missing/corrupt.
            max_workers: Number of parallel threads for file reading.
        """
        self.use_header_sort = use_header_sort
        self.max_workers = max_workers

    def load(self, folder_path: str, callback: Optional[Callable[[int, str], None]] = None) -> VolumeData:
        print(f"[Loader] Scanning sample folder: {folder_path} ...")
        if callback: callback(0, "Scanning directory...")

        _validate_path(folder_path)

        files = _find_dicom_files(folder_path)
        if callback: callback(10, f"Found {len(files)} files. Sorting...")
        
        # Sort files by filename (natural sort)
        files.sort(key=lambda f: _natural_sort_key(os.path.basename(f)))
        
        # Optionally verify with header positions
        if self.use_header_sort:
            files = self._verify_sort_with_headers(files, callback)
        
        if callback: callback(20, f"Reading {len(files)} slices...")
        slices = self._parallel_read_files(files, callback)
        
        if not slices:
            raise ValueError("No valid DICOM slices loaded.")
        
        # Sort slices by Z-position
        slices = _sort_slices_by_position(slices, "Loader")
        
        if callback: callback(50, "Building 3D volume...")
        volume, spacing, origin = self._build_volume(slices, callback)

        metadata = _extract_metadata(slices[0], len(slices), {
            "Description": getattr(slices[0], "StudyDescription", "No Description")
        })

        print(f"[Loader] Loading complete: {volume.shape}, Voxel Spacing: {spacing}")
        if callback: callback(100, "Loading complete.")
        return VolumeData(raw_data=volume, spacing=spacing, origin=origin, metadata=metadata)



    def _verify_sort_with_headers(self, files: List[str], 
                                   callback: Optional[Callable] = None) -> List[str]:
        """Optionally verify sort order using ImagePositionPatient headers."""
        try:
            file_positions = []
            total = len(files)
            for i, f in enumerate(files):
                if callback and i % 50 == 0:
                    callback(10 + int(10 * i / total), f"Reading header {i+1}/{total}...")
                ds = pydicom.dcmread(f, stop_before_pixels=True)
                if hasattr(ds, 'ImagePositionPatient'):
                    file_positions.append((f, float(ds.ImagePositionPatient[2])))
                else:
                    print(f"[Loader] Header missing ImagePositionPatient, using filename order")
                    return files
            
            # Sort by Z position
            file_positions.sort(key=lambda x: x[1])
            print(f"[Loader] Header sort verified. Z-range: {file_positions[0][1]:.2f} -> {file_positions[-1][1]:.2f}")
            return [fp[0] for fp in file_positions]
        except Exception as e:
            print(f"[Loader] Header sort failed ({e}), using filename order")
            return files

    def _parallel_read_files(self, files: List[str], 
                             callback: Optional[Callable] = None) -> List[pydicom.dataset.FileDataset]:
        """Parallel DICOM file reading using ThreadPoolExecutor."""
        def read_single(args):
            idx, f = args
            try:
                return (idx, pydicom.dcmread(f))
            except Exception as e:
                print(f"Warning: Failed to read {f} - {e}")
                return (idx, None)
        
        total = len(files)
        results = [None] * total
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(read_single, (i, f)): i for i, f in enumerate(files)}
            completed = 0
            for future in concurrent.futures.as_completed(futures):
                idx, ds = future.result()
                results[idx] = ds
                completed += 1
                if callback and completed % 20 == 0:
                    percent = 20 + int(30 * completed / total)
                    callback(percent, f"Reading slice {completed}/{total}...")
        
        return [r for r in results if r is not None]

    def _build_volume(self, slices: List[pydicom.dataset.FileDataset], 
                      callback: Optional[Callable] = None) -> Tuple[np.ndarray, tuple, tuple]:
        """Build 3D volume from DICOM slices with optimized memory operations."""
        ds2 = slices[1] if len(slices) > 1 else None
        spacing, origin = _get_spacing_and_origin(slices[0], ds2)

        img_shape = list(slices[0].pixel_array.shape)
        img_shape.insert(0, len(slices))
        
        # Pre-allocate with np.empty (faster than zeros)
        volume = np.empty(img_shape, dtype=np.float32)
        
        # Collect rescale parameters for batch processing
        total = len(slices)
        slopes = np.empty(total, dtype=np.float32)
        intercepts = np.empty(total, dtype=np.float32)

        # First pass: copy pixel data and collect rescale params
        for i, s in enumerate(slices):
            if callback and i % 20 == 0:
                percent = 50 + int(40 * i / total)
                callback(percent, f"Reading slice {i+1}/{total}...")
            
            slopes[i] = float(getattr(s, 'RescaleSlope', 1.0))
            intercepts[i] = float(getattr(s, 'RescaleIntercept', 0.0))
            volume[i] = s.pixel_array.astype(np.float32)
        
        # Second pass: apply rescale (GPU > Numba > NumPy fallback)
        if GPU_ENABLED and CUPY_AVAILABLE:
            import cupy as cp
            backend = get_gpu_backend()
            if callback: callback(92, "Applying rescale (GPU)...")
            try:
                volume_gpu = cp.asarray(volume)
                _rescale_volume_gpu(volume_gpu, slopes, intercepts)
                volume = cp.asnumpy(volume_gpu)
                del volume_gpu
                backend.clear_memory()
            except Exception as e:
                print(f"[Loader] GPU rescale failed: {e}, falling back to Numba")
                _rescale_volume_numba(volume, slopes, intercepts)
        else:
            if callback: callback(92, "Applying rescale (Numba)..." if NUMBA_AVAILABLE else "Applying rescale...")
            _rescale_volume_numba(volume, slopes, intercepts)
        
        if callback: callback(98, "Finalizing...")
        gc.collect()

        return volume, spacing, origin


class FastDicomLoader(DicomSeriesLoader):
    """
    Fast loader that downsamples data (lowers resolution).
    Essential for previewing large Micro-CT datasets.
    """

    def __init__(self, step: int = 2, max_workers: int = 4):
        super().__init__(use_header_sort=False, max_workers=max_workers)
        self.step = step

    def load(self, folder_path: str, callback: Optional[Callable[[int, str], None]] = None) -> VolumeData:
        print(f"[FastLoader] Scanning (Step={self.step}): {folder_path} ...")
        if callback: callback(0, f"Fast scanning (step={self.step})...")

        _validate_path(folder_path)
        files = _find_dicom_files(folder_path)

        print("[FastLoader] Sorting files by natural filename order...")
        if callback: callback(10, "Sorting files...")
        files.sort(key=lambda f: _natural_sort_key(os.path.basename(f)))

        selected_files = files[::self.step]
        print(f"[FastLoader] Selected {len(selected_files)} / {len(files)} files.")

        if callback: callback(20, f"Reading {len(selected_files)} slices...")
        slices = self._parallel_read_files(selected_files, callback)
        
        if not slices:
            raise ValueError("No valid slices loaded.")
        
        slices = _sort_slices_by_position(slices, "FastLoader")

        if callback: callback(50, "Building downsampled volume...")
        volume, spacing, origin = self._build_volume_downsampled(slices, callback)

        metadata = _extract_metadata(slices[0], len(slices), {"Type": "Fast/Downsampled"})

        print(f"[FastLoader] Complete: {volume.shape}, Spacing: {spacing}")
        if callback: callback(100, "Fast load complete.")
        return VolumeData(raw_data=volume, spacing=spacing, origin=origin, metadata=metadata)

    def _build_volume_downsampled(self, slices: List[pydicom.dataset.FileDataset], 
                                   callback: Optional[Callable] = None) -> Tuple[np.ndarray, tuple, tuple]:
        ds2 = slices[1] if len(slices) > 1 else None
        spacing, origin = _get_spacing_and_origin(slices[0], ds2, xy_step=self.step, z_step=self.step)

        base_shape = slices[0].pixel_array.shape
        new_h = base_shape[0] // self.step
        new_w = base_shape[1] // self.step

        volume = np.empty((len(slices), new_h, new_w), dtype=np.float32)

        total = len(slices)
        for i, s in enumerate(slices):
            if callback and i % 10 == 0:
                percent = 50 + int(50 * i / total)
                callback(percent, f"Downsampling slice {i+1}/{total}...")
                
            arr = s.pixel_array[::self.step, ::self.step].astype(np.float32)
            arr = arr[:new_h, :new_w]
            _apply_rescale(arr, s)
            volume[i] = arr

        return volume, spacing, origin


class MemoryMappedDicomLoader(DicomSeriesLoader):
    """
    Memory-mapped loader for very large DICOM datasets.
    Uses numpy memory-mapping to avoid loading entire volume into RAM.
    """
    
    def __init__(self, cache_dir: str = None, use_header_sort: bool = False, max_workers: int = 4):
        super().__init__(use_header_sort=use_header_sort, max_workers=max_workers)
        import tempfile
        self.cache_dir = cache_dir or tempfile.gettempdir()
    
    def load(self, folder_path: str, callback: Optional[Callable[[int, str], None]] = None) -> VolumeData:
        print(f"[MemoryMappedLoader] Scanning: {folder_path} ...")
        if callback: callback(0, "Scanning directory...")
        
        _validate_path(folder_path)
        files = _find_dicom_files(folder_path)
        if callback: callback(10, f"Found {len(files)} files. Sorting...")
        
        files.sort(key=lambda f: _natural_sort_key(os.path.basename(f)))
        
        if self.use_header_sort:
            files = self._verify_sort_with_headers(files, callback)
        
        print(f"[MemoryMappedLoader] Found {len(files)} valid slices")
        
        # Read first and second file for metadata
        first_full = pydicom.dcmread(files[0])
        second_ds = pydicom.dcmread(files[1], stop_before_pixels=True) if len(files) > 1 else None
        rows, cols = first_full.pixel_array.shape
        num_slices = len(files)
        
        spacing, origin = _get_spacing_and_origin(first_full, second_ds)
        
        mmap_file = os.path.join(self.cache_dir, f"ct_volume_{id(self)}_{num_slices}.dat")
        print(f"[MemoryMappedLoader] Creating memory-mapped file: {mmap_file}")
        if callback: callback(30, "Creating memory map...")
        
        volume = np.memmap(mmap_file, dtype=np.float32, mode='w+', shape=(num_slices, rows, cols))
        
        print(f"[MemoryMappedLoader] Loading {num_slices} slices into memory-mapped array...")
        for i, f in enumerate(files):
            if i % 10 == 0 and callback:
                percent = 30 + int(70 * i / num_slices)
                callback(percent, f"Mapping slice {i+1}/{num_slices}...")
            
            ds = pydicom.dcmread(f)
            arr = ds.pixel_array.astype(np.float32)
            _apply_rescale(arr, ds)
            volume[i] = arr
        
        volume.flush()
        print(f"[MemoryMappedLoader] Complete: {volume.shape}, Memory-mapped to disk")
        
        metadata = _extract_metadata(first_full, num_slices, {
            "Type": "Memory-Mapped",
            "MmapFile": mmap_file
        })
        
        if callback: callback(100, "Memory map complete.")
        return VolumeData(raw_data=volume, spacing=spacing, origin=origin, metadata=metadata)


class ChunkedDicomLoader(DicomSeriesLoader):
    """
    Chunked loader that loads data in smaller chunks for memory efficiency.
    """
    
    def __init__(self, chunk_size: int = 64, use_header_sort: bool = False):
        super().__init__(use_header_sort=use_header_sort, max_workers=4)
        self.chunk_size = chunk_size
        self._file_list = None
        self._metadata = None
        self._current_chunk = None
        self._current_chunk_range = None
    
    def load(self, folder_path: str, callback: Optional[Callable[[int, str], None]] = None) -> VolumeData:
        print(f"[ChunkedLoader] Preparing chunked access: {folder_path} ...")
        if callback: callback(0, "Scanning directory...")
        
        _validate_path(folder_path)
        files = _find_dicom_files(folder_path)
        if callback: callback(10, f"Found {len(files)} files. Sorting...")
        
        files.sort(key=lambda f: _natural_sort_key(os.path.basename(f)))
        
        if self.use_header_sort:
            files = self._verify_sort_with_headers(files, callback)
        
        self._file_list = files
        
        if callback: callback(50, "Reading first slice metadata...")
        first = pydicom.dcmread(self._file_list[0])
        second = pydicom.dcmread(self._file_list[1], stop_before_pixels=True) if len(files) > 1 else None
        rows, cols = first.pixel_array.shape
        num_slices = len(self._file_list)
        
        spacing, origin = _get_spacing_and_origin(first, second)
        
        self._metadata = _extract_metadata(first, num_slices, {
            "Type": "Chunked",
            "ChunkSize": self.chunk_size,
            "Dimensions": (num_slices, rows, cols)
        })
        
        print(f"[ChunkedLoader] Loading first chunk (0-{min(self.chunk_size, num_slices)})...")
        if callback: callback(80, "Loading initial preview chunk...")
        chunk = self.load_chunk(0)
        
        print(f"[ChunkedLoader] Ready. Total slices: {num_slices}, Chunk size: {self.chunk_size}")
        
        if callback: callback(100, "Initialization complete.")
        return VolumeData(raw_data=chunk, spacing=spacing, origin=origin, metadata=self._metadata)
    
    def load_chunk(self, start_slice: int) -> np.ndarray:
        if self._file_list is None:
            raise RuntimeError("Must call load() first to initialize the loader")
        
        end_slice = min(start_slice + self.chunk_size, len(self._file_list))
        
        if self._current_chunk_range == (start_slice, end_slice):
            return self._current_chunk
        
        print(f"[ChunkedLoader] Loading chunk: slices {start_slice}-{end_slice}")
        
        first = pydicom.dcmread(self._file_list[0])
        rows, cols = first.pixel_array.shape
        chunk_size = end_slice - start_slice
        
        chunk = np.empty((chunk_size, rows, cols), dtype=np.float32)
        
        for i, idx in enumerate(range(start_slice, end_slice)):
            ds = pydicom.dcmread(self._file_list[idx])
            arr = ds.pixel_array.astype(np.float32)
            _apply_rescale(arr, ds)
            chunk[i] = arr
        
        self._current_chunk = chunk
        self._current_chunk_range = (start_slice, end_slice)
        
        return chunk
    
    def get_total_slices(self) -> int:
        return len(self._file_list) if self._file_list else 0
    
    def get_num_chunks(self) -> int:
        total = self.get_total_slices()
        return (total + self.chunk_size - 1) // self.chunk_size


# ==========================================
# Strategy Enum and Smart Loader
# ==========================================

class LoadStrategy(Enum):
    """Loading strategy options."""
    AUTO = "auto"           # Automatically select best strategy
    FULL = "full"           # Full resolution loading
    FAST = "fast"           # Downsampled preview
    MEMORY_MAPPED = "mmap"  # Memory-mapped for very large files
    CHUNKED = "chunked"     # Chunked loading for huge files


class SmartDicomLoader(BaseLoader):
    """
    Intelligent DICOM loader that automatically selects the best loading strategy
    based on dataset size and available system resources.
    
    Delegates to specialized loaders:
    - DicomSeriesLoader: Full resolution loading
    - FastDicomLoader: Downsampled preview
    - MemoryMappedDicomLoader: Memory-mapped for very large files
    - ChunkedDicomLoader: Chunked loading for huge files
    
    Usage:
        # Auto-select strategy
        loader = SmartDicomLoader()
        data = loader.load("path/to/dicom")
        
        # Force specific strategy
        loader = SmartDicomLoader(strategy=LoadStrategy.FAST)
        data = loader.load("path/to/dicom")
    """

    def __init__(self, 
                 strategy: Optional[LoadStrategy] = None,
                 downsample_step: int = LOADER_DOWNSAMPLE_STEP,
                 max_workers: int = LOADER_MAX_WORKERS):
        """
        Args:
            strategy: Loading strategy. If None or AUTO, auto-selects based on file count.
            downsample_step: Step size for fast loading (default: 2).
            max_workers: Number of parallel threads for file reading.
        """
        self.strategy = strategy
        self.downsample_step = downsample_step
        self.max_workers = max_workers
        self._selected_loader: Optional[BaseLoader] = None
    
    def load(self, folder_path: str, 
             callback: Optional[Callable[[int, str], None]] = None) -> VolumeData:
        """
        Load DICOM series with automatic optimization.
        
        Args:
            folder_path: Path to the folder containing DICOM files.
            callback: Optional progress callback (percent, message).
            
        Returns:
            VolumeData with loaded volume.
        """
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"Path does not exist: {folder_path}")
        
        # Count files to determine strategy
        n_files = self._count_dicom_files(folder_path)
        print(f"[SmartLoader] Found {n_files} DICOM files")
        
        # Select strategy
        strategy = self._select_strategy(n_files)
        if callback:
            callback(5, f"Strategy: {strategy.value} ({n_files} files)")
        
        # Create appropriate loader
        self._selected_loader = self._create_loader(strategy)
        
        # Delegate to selected loader
        print(f"[SmartLoader] Using {type(self._selected_loader).__name__}")
        data = self._selected_loader.load(folder_path, callback=callback)
        
        # Add strategy info to metadata
        if data.metadata:
            data.metadata['LoadStrategy'] = strategy.value
            data.metadata['LoaderClass'] = type(self._selected_loader).__name__
        
        return data
    
    def _count_dicom_files(self, folder_path: str) -> int:
        """Count DICOM files in folder."""
        dcm_files = glob(os.path.join(folder_path, "*.dcm"))
        if dcm_files:
            return len(dcm_files)
        
        # Count all files (may be DICOM without extension)
        all_files = [f for f in glob(os.path.join(folder_path, "*")) 
                     if os.path.isfile(f)]
        return len(all_files)
    
    def _select_strategy(self, n_files: int) -> LoadStrategy:
        """Auto-select loading strategy based on file count."""
        # Use explicit strategy if provided
        if self.strategy is not None and self.strategy != LoadStrategy.AUTO:
            print(f"[SmartLoader] Using explicit strategy: {self.strategy.value}")
            return self.strategy
        
        # Auto-select based on file count
        if n_files >= LOADER_THRESHOLD_CHUNKED:
            print(f"[SmartLoader] Auto: CHUNKED (>{LOADER_THRESHOLD_CHUNKED} files)")
            return LoadStrategy.CHUNKED
        elif n_files >= LOADER_THRESHOLD_MMAP:
            print(f"[SmartLoader] Auto: MEMORY_MAPPED (>{LOADER_THRESHOLD_MMAP} files)")
            return LoadStrategy.MEMORY_MAPPED
        elif n_files >= LOADER_THRESHOLD_FAST:
            print(f"[SmartLoader] Auto: FAST (>{LOADER_THRESHOLD_FAST} files)")
            return LoadStrategy.FAST
        else:
            print(f"[SmartLoader] Auto: FULL (<{LOADER_THRESHOLD_FAST} files)")
            return LoadStrategy.FULL
    
    def _create_loader(self, strategy: LoadStrategy) -> BaseLoader:
        """Create the appropriate loader for the selected strategy."""
        if strategy == LoadStrategy.FAST:
            return FastDicomLoader(step=self.downsample_step, max_workers=self.max_workers)
        elif strategy == LoadStrategy.MEMORY_MAPPED:
            return MemoryMappedDicomLoader(max_workers=self.max_workers)
        elif strategy == LoadStrategy.CHUNKED:
            return ChunkedDicomLoader()
        else:  # FULL or AUTO
            return DicomSeriesLoader(max_workers=self.max_workers)
    
    @property
    def selected_loader(self) -> Optional[BaseLoader]:
        """Get the loader that was selected for the last load operation."""
        return self._selected_loader


--- FILE: .\loaders\dicom_utils.py ---
"""DICOM loading utilities with GPU/Numba acceleration."""

import os
import re
import numpy as np
import pydicom
from glob import glob
from typing import List, Tuple

try:
    from numba import jit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False


if NUMBA_AVAILABLE:
    @jit(nopython=True, parallel=True, cache=True)
    def rescale_volume_numba(volume: np.ndarray, slopes: np.ndarray, intercepts: np.ndarray):
        """Numba JIT accelerated volume rescaling."""
        for i in prange(volume.shape[0]):
            if slopes[i] != 1.0 or intercepts[i] != 0.0:
                for j in range(volume.shape[1]):
                    for k in range(volume.shape[2]):
                        volume[i, j, k] = volume[i, j, k] * slopes[i] + intercepts[i]
else:
    def rescale_volume_numba(volume, slopes, intercepts):
        for i in range(len(slopes)):
            if slopes[i] != 1.0:
                volume[i] *= slopes[i]
            if intercepts[i] != 0.0:
                volume[i] += intercepts[i]


def rescale_volume_gpu(volume_gpu, slopes: np.ndarray, intercepts: np.ndarray):
    """GPU-accelerated volume rescaling (in-place)."""
    import cupy as cp
    
    slopes_gpu = cp.asarray(slopes, dtype=cp.float32)
    intercepts_gpu = cp.asarray(intercepts, dtype=cp.float32)
    
    kernel = cp.RawKernel(r'''
    extern "C" __global__
    void rescale_kernel(float* volume, const float* slopes, const float* intercepts,
                        const int n_slices, const int slice_size) {
        int idx = blockDim.x * blockIdx.x + threadIdx.x;
        if (idx >= n_slices * slice_size) return;
        int z = idx / slice_size;
        float s = slopes[z], b = intercepts[z];
        if (s != 1.0f || b != 0.0f) volume[idx] = volume[idx] * s + b;
    }
    ''', 'rescale_kernel')
    
    n_slices = volume_gpu.shape[0]
    slice_size = volume_gpu.shape[1] * volume_gpu.shape[2]
    blocks = (volume_gpu.size + 255) // 256
    kernel((blocks,), (256,), (volume_gpu, slopes_gpu, intercepts_gpu, n_slices, slice_size))
    del slopes_gpu, intercepts_gpu


def natural_sort_key(text: str):
    """Natural sort key for filenames (img_1, img_2, ..., img_10)."""
    return [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\d+)', text)]


def validate_path(folder_path: str) -> None:
    """Validate folder path exists."""
    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"Path does not exist: {folder_path}")


def find_dicom_files(folder_path: str) -> List[str]:
    """Find DICOM files in folder (by extension then content check)."""
    files = glob(os.path.join(folder_path, "*.dcm"))
    if not files:
        files = [f for f in glob(os.path.join(folder_path, "*")) 
                 if os.path.isfile(f) and _is_dicom(f)]
    if not files:
        raise FileNotFoundError("No valid DICOM/CT files found")
    return files


def _is_dicom(filepath: str) -> bool:
    """Check if file is valid DICOM."""
    try:
        pydicom.dcmread(filepath, stop_before_pixels=True)
        return True
    except Exception:
        return False


def calculate_z_spacing(ds1, ds2=None, step: int = 1) -> float:
    """Calculate Z-spacing from DICOM headers."""
    if ds2 is not None:
        if hasattr(ds1, 'ImagePositionPatient') and hasattr(ds2, 'ImagePositionPatient'):
            return abs(float(ds2.ImagePositionPatient[2]) - float(ds1.ImagePositionPatient[2]))
    return float(getattr(ds1, 'SliceThickness', 1.0)) * step


def get_spacing_and_origin(ds1, ds2=None, xy_step: int = 1, z_step: int = 1) -> Tuple[tuple, tuple]:
    """Extract spacing and origin from DICOM dataset."""
    pixel_spacing = ds1.PixelSpacing
    z_spacing = calculate_z_spacing(ds1, ds2, z_step)
    spacing = (
        float(pixel_spacing[0]) * xy_step,
        float(pixel_spacing[1]) * xy_step,
        z_spacing
    )
    origin = tuple(getattr(ds1, 'ImagePositionPatient', (0.0, 0.0, 0.0)))
    return spacing, origin


def apply_rescale(arr: np.ndarray, ds) -> np.ndarray:
    """Apply DICOM rescale slope/intercept to array (in-place)."""
    slope = float(getattr(ds, 'RescaleSlope', 1.0))
    intercept = float(getattr(ds, 'RescaleIntercept', 0.0))
    if slope != 1.0:
        arr *= slope
    if intercept != 0.0:
        arr += intercept
    return arr


def sort_slices_by_position(slices: List, loader_name: str = "Loader") -> List:
    """Sort DICOM slices by Z-position."""
    if len(slices) > 1 and hasattr(slices[0], 'ImagePositionPatient'):
        try:
            slices.sort(key=lambda s: float(s.ImagePositionPatient[2]))
            z_start = float(slices[0].ImagePositionPatient[2])
            z_end = float(slices[-1].ImagePositionPatient[2])
            print(f"[{loader_name}] Sorted by Z-position: {z_start:.2f} -> {z_end:.2f}")
        except Exception as e:
            print(f"[{loader_name}] Warning: Could not sort by header position: {e}")
    return slices


def extract_metadata(ds, slice_count: int, extra: dict = None) -> dict:
    """Extract common metadata from DICOM dataset."""
    metadata = {
        "SampleID": getattr(ds, "PatientID", "Unknown Sample"),
        "ScanType": getattr(ds, "Modality", "CT"),
        "SliceCount": slice_count,
        "SortMethod": "Header" if hasattr(ds, 'ImagePositionPatient') else "Filename"
    }
    if extra:
        metadata.update(extra)
    return metadata


--- FILE: .\loaders\dummy.py ---
"""
Synthetic data generators for testing.
"""

import numpy as np
from scipy.ndimage import gaussian_filter
from typing import Optional, Callable

from core import BaseLoader, VolumeData


class DummyLoader(BaseLoader):
    """Synthetic porous media generator for testing"""

    def load(self, size: int = 128, callback: Optional[Callable[[int, str], None]] = None) -> VolumeData:
        print(f"[Loader] Generating synthetic internal porous structure (Size: {size})...")
        if callback: callback(0, "Generating random noise...")
        
        # 1. Generate random noise (Gaussian Random Field)
        np.random.seed(None)
        noise = np.random.rand(size, size, size)

        # 2. Apply Gaussian Blur to create organic, connected "blobs"
        sigma = 4.0
        print(f"[Loader] Applying Gaussian filter (sigma={sigma})...")
        if callback: callback(30, "Applying Gaussian filter...")
        blob_field = gaussian_filter(noise, sigma=sigma)

        # 3. Threshold to define Solid vs Void
        threshold = np.mean(blob_field)

        volume = np.zeros((size, size, size), dtype=np.float32)

        # Solid matrix (High Intensity)
        if callback: callback(60, "Thresholding...")
        mask_solid = blob_field > threshold
        volume[mask_solid] = 1000

        # Void/Pore space (Low Intensity)
        mask_void = blob_field <= threshold
        volume[mask_void] = -1000

        # 4. Enforce Solid Boundary (Shell)
        print("[Loader] Enforcing solid boundary shell...")
        if callback: callback(80, "Adding boundary shell...")
        border = 5
        volume[:border, :, :] = 1000
        volume[-border:, :, :] = 1000
        volume[:, :border, :] = 1000
        volume[:, -border:, :] = 1000
        volume[:, :, :border] = 1000
        volume[:, :, -border:] = 1000

        # 5. Add realistic scanner noise
        volume += np.random.normal(0, 50, (size, size, size)).astype(np.float32)
        
        if callback: callback(100, "Generation complete.")
        return VolumeData(
            raw_data=volume,
            spacing=(1.0, 1.0, 1.0),
            origin=(0.0, 0.0, 0.0),
            metadata={
                "Type": "Synthetic",
                "Description": "Solid Block with Internal Random Pores",
                "GenerationMethod": "Gaussian Random Field + Solid Shell"
            }
        )


--- FILE: .\loaders\time_series.py ---
"""
Time series DICOM loader for 4D CT analysis.

Loads multiple DICOM folders representing different timepoints
of the same sample (e.g., compression sequence).
"""

import os
import re
import json
from typing import Any, Dict, List, Optional, Callable, Tuple

import numpy as np

from core import VolumeData
from loaders.dicom import SmartDicomLoader


class TimeSeriesDicomLoader:
    """
    Load multiple DICOM folders as a time series.
    
    Expected folder structure:
    parent_folder/
    鈹溾攢鈹€ t0/         (or any name, sorted by selected mode)
    鈹?  鈹溾攢鈹€ slice001.dcm
    鈹?  鈹斺攢鈹€ ...
    鈹溾攢鈹€ t1/
    鈹?  鈹溾攢鈹€ slice001.dcm
    鈹?  鈹斺攢鈹€ ...
    鈹斺攢鈹€ t2/
        鈹斺攢鈹€ ...
    
    Supports multiple sorting modes:
    - alphabetical: Sort by folder name A-Z
    - numeric: Extract numbers from names (t1, t2, t10...)
    - date modified: Sort by file modification time
    - manual: User selects order in dialog
    """
    

    def __init__(
        self,
        loader: Optional[SmartDicomLoader] = None,
        read_sim_annotations: bool = True,
        strict_annotation_validation: bool = False,
    ):
        self._loader = loader or SmartDicomLoader()
        self._read_sim_annotations = bool(read_sim_annotations)
        self._strict_annotation_validation = bool(strict_annotation_validation)

    
    def load_series(self, 
                   parent_folder: str,
                   sort_mode: str = 'alphabetical',
                   manual_order: Optional[List[str]] = None,
                   callback: Optional[Callable[[int, str], None]] = None) -> List[VolumeData]:
        """
        Load all timepoint folders from a parent directory.
        
        Args:
            parent_folder: Path to folder containing timepoint subfolders
            sort_mode: 'alphabetical', 'numeric', 'date modified', or 'manual'
            manual_order: List of folder paths in desired order (for manual mode)
            callback: Progress callback (percent, message)
            
        Returns:
            List of VolumeData objects, one per timepoint, in time order
        """
        if not os.path.isdir(parent_folder):
            raise ValueError(f"Parent folder does not exist: {parent_folder}")
        
        # Find all subdirectories with DICOM files
        subfolders = []
        for item in os.listdir(parent_folder):
            item_path = os.path.join(parent_folder, item)
            if os.path.isdir(item_path):
                if self._contains_dicom(item_path):
                    subfolders.append(item_path)
        
        if not subfolders:
            raise ValueError(f"No DICOM subfolders found in: {parent_folder}")
        
        # Sort based on mode
        if manual_order is not None:
            # Use provided manual order
            subfolders = manual_order
        else:
            subfolders = self._sort_folders(subfolders, sort_mode)
        
        print(f"[TimeSeriesLoader] Found {len(subfolders)} timepoints, sort mode: {sort_mode}")
        for i, folder in enumerate(subfolders):
            print(f"  t={i}: {os.path.basename(folder)}")

        summary_bundle: Dict[str, Any] = {
            "available": False,
            "data": None,
            "validation": {"ok": True, "errors": [], "warnings": []},
            "path": None,
        }
        if self._read_sim_annotations:
            summary_bundle = self._load_series_annotations_summary(parent_folder, expected_steps=len(subfolders))

        volumes = []
        total = len(subfolders)
        series_validation_errors: List[str] = []
        series_validation_warnings: List[str] = []

        for i, folder_path in enumerate(subfolders):
            folder_name = os.path.basename(folder_path)
            
            def timepoint_callback(percent, msg):
                base_progress = int(100 * i / total)
                folder_progress = int(percent / total)
                overall = base_progress + folder_progress
                if callback:
                    callback(overall, f"[t={i}] {msg}")
                print(f"[t={i}] {msg}")
            
            if callback:
                callback(int(100 * i / total), f"Loading timepoint {i}: {folder_name}")
            
            try:
                volume = self._loader.load(folder_path, callback=timepoint_callback)
                volume.metadata['time_index'] = i
                volume.metadata['source_folder'] = folder_path
                volume.metadata['folder_name'] = folder_name

                if self._read_sim_annotations:
                    step_annotation = self._load_step_annotations(
                        step_folder=folder_path,
                        volume=volume,
                        step_index=i,
                        summary_bundle=summary_bundle,
                    )
                    volume.metadata['sim_annotations'] = step_annotation
                    if step_annotation["validation"]["errors"]:
                        for item in step_annotation["validation"]["errors"]:
                            series_validation_errors.append(f"t={i} {item}")
                    if step_annotation["validation"]["warnings"]:
                        for item in step_annotation["validation"]["warnings"]:
                            series_validation_warnings.append(f"t={i} {item}")

                volumes.append(volume)
                print(f"[TimeSeriesLoader] Loaded t={i}: {folder_name}, shape={volume.dimensions}")
            except Exception as e:
                print(f"[TimeSeriesLoader] Failed to load t={i} ({folder_name}): {e}")
        
        if not volumes:
            raise ValueError("Failed to load any timepoints")
        
        if callback:
            callback(100, f"Loaded {len(volumes)} timepoints")

        if self._read_sim_annotations:
            summary_validation = summary_bundle.get("validation", {})
            for msg in summary_validation.get("errors", []):
                series_validation_errors.append(f"summary {msg}")
            for msg in summary_validation.get("warnings", []):
                series_validation_warnings.append(f"summary {msg}")

            series_annotation_report = {
                "summary": summary_bundle,
                "validation": {
                    "ok": len(series_validation_errors) == 0,
                    "errors": series_validation_errors,
                    "warnings": series_validation_warnings,
                    "error_count": len(series_validation_errors),
                    "warning_count": len(series_validation_warnings),
                },
            }
            for volume in volumes:
                volume.metadata["sim_annotation_series"] = series_annotation_report

            if self._strict_annotation_validation and series_validation_errors:
                raise ValueError(
                    "Simulation annotation validation failed: "
                    + "; ".join(series_validation_errors[:5])
                )

        return volumes

    def _load_series_annotations_summary(self, parent_folder: str, expected_steps: int) -> Dict[str, Any]:
        """Load and lightly validate annotations_summary.json at series root."""
        summary_path = os.path.join(parent_folder, "annotations_summary.json")
        report: Dict[str, Any] = {
            "available": False,
            "path": summary_path,
            "data": None,
            "steps_by_index": {},
            "validation": {"ok": True, "errors": [], "warnings": []},
        }
        if not os.path.isfile(summary_path):
            report["validation"]["warnings"].append("annotations_summary.json not found")
            return report

        try:
            with open(summary_path, "r", encoding="utf-8") as fh:
                summary = json.load(fh)
        except Exception as exc:
            report["validation"]["ok"] = False
            report["validation"]["errors"].append(f"Failed to parse annotations_summary.json: {exc}")
            return report

        report["available"] = True
        report["data"] = summary

        steps = summary.get("steps", [])
        if not isinstance(steps, list):
            steps = []
            report["validation"]["errors"].append("annotations_summary.json field 'steps' is not a list")

        for row in steps:
            if not isinstance(row, dict):
                continue
            idx = row.get("step_index")
            if isinstance(idx, int):
                report["steps_by_index"][idx] = row

        num_steps_declared = summary.get("num_steps")
        if isinstance(num_steps_declared, int) and num_steps_declared != expected_steps:
            report["validation"]["warnings"].append(
                f"summary num_steps={num_steps_declared} differs from loaded timepoints={expected_steps}"
            )
        if len(report["steps_by_index"]) and len(report["steps_by_index"]) != expected_steps:
            report["validation"]["warnings"].append(
                f"summary steps entries={len(report['steps_by_index'])} differs from loaded timepoints={expected_steps}"
            )

        if report["validation"]["errors"]:
            report["validation"]["ok"] = False
        return report

    def _load_step_annotations(
        self,
        step_folder: str,
        volume: VolumeData,
        step_index: int,
        summary_bundle: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Load per-step simulation annotations and validate against CT volume."""
        annotations_path = os.path.join(step_folder, "annotations.json")
        coco_path = os.path.join(step_folder, "coco_annotations.json")
        labels_path = os.path.join(step_folder, "labels.npy")

        step_report: Dict[str, Any] = {
            "step_index": int(step_index),
            "folder": step_folder,
            "files": {
                "annotations_json": annotations_path if os.path.isfile(annotations_path) else None,
                "coco_annotations_json": coco_path if os.path.isfile(coco_path) else None,
                "labels_npy": labels_path if os.path.isfile(labels_path) else None,
            },
            "annotations": None,
            "coco": None,
            "labels": None,
            "validation": {"ok": True, "errors": [], "warnings": [], "checks": {}},
        }

        if step_report["files"]["annotations_json"] is not None:
            try:
                with open(annotations_path, "r", encoding="utf-8") as fh:
                    step_report["annotations"] = json.load(fh)
            except Exception as exc:
                step_report["validation"]["errors"].append(f"Failed to parse annotations.json: {exc}")
        else:
            step_report["validation"]["warnings"].append("annotations.json missing")

        if step_report["files"]["coco_annotations_json"] is not None:
            try:
                with open(coco_path, "r", encoding="utf-8") as fh:
                    step_report["coco"] = json.load(fh)
            except Exception as exc:
                step_report["validation"]["errors"].append(f"Failed to parse coco_annotations.json: {exc}")
        else:
            step_report["validation"]["warnings"].append("coco_annotations.json missing")

        if step_report["files"]["labels_npy"] is not None:
            try:
                labels_volume = np.load(labels_path, mmap_mode="r")
                step_report["labels"] = self._collect_label_stats(labels_volume)
            except Exception as exc:
                step_report["validation"]["errors"].append(f"Failed to load labels.npy: {exc}")
        else:
            step_report["validation"]["warnings"].append("labels.npy missing")

        summary_step = summary_bundle.get("steps_by_index", {}).get(step_index)
        validation = self._validate_step_annotations(
            step_index=step_index,
            volume_shape=list(volume.dimensions),
            annotations=step_report["annotations"],
            coco=step_report["coco"],
            label_stats=step_report["labels"],
            summary_step=summary_step,
            summary_available=bool(summary_bundle.get("available", False)),
        )
        step_report["validation"] = validation
        return step_report

    def _collect_label_stats(self, labels_volume: np.ndarray) -> Dict[str, Any]:
        """Collect labels.npy stats without forcing full-memory copy."""
        stats: Dict[str, Any] = {
            "shape": [int(v) for v in labels_volume.shape],
            "dtype": str(labels_volume.dtype),
            "unique_ids": [],
            "unique_ids_truncated": False,
            "nonzero_count": 0,
            "nonzero_count_estimated": False,
        }

        full_scan_voxel_limit = 40_000_000
        if labels_volume.size <= full_scan_voxel_limit:
            unique_all = np.unique(labels_volume)
            unique_positive = unique_all[unique_all > 0]
            stats["unique_ids"] = [int(v) for v in unique_positive.tolist()]
            stats["nonzero_count"] = int(np.count_nonzero(labels_volume))
            return stats

        flat = labels_volume.reshape(-1)
        stride = max(1, int(flat.size // 5_000_000))
        sample = flat[::stride]
        unique_sample = np.unique(sample)
        unique_positive = unique_sample[unique_sample > 0]
        stats["unique_ids"] = [int(v) for v in unique_positive.tolist()]
        stats["unique_ids_truncated"] = True
        sampled_nonzero = int(np.count_nonzero(sample))
        stats["nonzero_count"] = int(sampled_nonzero * stride)
        stats["nonzero_count_estimated"] = True
        return stats

    def _validate_step_annotations(
        self,
        step_index: int,
        volume_shape: List[int],
        annotations: Optional[Dict[str, Any]],
        coco: Optional[Dict[str, Any]],
        label_stats: Optional[Dict[str, Any]],
        summary_step: Optional[Dict[str, Any]],
        summary_available: bool,
    ) -> Dict[str, Any]:
        """Validate one step's annotation assets and return structured report."""
        errors: List[str] = []
        warnings: List[str] = []
        checks: Dict[str, Any] = {}

        annotation_ids = set()
        if annotations is not None:
            ann_step_idx = annotations.get("step_index")
            if isinstance(ann_step_idx, int):
                checks["step_index_match"] = ann_step_idx == step_index
                if ann_step_idx != step_index:
                    errors.append(f"annotations step_index={ann_step_idx} != loaded index={step_index}")
            else:
                warnings.append("annotations step_index missing or not int")

            ann_shape = annotations.get("volume_shape")
            if isinstance(ann_shape, list) and len(ann_shape) == 3:
                ann_shape_i = [int(v) for v in ann_shape]
                checks["volume_shape_match"] = ann_shape_i == volume_shape
                if ann_shape_i != volume_shape:
                    errors.append(f"annotations volume_shape={ann_shape_i} != CT shape={volume_shape}")
            else:
                warnings.append("annotations volume_shape missing or invalid")

            voids = annotations.get("voids", [])
            if not isinstance(voids, list):
                warnings.append("annotations voids missing or not list")
                voids = []
            parsed_ids = []
            for item in voids:
                if not isinstance(item, dict):
                    continue
                vid = item.get("id")
                if isinstance(vid, int) and vid > 0:
                    parsed_ids.append(vid)
            annotation_ids = set(parsed_ids)
            checks["num_voids_detected"] = len(annotation_ids)
            if len(parsed_ids) != len(annotation_ids):
                errors.append("annotations void ids are duplicated")

            num_voids = annotations.get("num_voids")
            if isinstance(num_voids, int):
                checks["num_voids_match"] = (num_voids == len(voids))
                if num_voids != len(voids):
                    warnings.append(f"annotations num_voids={num_voids} but len(voids)={len(voids)}")
            else:
                warnings.append("annotations num_voids missing or not int")
        else:
            warnings.append("annotations.json unavailable, skipped detailed 3D checks")

        if coco is not None:
            for required in ("images", "annotations", "categories"):
                if required not in coco:
                    warnings.append(f"coco_annotations.json missing key '{required}'")
            checks["coco_annotations_count"] = (
                len(coco.get("annotations", [])) if isinstance(coco.get("annotations"), list) else 0
            )
        else:
            warnings.append("coco_annotations.json unavailable, skipped COCO checks")

        if label_stats is not None:
            label_shape = label_stats.get("shape")
            if isinstance(label_shape, list) and len(label_shape) == 3:
                checks["labels_shape_match"] = [int(v) for v in label_shape] == [int(v) for v in volume_shape]
                if not checks["labels_shape_match"]:
                    errors.append(f"labels.npy shape={label_shape} != CT shape={volume_shape}")
            else:
                errors.append("labels.npy shape metadata missing")

            dtype_str = str(label_stats.get("dtype", ""))
            if "int" not in dtype_str:
                warnings.append(f"labels.npy dtype is '{dtype_str}', expected integer type")

            label_ids = set(int(v) for v in label_stats.get("unique_ids", []))
            checks["label_instance_count"] = len(label_ids)
            if annotation_ids:
                missing_ids = sorted(annotation_ids - label_ids)
                extra_ids = sorted(label_ids - annotation_ids)
                if missing_ids:
                    errors.append(f"labels.npy missing instance ids from annotations: {missing_ids[:20]}")
                if extra_ids:
                    warnings.append(f"labels.npy has extra instance ids not in annotations: {extra_ids[:20]}")
                if label_stats.get("unique_ids_truncated", False):
                    warnings.append("labels.npy unique id scan was sampled (very large volume)")
        else:
            warnings.append("labels.npy unavailable, skipped instance-volume checks")

        if summary_step is not None:
            step_in_summary = summary_step.get("step_index")
            if isinstance(step_in_summary, int) and step_in_summary != step_index:
                errors.append(f"summary step_index={step_in_summary} != loaded index={step_index}")

            if annotations is not None:
                ann_ratio = annotations.get("compression_ratio")
                sum_ratio = summary_step.get("compression_ratio")
                if isinstance(ann_ratio, (int, float)) and isinstance(sum_ratio, (int, float)):
                    if abs(float(ann_ratio) - float(sum_ratio)) > 1e-6:
                        warnings.append(
                            "compression_ratio mismatch between annotations.json "
                            f"({ann_ratio}) and summary ({sum_ratio})"
                        )

                ann_num_voids = annotations.get("num_voids")
                sum_num_voids = summary_step.get("num_voids")
                if isinstance(ann_num_voids, int) and isinstance(sum_num_voids, int):
                    if ann_num_voids != sum_num_voids:
                        warnings.append(
                            f"num_voids mismatch between annotations.json ({ann_num_voids}) "
                            f"and summary ({sum_num_voids})"
                        )
        elif summary_available and summary_step is None:
            warnings.append("annotations_summary has no entry for this step")

        return {
            "ok": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "checks": checks,
        }
    
    def _sort_folders(self, folders: List[str], mode: str) -> List[str]:
        """Sort folders based on the specified mode."""
        mode = mode.lower()
        
        if mode == 'numeric':
            # Extract numbers from folder names and sort numerically
            def extract_number(path):
                name = os.path.basename(path)
                # Find all numbers in the name
                numbers = re.findall(r'\d+', name)
                if numbers:
                    # Use the first number found
                    return int(numbers[0])
                return 0
            return sorted(folders, key=extract_number)
        
        elif mode == 'date modified':
            # Sort by modification time of the folder
            return sorted(folders, key=lambda p: os.path.getmtime(p))
        
        else:  # alphabetical (default)
            return sorted(folders)
    
    def _contains_dicom(self, folder_path: str) -> bool:
        """Check if folder contains DICOM files."""
        for item in os.listdir(folder_path):
            if item.lower().endswith(('.dcm', '.dicom')):
                return True
            # Also check for files without extension (common in DICOM)
            item_path = os.path.join(folder_path, item)
            if os.path.isfile(item_path) and '.' not in item:
                try:
                    import pydicom
                    pydicom.dcmread(item_path, stop_before_pixels=True)
                    return True
                except:
                    pass
        return False
    
    def get_folder_list(self, parent_folder: str, sort_mode: str = 'alphabetical') -> List[Tuple[int, str, str]]:
        """
        Get list of timepoint folders without loading data.
        
        Returns:
            List of (time_index, folder_name, folder_path) tuples
        """
        if not os.path.isdir(parent_folder):
            return []
        
        subfolders = []
        for item in os.listdir(parent_folder):
            item_path = os.path.join(parent_folder, item)
            if os.path.isdir(item_path) and self._contains_dicom(item_path):
                subfolders.append(item_path)
        
        subfolders = self._sort_folders(subfolders, sort_mode)
        
        return [(i, os.path.basename(f), f) for i, f in enumerate(subfolders)]


def load_time_series(parent_folder: str,
                    sort_mode: str = 'alphabetical',
                    read_sim_annotations: bool = True,
                    strict_annotation_validation: bool = False,
                    callback: Optional[Callable[[int, str], None]] = None) -> List[VolumeData]:
    """
    Convenience function to load a 4D CT time series.
    
    Args:
        parent_folder: Path to folder containing timepoint subfolders
        sort_mode: 'alphabetical', 'numeric', 'date modified', or 'manual'
        read_sim_annotations: Whether to load annotations.json/coco/labels files.
        strict_annotation_validation: Raise when validation errors are found.
        callback: Progress callback (percent, message)
        
    Returns:
        List of VolumeData objects in time order
    """
    loader = TimeSeriesDicomLoader(
        read_sim_annotations=read_sim_annotations,
        strict_annotation_validation=strict_annotation_validation,
    )
    return loader.load_series(parent_folder, sort_mode=sort_mode, callback=callback)


class TimeSeriesOrderDialog:
    """
    Dialog for manually reordering timepoint folders.
    
    Usage:
        dialog = TimeSeriesOrderDialog(parent_widget)
        ordered_folders = dialog.get_order(folder_list)
    """
    
    @staticmethod
    def get_order(parent_widget, folder_list: List[Tuple[int, str, str]]) -> Optional[List[str]]:
        """
        Show dialog to manually reorder folders.
        
        Args:
            parent_widget: Parent Qt widget for the dialog
            folder_list: List of (index, name, path) tuples
            
        Returns:
            List of folder paths in user-selected order, or None if cancelled
        """
        from PyQt5.QtWidgets import (
            QDialog, QVBoxLayout, QHBoxLayout, QListWidget, 
            QPushButton, QLabel, QDialogButtonBox, QListWidgetItem
        )
        from PyQt5.QtCore import Qt
        
        dialog = QDialog(parent_widget)
        dialog.setWindowTitle("Reorder Timepoints")
        dialog.setMinimumWidth(400)
        dialog.setMinimumHeight(300)
        
        layout = QVBoxLayout(dialog)
        
        layout.addWidget(QLabel("Drag items or use buttons to reorder timepoints:"))
        
        # List widget with drag-and-drop
        list_widget = QListWidget()
        list_widget.setDragDropMode(QListWidget.InternalMove)
        
        for idx, name, path in folder_list:
            item = QListWidgetItem(f"t={idx}: {name}")
            item.setData(Qt.UserRole, path)
            list_widget.addItem(item)
        
        layout.addWidget(list_widget)
        
        # Move buttons
        btn_layout = QHBoxLayout()
        
        up_btn = QPushButton("鈫?Move Up")
        down_btn = QPushButton("鈫?Move Down")
        
        def move_up():
            row = list_widget.currentRow()
            if row > 0:
                item = list_widget.takeItem(row)
                list_widget.insertItem(row - 1, item)
                list_widget.setCurrentRow(row - 1)
        
        def move_down():
            row = list_widget.currentRow()
            if row < list_widget.count() - 1:
                item = list_widget.takeItem(row)
                list_widget.insertItem(row + 1, item)
                list_widget.setCurrentRow(row + 1)
        
        up_btn.clicked.connect(move_up)
        down_btn.clicked.connect(move_down)
        
        btn_layout.addWidget(up_btn)
        btn_layout.addWidget(down_btn)
        layout.addLayout(btn_layout)
        
        # Dialog buttons
        button_box = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)
        button_box.accepted.connect(dialog.accept)
        button_box.rejected.connect(dialog.reject)
        layout.addWidget(button_box)
        
        # Show dialog
        if dialog.exec_() == QDialog.Accepted:
            # Return paths in new order
            ordered = []
            for i in range(list_widget.count()):
                item = list_widget.item(i)
                ordered.append(item.data(Qt.UserRole))
            return ordered
        
        return None


--- FILE: .\loaders\__init__.py ---
"""
Data loaders package.
"""

from loaders.dicom import (
    DicomSeriesLoader,
    FastDicomLoader,
    MemoryMappedDicomLoader,
    ChunkedDicomLoader,
    SmartDicomLoader,
    LoadStrategy
)
from loaders.dummy import DummyLoader
from loaders.time_series import TimeSeriesDicomLoader, load_time_series

__all__ = [
    'DicomSeriesLoader',
    'FastDicomLoader',
    'MemoryMappedDicomLoader',
    'ChunkedDicomLoader',
    'DummyLoader',
    'SmartDicomLoader',
    'LoadStrategy',
    'TimeSeriesDicomLoader',
    'load_time_series',
]


--- FILE: .\processors\gpu_pipeline.py ---
"""
"""GPU Pipeline for unified segmentation (EDT -> local_maxima -> watershed)."""

import numpy as np
import time
from typing import Optional, Tuple
import gc

from config import GPU_ENABLED, GPU_MIN_SIZE_MB
from core.gpu_backend import get_gpu_backend, CUPY_AVAILABLE


def run_segmentation_pipeline_gpu(
    pores_mask: np.ndarray,
    min_peak_distance: int = 6
) -> Tuple[np.ndarray, np.ndarray, int]:
    """GPU-accelerated segmentation: EDT -> local_maxima -> watershed."""
    
    if not GPU_ENABLED or not CUPY_AVAILABLE:
        # Fall back to individual CPU functions
        from processors.utils import distance_transform_edt, find_local_maxima
        from skimage.segmentation import watershed
        
        distance_map = distance_transform_edt(pores_mask)
        local_maxi = find_local_maxima(distance_map, min_distance=min_peak_distance, labels=pores_mask)
        
        markers = np.zeros(pores_mask.shape, dtype=np.int32)
        if len(local_maxi) > 0:
            markers[tuple(local_maxi.T)] = np.arange(len(local_maxi)) + 1
        
        segmented = watershed(-distance_map, markers, mask=distance_map > 0)
        num_pores = int(np.max(segmented))
        
        return distance_map, segmented, num_pores
    
    import cupy as cp
    import cupyx.scipy.ndimage as gpu_ndimage
    
    backend = get_gpu_backend()
    size_mb = pores_mask.nbytes / (1024 * 1024)
    
    # Check if we have enough GPU memory for the full pipeline
    # Need: mask + distance + markers + labels + workspace ~= 6x input size
    required_mb = size_mb * 6
    free_mb = backend.get_free_memory_mb()
    
    if required_mb > free_mb * 0.8:
        print(f"[GPU Pipeline] Insufficient memory ({required_mb:.0f}MB needed, {free_mb:.0f}MB free), using CPU")
        from processors.utils import distance_transform_edt, find_local_maxima, watershed_gpu
        distance_map = distance_transform_edt(pores_mask)
        local_maxi = find_local_maxima(distance_map, min_distance=min_peak_distance, labels=pores_mask)
        markers = np.zeros(pores_mask.shape, dtype=np.int32)
        if len(local_maxi) > 0:
            markers[tuple(local_maxi.T)] = np.arange(len(local_maxi)) + 1
        segmented = watershed_gpu(-distance_map, markers, mask=distance_map > 0)
        return distance_map, segmented, int(np.max(segmented))
    
    start_total = time.time()
    print(f"[GPU Pipeline] Starting ({size_mb:.1f}MB)")
    
    # Upload
    mask_gpu = cp.asarray(pores_mask.astype(np.uint8))
    
    # EDT
    start = time.time()
    distance_gpu = gpu_ndimage.distance_transform_edt(mask_gpu).astype(cp.float32)
    del mask_gpu
    print(f"[GPU Pipeline] EDT: {time.time() - start:.2f}s")
    
    # Local maxima
    start = time.time()
    size = 2 * min_peak_distance + 1
    max_filtered = gpu_ndimage.maximum_filter(distance_gpu, size=size)
    is_peak = (distance_gpu == max_filtered) & (distance_gpu > 0)
    del max_filtered
    
    candidates = cp.argwhere(is_peak)
    n_candidates = candidates.shape[0]
    
    if n_candidates == 0:
        print("[GPU Pipeline] No peaks found")
        backend.clear_memory()
        return np.zeros(pores_mask.shape, dtype=np.float32), np.zeros(pores_mask.shape, dtype=np.int32), 0
    
    candidate_values = distance_gpu[is_peak]
    del is_peak
    
    sort_idx = cp.argsort(-candidate_values)
    sorted_candidates_cpu = cp.asnumpy(candidates[sort_idx])
    del candidates, candidate_values, sort_idx
    
    selected_peaks = _nms_cpu(sorted_candidates_cpu, min_peak_distance)
    print(f"[GPU Pipeline] Local maxima: {time.time() - start:.2f}s, {len(selected_peaks)} peaks")
    
    # Markers
    markers_gpu = cp.zeros(pores_mask.shape, dtype=cp.int32)
    if len(selected_peaks) > 0:
        peak_coords = np.array(selected_peaks)
        markers_gpu[tuple(peak_coords.T)] = cp.arange(len(selected_peaks), dtype=cp.int32) + 1
    
    # Watershed
    start = time.time()
    image_gpu = -distance_gpu
    mask_bool_gpu = distance_gpu > 0
    distance_map_cpu = cp.asnumpy(distance_gpu)
    del distance_gpu
    
    segmented_gpu = _watershed_gpu_inplace(image_gpu, markers_gpu, mask_bool_gpu)
    del image_gpu, markers_gpu, mask_bool_gpu
    print(f"[GPU Pipeline] Watershed: {time.time() - start:.2f}s")
    
    # Download
    segmented_cpu = cp.asnumpy(segmented_gpu)
    del segmented_gpu
    
    num_pores = int(np.max(segmented_cpu))
    print(f"[GPU Pipeline] Total: {time.time() - start_total:.2f}s, {num_pores} pores")
    
    # Only free unused blocks instead of all blocks
    backend.clear_memory(force=False)
    
    return distance_map_cpu, segmented_cpu, num_pores


def _nms_cpu(sorted_candidates: np.ndarray, min_distance: int) -> list:
    """Non-maximum suppression with adaptive GPU/KDTree acceleration."""
    n = len(sorted_candidates)
    
    if n > 50000 and CUPY_AVAILABLE:
        try:
            return _nms_gpu_parallel(sorted_candidates, min_distance)
        except Exception:
            pass
    
    if n > 10000:
        from scipy.spatial import cKDTree
        tree = cKDTree(sorted_candidates)
        suppressed = np.zeros(n, dtype=bool)
        selected = []
        for i in range(n):
            if suppressed[i]:
                continue
            peak = sorted_candidates[i]
            selected.append(peak)
            neighbors = tree.query_ball_point(peak, r=min_distance, p=2)
            for j in neighbors:
                if j > i:
                    suppressed[j] = True
        return selected
    suppressed = np.zeros(n, dtype=bool)
    min_dist_sq = min_distance * min_distance
    selected = []
    for i in range(n):
        if suppressed[i]:
            continue
        peak = sorted_candidates[i]
        selected.append(peak)
        if i + 1 < n:
            diff = sorted_candidates[i+1:] - peak
            suppressed[i+1:][np.sum(diff * diff, axis=1) < min_dist_sq] = True
    return selected


def _nms_gpu_parallel(candidates: np.ndarray, min_distance: int) -> list:
    """GPU parallel NMS for large candidate sets."""
    import cupy as cp
    
    backend = get_gpu_backend()
    min_dist_sq = min_distance * min_distance
    candidates_gpu = cp.asarray(candidates, dtype=cp.float32)
    suppressed = cp.zeros(len(candidates), dtype=cp.bool_)
    selected = []
    
    for idx in range(len(candidates)):
        if suppressed[idx].item():
            continue
        selected.append(candidates[idx])
        if idx + 1 < len(candidates):
            diff = candidates_gpu[idx+1:] - candidates_gpu[idx]
            suppressed[idx+1:] |= (cp.sum(diff * diff, axis=1) < min_dist_sq)
    
    del candidates_gpu, suppressed
    backend.clear_memory(force=False)
    return selected


def _watershed_gpu_inplace(image_gpu, markers_gpu, mask_gpu, max_iterations: int = 500):
    """GPU watershed with optimized RawKernel."""
    import cupy as cp
    
    watershed_kernel = cp.RawKernel(r'''
    extern "C" __global__
    void watershed_kernel(
        const float* image,
        const int* labels_in,
        int* labels_out,
        const bool* mask,
        const int* shape,
        int* changed_flag
    ) {
        int idx = blockDim.x * blockIdx.x + threadIdx.x;
        int total = shape[0] * shape[1] * shape[2];
        if (idx >= total) return;
        
        if (!mask[idx]) { labels_out[idx] = 0; return; }
        int current = labels_in[idx];
        if (current > 0) { labels_out[idx] = current; return; }
        
        int d = shape[0], h = shape[1], w = shape[2];
        int z = idx / (h * w), rem = idx % (h * w), y = rem / w, x = rem % w;
        
        int best_label = 0;
        float best_val = 1e30f;
        
        // 6-connectivity neighbor check (unrolled)
        if (z > 0) {
            int n_idx = (z-1) * h * w + y * w + x;
            int n_label = labels_in[n_idx];
            if (n_label > 0) {
                float n_val = image[n_idx];
                if (n_val < best_val || (n_val == best_val && n_label < best_label)) {
                    best_val = n_val; best_label = n_label;
                }
            }
        }
        if (z < d-1) {
            int n_idx = (z+1) * h * w + y * w + x;
            int n_label = labels_in[n_idx];
            if (n_label > 0) {
                float n_val = image[n_idx];
                if (n_val < best_val || (n_val == best_val && n_label < best_label)) {
                    best_val = n_val; best_label = n_label;
                }
            }
        }
        if (y > 0) {
            int n_idx = z * h * w + (y-1) * w + x;
            int n_label = labels_in[n_idx];
            if (n_label > 0) {
                float n_val = image[n_idx];
                if (n_val < best_val || (n_val == best_val && n_label < best_label)) {
                    best_val = n_val; best_label = n_label;
                }
            }
        }
        if (y < h-1) {
            int n_idx = z * h * w + (y+1) * w + x;
            int n_label = labels_in[n_idx];
            if (n_label > 0) {
                float n_val = image[n_idx];
                if (n_val < best_val || (n_val == best_val && n_label < best_label)) {
                    best_val = n_val; best_label = n_label;
                }
            }
        }
        if (x > 0) {
            int n_idx = z * h * w + y * w + (x-1);
            int n_label = labels_in[n_idx];
            if (n_label > 0) {
                float n_val = image[n_idx];
                if (n_val < best_val || (n_val == best_val && n_label < best_label)) {
                    best_val = n_val; best_label = n_label;
                }
            }
        }
        if (x < w-1) {
            int n_idx = z * h * w + y * w + (x+1);
            int n_label = labels_in[n_idx];
            if (n_label > 0) {
                float n_val = image[n_idx];
                if (n_val < best_val || (n_val == best_val && n_label < best_label)) {
                    best_val = n_val; best_label = n_label;
                }
            }
        }
        
        if (best_label > 0) {
            labels_out[idx] = best_label;
            atomicAdd(changed_flag, 1);
        } else {
            labels_out[idx] = 0;
        }
    }
    ''', 'watershed_kernel')
    
    shape_gpu = cp.array(image_gpu.shape, dtype=cp.int32)
    labels_curr = markers_gpu.astype(cp.int32)
    labels_next = cp.empty_like(labels_curr)
    changed = cp.zeros(1, dtype=cp.int32)
    
    total = image_gpu.size
    threads = 256
    blocks = (total + threads - 1) // threads
    
    check_interval, last_changed = 20, -1
    
    for iteration in range(max_iterations):
        watershed_kernel((blocks,), (threads,), (image_gpu, labels_curr, labels_next, mask_gpu, shape_gpu, changed))
        labels_curr, labels_next = labels_next, labels_curr
        
        if iteration % check_interval == check_interval - 1:
            n_changed = changed.item()
            if n_changed == 0:
                break
            if last_changed > 0 and n_changed < last_changed * 0.1:
                check_interval = max(5, check_interval // 2)
            last_changed = n_changed
            changed.fill(0)
    
    del labels_next, shape_gpu, changed
    return labels_curr


--- FILE: .\processors\pnm.py ---
"""
Pore to sphere processor for Pore Network Modeling (PNM).
GPU-accelerated with CuPy, memory-optimized with disk caching for large volumes.

This module orchestrates the PNM workflow:
1. Segmentation (with shared cache)
2. Distance transform & watershed  
3. Pore extraction
4. Adjacency detection (delegated to adjacency.py)
5. Throat mesh generation (delegated to throat.py)
"""

import os
import tempfile
import numpy as np
import scipy.ndimage as ndimage
import pyvista as pv
from skimage.segmentation import watershed
from typing import Optional, Callable
import gc

from core import BaseProcessor, VolumeData
from processors.utils import binary_fill_holes, distance_transform_edt, find_local_maxima
from processors.pnm_adjacency import find_adjacency
from processors.pnm_throat import create_throat_mesh
from config import GPU_ENABLED, GPU_MIN_SIZE_MB
from core.gpu_backend import CUPY_AVAILABLE, get_gpu_backend

# Optional high-performance libraries
try:
    from joblib import Parallel, delayed
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False


class PoreToSphereProcessor(BaseProcessor):
    """
    GPU-Optimized Pore Network Modeling (PNM).
    
    Generates a mesh representation of pore space with:
    - Spheres representing pores (sized by equivalent radius)
    - Tubes representing throats (connections between adjacent pores)
    
    Workflow:
    1. Create binary pore mask (with shared cache)
    2. Distance transform for pore sizing
    3. Watershed segmentation for individual pores
    4. Extract pore centers and radii
    5. Detect adjacency between pores
    6. Generate mesh geometry
    """

    MIN_PEAK_DISTANCE = 6

    def __init__(self):
        super().__init__()
        self._cache = {}

    def process(self, data: VolumeData, callback: Optional[Callable[[int, str], None]] = None,
                threshold: int = -300) -> VolumeData:
        """
        Process volume data to generate PNM mesh.
        
        If input data is already processed pore data (from PoreExtractionProcessor),
        it will be reused directly, avoiding duplicate segmentation.
        
        Args:
            data: Input VolumeData with raw_data
            callback: Progress callback (percent, message)
            threshold: HU threshold for solid/void separation
            
        Returns:
            VolumeData with mesh attribute containing PNM geometry
        """
        if data.raw_data is None:
            raise ValueError("Input data must contain raw voxel data.")

        def report(p, msg):
            print(f"[PNM] {msg}")
            if callback: callback(p, msg)

        from data.disk_cache import get_segmentation_cache
        
        # Use shared cache for segmentation
        seg_cache = get_segmentation_cache()
        volume_id = f"{id(data.raw_data)}_{threshold}"
        
        # Internal PNM cache for watershed results
        cache_key = (id(data.raw_data), threshold)
        cached_result = self._cache.get(cache_key)

        if cached_result:
            report(10, "Cache Hit! Reusing watershed results...")
            distance_map = cached_result['distance_map']
            segmented_regions = cached_result['labels']
            num_pores = cached_result['num_pores']
            report(80, "Skipped segmentation. Generating optimized mesh...")
        else:
            # Phase 1: Segmentation
            distance_map, segmented_regions, num_pores = self._run_segmentation(
                data, threshold, volume_id, seg_cache, report
            )
            
            if num_pores == 0:
                return self._create_empty_result(data)
            
            # Cache for future use
            self._cache[cache_key] = {
                'distance_map': distance_map,
                'labels': segmented_regions,
                'num_pores': num_pores
            }

        # Phase 2: Mesh Generation
        report(80, "Generating optimized mesh structures (Spheres & Tubes)...")

        pore_centers, pore_radii, pore_ids, _ = self._extract_pore_data(
            segmented_regions, num_pores, data.spacing, data.origin
        )

        pores_mesh = self._create_pores_mesh(pore_centers, pore_radii, pore_ids)

        report(90, "Connecting pores (GPU-accelerated)...")
        
        # GPU-accelerated adjacency detection (delegated)
        connections = find_adjacency(segmented_regions)
        
        # Throat mesh generation (delegated)
        throats_mesh, throat_radii = create_throat_mesh(
            connections, pore_centers, pore_radii, pore_ids,
            distance_map=distance_map, 
            segmented_regions=segmented_regions,
            spacing=data.spacing
        )

        # Phase 3: Metrics & Assembly
        report(93, "Calculating advanced metrics...")
        
        metrics = self._calculate_metrics(
            pore_radii, throat_radii, segmented_regions, num_pores, connections
        )

        report(95, "Merging geometry...")
        
        if throats_mesh.n_points > 0:
            throats_mesh["PoreRadius"] = np.zeros(throats_mesh.n_points)
        
        combined_mesh = pores_mesh.merge(throats_mesh)

        report(100, "PNM Generation Complete.")

        return VolumeData(
            raw_data=None,
            mesh=combined_mesh,
            spacing=data.spacing,
            origin=data.origin,
            metadata={
                "Type": "Processed - PNM Mesh",
                "PoreCount": int(num_pores),
                "ConnectionCount": len(connections),
                "MeshPoints": combined_mesh.n_points,
                **metrics
            }
        )

    def _run_segmentation(self, data, threshold, volume_id, seg_cache, report):
        """Run segmentation pipeline: mask -> distance transform -> watershed."""
        # Check shared segmentation cache first
        cached_mask = seg_cache.get_pores_mask(volume_id)
        
        if cached_mask is not None:
            report(0, "Using cached pores_mask from shared cache...")
            pores_mask = cached_mask > 0
            report(10, f"Pore mask: {np.sum(pores_mask)} voxels")
        elif data.metadata.get("Type", "").startswith("Processed - Void"):
            # Input is already processed pore data
            report(0, "Detected pre-processed pore data...")
            pores_mask = data.raw_data > 0
            report(10, f"Pore mask: {np.sum(pores_mask)} voxels")
        else:
            report(0, f"Starting PNM Extraction (Threshold > {threshold})...")
            
            # Step 1: Segmentation
            solid_mask = data.raw_data > threshold
            filled_mask = binary_fill_holes(solid_mask)
            pores_mask = filled_mask ^ solid_mask
            
            del solid_mask, filled_mask
            gc.collect()
            
            # Store in shared cache
            porosity = (np.sum(pores_mask) / data.raw_data.size) * 100
            seg_cache.store_pores_mask(volume_id, pores_mask.astype(np.uint8), 
                                      metadata={'porosity': porosity})
        
        report(20, "Segmentation complete. Calculating distance map...")

        if np.sum(pores_mask) == 0:
            return None, None, 0

        # Check if we can use unified GPU pipeline (keeps data on GPU)
        shape = pores_mask.shape
        size_mb = pores_mask.nbytes / (1024 * 1024)
        use_disk = size_mb > 200  # > 200MB
        
        if GPU_ENABLED and CUPY_AVAILABLE and not use_disk:
            backend = get_gpu_backend()
            free_mb = backend.get_free_memory_mb()
            required_mb = size_mb * 6  # Pipeline needs ~6x memory
            
            if required_mb < free_mb * 0.8:
                report(30, "Using unified GPU pipeline...")
                try:
                    from processors.gpu_pipeline import run_segmentation_pipeline_gpu
                    distance_map, segmented_regions, num_pores = run_segmentation_pipeline_gpu(
                        pores_mask, min_peak_distance=self.MIN_PEAK_DISTANCE
                    )
                    del pores_mask
                    gc.collect()
                    report(70, f"GPU pipeline complete. Found {num_pores} pores.")
                    return distance_map, segmented_regions, num_pores
                except Exception as e:
                    report(30, f"GPU pipeline failed: {e}, falling back to chunked...")
        
        # Fallback: chunked processing for large volumes or when GPU unavailable
        if use_disk:
            report(25, "Using disk-backed arrays for large volume...")
            cache_dir = tempfile.gettempdir()
            dist_file = os.path.join(cache_dir, f"pnm_dist_{id(data)}.dat")
            distance_map = np.memmap(dist_file, dtype=np.float32, mode='w+', shape=shape)
        else:
            distance_map = np.zeros(shape, dtype=np.float32)
        
        # Compute distance transform in chunks
        report(30, "Computing distance transform...")
        chunk_size = 64
        for i in range(0, shape[0], chunk_size):
            end = min(i + chunk_size, shape[0])
            start_ext = max(0, i - 10)
            end_ext = min(shape[0], end + 10)
            chunk_dist = distance_transform_edt(pores_mask[start_ext:end_ext])
            offset = i - start_ext
            distance_map[i:end] = chunk_dist[offset:offset + (end - i)]
            del chunk_dist
            gc.collect()
        
        if use_disk:
            distance_map.flush()
        
        report(40, "Distance map computed. Finding local maxima...")

        # Find peaks
        local_maxi = find_local_maxima(
            distance_map,
            min_distance=self.MIN_PEAK_DISTANCE,
            labels=pores_mask
        )
        
        del pores_mask
        gc.collect()

        report(50, f"Found {len(local_maxi)} peaks. Creating markers...")
        
        # Step 3: Watershed segmentation
        markers = np.zeros(shape, dtype=np.int32)
        if len(local_maxi) > 0:
            markers[tuple(local_maxi.T)] = np.arange(len(local_maxi)) + 1
        
        report(60, "Running Watershed segmentation...")
        
        if use_disk:
            cache_dir = tempfile.gettempdir()
            labels_file = os.path.join(cache_dir, f"pnm_labels_{id(data)}.dat")
            segmented_regions = np.memmap(labels_file, dtype=np.int32, mode='w+', shape=shape)
            
            small_chunk = 32
            for i in range(0, shape[0], small_chunk):
                end = min(i + small_chunk, shape[0])
                chunk_seg = watershed(
                    -distance_map[i:end], 
                    markers[i:end], 
                    mask=distance_map[i:end] > 0
                )
                segmented_regions[i:end] = chunk_seg
                del chunk_seg
                gc.collect()
            segmented_regions.flush()
        else:
            segmented_regions = watershed(-distance_map, markers, mask=distance_map > 0)
        
        num_pores = int(np.max(segmented_regions))
        report(70, f"Watershed complete. Found {num_pores} pores.")
        
        del markers
        gc.collect()
        
        return distance_map, segmented_regions, num_pores

    def _extract_pore_data(self, regions, num_pores, spacing, origin):
        """Extract pore centroids, radii, and volumes for mesh generation and tracking."""
        slices = ndimage.find_objects(regions)
        sx, sy, sz = spacing
        ox, oy, oz = origin
        avg_spacing = (sx + sy + sz) / 3.0

        def process_single_pore(i):
            label_idx = i + 1
            slice_obj = slices[i]
            if slice_obj is None:
                return None

            local_mask = (regions[slice_obj] == label_idx)
            voxel_count = np.sum(local_mask)
            if voxel_count == 0:
                return None

            r_vox = (3 * voxel_count / (4 * np.pi)) ** (1 / 3)
            radius = r_vox * avg_spacing

            local_cent = ndimage.center_of_mass(local_mask)

            cz = (slice_obj[0].start + local_cent[0]) * sz + oz
            cy = (slice_obj[1].start + local_cent[1]) * sy + oy
            cx = (slice_obj[2].start + local_cent[2]) * sx + ox

            return (label_idx, [cx, cy, cz], radius, int(voxel_count))

        # Use parallel processing for large number of pores
        if JOBLIB_AVAILABLE and num_pores > 100:
            results = Parallel(n_jobs=-1, prefer="threads")(
                delayed(process_single_pore)(i) for i in range(num_pores)
            )
            results = [r for r in results if r is not None]
        else:
            from concurrent.futures import ThreadPoolExecutor, as_completed
            results = []
            max_workers = min(8, max(1, num_pores))
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(process_single_pore, i): i for i in range(num_pores)}
                for future in as_completed(futures):
                    result = future.result()
                    if result is not None:
                        results.append(result)

        results.sort(key=lambda x: x[0])
        
        if not results:
            return np.array([]), np.array([]), np.array([]), np.array([])
        
        ids = [r[0] for r in results]
        centers = [r[1] for r in results]
        radii = [r[2] for r in results]
        volumes = [r[3] for r in results]

        return np.array(centers), np.array(radii), np.array(ids), np.array(volumes)

    def extract_snapshot(self, data: VolumeData, 
                        callback: Optional[Callable[[int, str], None]] = None,
                        threshold: int = -300,
                        time_index: int = 0,
                        compute_connectivity: bool = True) -> 'PNMSnapshot':
        """
        Extract a PNMSnapshot for 4D CT tracking.
        
        This performs the same segmentation as process() but returns
        a PNMSnapshot data structure suitable for temporal tracking
        instead of a mesh.
        
        Args:
            data: Input VolumeData with raw_data
            callback: Progress callback (percent, message)
            threshold: HU threshold for solid/void separation
            time_index: Time index for this snapshot
            compute_connectivity: If False, skip connection computation (saves time for t>0)
            
        Returns:
            PNMSnapshot with pore data and segmented regions
        """
        from core.time_series import PNMSnapshot
        
        if data.raw_data is None:
            raise ValueError("Input data must contain raw voxel data.")

        def report(p, msg):
            print(f"[PNM Snapshot] {msg}")
            if callback: callback(p, msg)

        from data.disk_cache import get_segmentation_cache
        
        seg_cache = get_segmentation_cache()
        volume_id = f"{id(data.raw_data)}_{threshold}_snapshot"
        
        # Run segmentation
        distance_map, segmented_regions, num_pores = self._run_segmentation(
            data, threshold, volume_id, seg_cache, report
        )
        
        if num_pores == 0:
            return PNMSnapshot(
                time_index=time_index,
                pore_centers=np.array([]),
                pore_radii=np.array([]),
                pore_ids=np.array([]),
                pore_volumes=np.array([]),
                connections=[],
                segmented_regions=segmented_regions,
                spacing=data.spacing,
                origin=data.origin,
                metadata={'threshold': threshold, 'num_pores': 0}
            )
        
        # Extract pore data
        report(80, "Extracting pore data for tracking...")
        pore_centers, pore_radii, pore_ids, pore_volumes = self._extract_pore_data(
            segmented_regions, num_pores, data.spacing, data.origin
        )
        
        # Find connections only for reference frame (t=0)
        # For subsequent frames, connectivity is inherited from reference
        if compute_connectivity:
            report(90, "Finding pore connections...")
            connections = find_adjacency(segmented_regions)
            report(100, f"Snapshot extracted: {len(pore_ids)} pores, {len(connections)} connections")
        else:
            connections = []  # Will be filled from reference snapshot
            report(100, f"Snapshot extracted: {len(pore_ids)} pores (connectivity from reference)")
        
        return PNMSnapshot(
            time_index=time_index,
            pore_centers=pore_centers,
            pore_radii=pore_radii,
            pore_ids=pore_ids,
            pore_volumes=pore_volumes,
            connections=connections,
            segmented_regions=segmented_regions,
            spacing=data.spacing,
            origin=data.origin,
            metadata={
                'threshold': threshold,
                'num_pores': len(pore_ids),
                'num_connections': len(connections)
            }
        )

    def _create_pores_mesh(self, centers, radii, ids, compression_ratios=None):
        """Create sphere glyphs for pore visualization."""
        if len(centers) == 0:
            return pv.PolyData()
            
        pore_cloud = pv.PolyData(centers)
        pore_cloud["radius"] = radii
        pore_cloud["IsPore"] = np.ones(len(centers), dtype=int)
        pore_cloud["ID"] = ids
        if compression_ratios is not None:
            pore_cloud["CompressionRatio"] = compression_ratios

        sphere_glyph = pv.Sphere(theta_resolution=10, phi_resolution=10)
        pores_mesh = pore_cloud.glyph(scale="radius", geom=sphere_glyph)
        
        n_pts_per_sphere = sphere_glyph.n_points
        pore_radius_scalars = np.repeat(radii, n_pts_per_sphere)
        pores_mesh["PoreRadius"] = pore_radius_scalars
        
        # Repeat ID scalars for each sphere vertex (needed for highlighting)
        pore_id_scalars = np.repeat(ids, n_pts_per_sphere)
        pores_mesh["ID"] = pore_id_scalars

        if compression_ratios is not None:
            comp_scalars = np.repeat(compression_ratios, n_pts_per_sphere)
            pores_mesh["CompressionRatio"] = comp_scalars
        else:
            pores_mesh["CompressionRatio"] = np.ones(len(pores_mesh.points))
        
        return pores_mesh

    def _calculate_metrics(self, pore_radii, throat_radii, segmented_regions, num_pores, connections):
        """Calculate scientific metrics for pore network analysis."""
        # Size distributions
        size_distribution = self._calculate_size_distribution(pore_radii)
        throat_distribution = self._calculate_size_distribution(throat_radii)
        
        # Throat statistics
        throat_stats = {}
        if len(throat_radii) > 0:
            throat_stats = {
                'min': float(np.min(throat_radii)),
                'max': float(np.max(throat_radii)),
                'mean': float(np.mean(throat_radii))
            }
        
        # Porosity
        total_voxels = segmented_regions.size
        pore_voxels = np.sum(segmented_regions > 0)
        porosity = pore_voxels / total_voxels if total_voxels > 0 else 0
        
        # Permeability (Kozeny-Carman)
        permeability_md = 0.0
        if len(pore_radii) > 0 and porosity > 0 and porosity < 1:
            mean_diameter = 2 * np.mean(pore_radii) * 1e-3
            k_m2 = (porosity**3 * mean_diameter**2) / (180 * (1 - porosity)**2)
            permeability_md = k_m2 / 9.869e-16
        
        # Tortuosity
        tortuosity = 1.0 / np.sqrt(porosity) if porosity > 0 else float('inf')
        
        # Coordination number
        coordination_number = 0.0
        if num_pores > 0:
            coordination_number = (2 * len(connections)) / num_pores
        
        # Connected pore fraction
        connected_pore_ids = set()
        for id_a, id_b in connections:
            connected_pore_ids.add(id_a)
            connected_pore_ids.add(id_b)
        connected_pore_fraction = (len(connected_pore_ids) / num_pores * 100) if num_pores > 0 else 0
        
        # Largest pore ratio
        largest_pore_ratio = self._calculate_connectivity(segmented_regions, num_pores)

        return {
            "PoreSizeDistribution": size_distribution,
            "ThroatSizeDistribution": throat_distribution,
            "LargestPoreRatio": f"{largest_pore_ratio:.2f}%",
            "ThroatStats": throat_stats,
            "PoreRadii": pore_radii.tolist() if len(pore_radii) > 0 else [],
            "Porosity": f"{porosity * 100:.2f}%",
            "Permeability_mD": f"{permeability_md:.4f}",
            "Tortuosity": f"{tortuosity:.3f}",
            "CoordinationNumber": f"{coordination_number:.2f}",
            "ConnectedPoreFraction": f"{connected_pore_fraction:.1f}%"
        }

    def _calculate_size_distribution(self, radii):
        """Calculate size distribution histogram."""
        if len(radii) == 0:
            return {"bins": [], "counts": []}
        
        bins = np.linspace(0, np.max(radii) * 1.1, 10)
        counts, bin_edges = np.histogram(radii, bins=bins)
        
        return {"bins": bin_edges.tolist(), "counts": counts.tolist()}

    def _calculate_connectivity(self, labels_volume, num_pores):
        """Calculate percentage of largest connected pore volume."""
        if num_pores == 0:
            return 0.0
        
        flat_labels = labels_volume.ravel()
        pore_volumes = np.bincount(flat_labels, minlength=num_pores + 1)
        pore_volumes = pore_volumes[1:]
        
        if len(pore_volumes) == 0:
            return 0.0
        
        largest_volume = np.max(pore_volumes)
        total_volume = np.sum(pore_volumes)
        
        return (largest_volume / total_volume) * 100.0 if total_volume > 0 else 0.0

    def _create_empty_result(self, data: VolumeData) -> VolumeData:
        return VolumeData(metadata={"Type": "Empty", "PoreCount": 0})

    def create_time_varying_mesh(self,
                                 reference_mesh: VolumeData,
                                 reference_snapshot,
                                 tracking_result,
                                 timepoint: int,
                                 current_snapshot=None) -> VolumeData:
        """
        Create a PNM mesh for a specific timepoint using the reference structure.
        
        The topology (connectivity) remains the same as the reference.
        Sphere radii and POSITIONS are updated based on tracking data.
        
        Args:
            reference_mesh: The VolumeData with mesh from t=0
            reference_snapshot: PNMSnapshot from t=0 with pore data
            tracking_result: PoreTrackingResult with volume history
            timepoint: Which timepoint to generate mesh for
            current_snapshot: Optional PNMSnapshot for the target timepoint (for positions)
            
        Returns:
            VolumeData with mesh adjusted for the given timepoint
        """
        if not reference_mesh.has_mesh:
            raise ValueError("Reference mesh must contain a mesh")
        
        # Get reference data
        ref_centers = reference_snapshot.pore_centers
        ref_radii = reference_snapshot.pore_radii
        ref_ids = reference_snapshot.pore_ids
        ref_volumes = reference_snapshot.pore_volumes
        
        # Calculate new radii and positions
        new_radii = np.zeros_like(ref_radii, dtype=np.float64)
        new_centers = np.copy(ref_centers) # Default to reference positions
        compression_ratios = np.ones(len(ref_ids), dtype=np.float64)
        
        # Get ID mapping for this timepoint if available
        id_mapping = tracking_result.id_mapping.get(timepoint, {})
        
        # Index current centers by ID for fast lookup if snapshot available
        current_center_map = {}
        if current_snapshot:
            current_ids = current_snapshot.pore_ids
            current_pos = current_snapshot.pore_centers
            for i, pid in enumerate(current_ids):
                current_center_map[int(pid)] = current_pos[i]

        center_history = getattr(tracking_result, "center_history", None)
        
        for i, pore_id in enumerate(ref_ids):
            pore_id = int(pore_id)
            ref_vol = ref_volumes[i] if i < len(ref_volumes) else 1.0
            
            # 1. Update Volume/Radius
            vol_history = tracking_result.volume_history.get(pore_id, [])
            if timepoint < len(vol_history):
                current_vol = vol_history[timepoint]
            else:
                current_vol = ref_vol
            
            # Scale radius by cube root of volume ratio (volume ~ r鲁)
            if ref_vol > 0 and current_vol > 0:
                vol_ratio = current_vol / ref_vol
                new_radii[i] = ref_radii[i] * (vol_ratio ** (1/3))
                compression_ratios[i] = vol_ratio
            elif current_vol <= 0:
                # Compressed pore: very small but visible
                new_radii[i] = ref_radii[i] * 0.1
                compression_ratios[i] = 0.0
                # Mark as compressed (we might want to hide it, but user asked for "compressed" view)
            else:
                new_radii[i] = ref_radii[i]
                compression_ratios[i] = 1.0
                
    
            # 2. Update Position
            # Prefer stabilized center history if available
            if center_history and pore_id in center_history and timepoint < len(center_history[pore_id]):
                new_centers[i] = np.array(center_history[pore_id][timepoint], dtype=np.float64)
            elif current_snapshot:
                matched_id = id_mapping.get(pore_id, -1)
                if matched_id != -1 and matched_id in current_center_map:
                    new_centers[i] = current_center_map[matched_id]
        
        # Create new sphere mesh with updated radii AND positions

        pores_mesh = self._create_pores_mesh(new_centers, new_radii, ref_ids, compression_ratios=compression_ratios)
        
        # Regenerate Throat Mesh using new centers (connectivity remains unchanged)
        # reference_snapshot.connections stores (id_a, id_b)
        connections = reference_snapshot.connections
        
        # Create new throat mesh
        # We don't use distance_map or segmented_regions here to keep it fast and 
        # because the connectivity is strictly from the reference frame.
        throats_mesh, throat_radii = create_throat_mesh(
            connections, new_centers, new_radii, ref_ids,
            distance_map=None, segmented_regions=None, spacing=reference_mesh.spacing
        )
        if throats_mesh.n_points > 0:
            throats_mesh["CompressionRatio"] = np.ones(throats_mesh.n_points, dtype=np.float64)
            throats_mesh["ID"] = np.zeros(throats_mesh.n_points, dtype=ref_ids.dtype)
        
        # Combine
        combined_mesh = pores_mesh.merge(throats_mesh)
        
        # Calculate stats for this timepoint
        active_count = sum(1 for pid in ref_ids 
                         if pid in tracking_result.status_history and
                         timepoint < len(tracking_result.status_history[pid]) and
                         tracking_result.status_history[pid][timepoint].value == 'active')
        
        # Calculate distributions for dynamic histograms
        pore_dist = self._calculate_size_distribution(new_radii)
        throat_dist = self._calculate_size_distribution(throat_radii)
        
        # Calculate throat stats
        throat_stats = {}
        if len(throat_radii) > 0:
            throat_stats = {
                'min': float(np.min(throat_radii)),
                'max': float(np.max(throat_radii)),
                'mean': float(np.mean(throat_radii))
            }

        return VolumeData(
            raw_data=None,
            mesh=combined_mesh,
            spacing=reference_mesh.spacing,
            origin=reference_mesh.origin,
            metadata={
                "Type": f"Processed - PNM Mesh (t={timepoint})",
                "PoreCount": len(ref_ids),
                "ActivePores": active_count,
                "CompressedPores": len(ref_ids) - active_count,
                "Timepoint": timepoint,
                "ConnectionCount": len(connections),
                "PoreSizeDistribution": pore_dist,
                "ThroatSizeDistribution": throat_dist,
                "ThroatStats": throat_stats,
                "LargestPoreRatio": reference_mesh.metadata.get("LargestPoreRatio", "N/A")
            }
        )



--- FILE: .\processors\pnm_adjacency.py ---
"""
Adjacency detection algorithms for Pore Network Modeling.
GPU-accelerated with CuPy, with CPU fallback.
"""

import time
import numpy as np
from typing import Set, Tuple

from config import GPU_ENABLED
from core.gpu_backend import get_gpu_backend, CUPY_AVAILABLE


def find_adjacency(labels_volume: np.ndarray) -> Set[Tuple[int, int]]:
    """
    Detect adjacent pores in a labeled volume.
    
    Uses GPU acceleration when available, falls back to CPU otherwise.
    
    Args:
        labels_volume: 3D integer array where each pore has a unique label (0 = background)
        
    Returns:
        Set of (label_a, label_b) tuples where label_a < label_b
    """
    backend = get_gpu_backend()
    
    if GPU_ENABLED and backend.available and CUPY_AVAILABLE:
        try:
            return _find_adjacency_gpu(labels_volume)
        except Exception as e:
            print(f"[GPU] Adjacency detection failed: {e}, using CPU")
            backend.clear_memory()
    
    return _find_adjacency_cpu(labels_volume)


def _find_adjacency_gpu(labels_volume: np.ndarray) -> Set[Tuple[int, int]]:
    """GPU implementation of adjacency detection using CuPy."""
    import cupy as cp
    
    backend = get_gpu_backend()
    start = time.time()
    
    labels_gpu = cp.asarray(labels_volume)
    adjacency = set()
    
    # Process each axis
    for axis in range(3):
        if axis == 0:
            curr = labels_gpu[:-1, :, :]
            next_ = labels_gpu[1:, :, :]
        elif axis == 1:
            curr = labels_gpu[:, :-1, :]
            next_ = labels_gpu[:, 1:, :]
        else:
            curr = labels_gpu[:, :, :-1]
            next_ = labels_gpu[:, :, 1:]
        
        # Vectorized mask
        mask = (curr > 0) & (next_ > 0) & (curr != next_)
        
        # Explicit GPU->CPU sync with .item()
        has_adjacency = bool(cp.any(mask).item())
        if has_adjacency:
            pairs_a = curr[mask]
            pairs_b = next_[mask]
            
            # Transfer pairs to CPU
            pairs_a_cpu = cp.asnumpy(pairs_a)
            pairs_b_cpu = cp.asnumpy(pairs_b)
            
            # Use numpy unique to reduce duplicates before set operations
            stacked = np.stack([np.minimum(pairs_a_cpu, pairs_b_cpu),
                               np.maximum(pairs_a_cpu, pairs_b_cpu)], axis=1)
            unique_pairs = np.unique(stacked, axis=0)
            
            for a, b in unique_pairs:
                adjacency.add((int(a), int(b)))
            
            del pairs_a, pairs_b
    
    del labels_gpu
    backend.clear_memory()
    
    elapsed = time.time() - start
    print(f"[GPU] Adjacency detection: {elapsed:.2f}s, found {len(adjacency)} connections")
    
    return adjacency


def _find_adjacency_cpu(labels_volume: np.ndarray) -> Set[Tuple[int, int]]:
    """CPU implementation of adjacency detection."""
    start = time.time()
    adjacency = set()
    shape = labels_volume.shape
    chunk_size = 64
    
    def process_axis(axis: int):
        """Process adjacency along a single axis."""
        for i in range(0, shape[axis] - 1, chunk_size):
            end = min(i + chunk_size, shape[axis] - 1)
            slices_curr = [slice(None)] * 3
            slices_next = [slice(None)] * 3
            slices_curr[axis] = slice(i, end)
            slices_next[axis] = slice(i + 1, end + 1)
            
            val_curr = labels_volume[tuple(slices_curr)]
            val_next = labels_volume[tuple(slices_next)]
            mask = (val_curr > 0) & (val_next > 0) & (val_curr != val_next)
            
            if np.any(mask):
                stacked = np.stack([
                    np.minimum(val_curr[mask], val_next[mask]),
                    np.maximum(val_curr[mask], val_next[mask])
                ], axis=1)
                for a, b in np.unique(stacked, axis=0):
                    adjacency.add((int(a), int(b)))
    
    for axis in range(3):
        process_axis(axis)
    
    print(f"[CPU] Adjacency detection: {time.time() - start:.2f}s, found {len(adjacency)} connections")
    return adjacency


--- FILE: .\processors\pnm_throat.py ---
"""
Throat mesh generation algorithms for Pore Network Modeling.
GPU-accelerated batch tube generation with CPU fallback.
"""

import numpy as np
import scipy.ndimage as ndimage
import pyvista as pv
from typing import Tuple, List, Optional, Set, Dict
from concurrent.futures import ThreadPoolExecutor

from config import GPU_ENABLED
from core.gpu_backend import get_gpu_backend, CUPY_AVAILABLE


def create_throat_mesh(
    connections: Set[Tuple[int, int]],
    centers: np.ndarray,
    radii: np.ndarray, 
    ids_list: np.ndarray,
    distance_map: Optional[np.ndarray] = None,
    segmented_regions: Optional[np.ndarray] = None,
    spacing: Optional[Tuple[float, float, float]] = None
) -> Tuple[pv.PolyData, np.ndarray]:
    """
    Create throat mesh connecting pore centers.
    
    Uses accurate distance-transform-based throat radius measurement when
    distance_map and segmented_regions are provided.
    
    Args:
        connections: Set of (id_a, id_b) tuples indicating connected pores
        centers: Array of pore center coordinates (N, 3)
        radii: Array of pore radii (N,)
        ids_list: Array of pore IDs (N,)
        distance_map: Optional distance transform for accurate throat measurement
        segmented_regions: Optional segmented labels for throat measurement
        spacing: Voxel spacing (x, y, z)
        
    Returns:
        Tuple of (throat_mesh, throat_radii_array)
    """
    if not connections or len(centers) == 0:
        return pv.PolyData(), np.array([])

    id_map = {uid: idx for idx, uid in enumerate(ids_list)}
    
    use_accurate = (distance_map is not None and 
                    segmented_regions is not None and 
                    spacing is not None)

    # Pre-compute bounding boxes for throat measurement
    throat_radii_map = {}
    if use_accurate:
        slices = ndimage.find_objects(segmented_regions)
        slices_cache = {i + 1: s for i, s in enumerate(slices) if s is not None}
        
        # Parallel throat radius measurement
        def measure_throat(conn):
            id_a, id_b = conn
            key = (id_a, id_b) if id_a < id_b else (id_b, id_a)
            radius, _ = _measure_throat_radius_fast(
                distance_map, segmented_regions, id_a, id_b, spacing, slices_cache
            )
            return key, radius
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(measure_throat, connections))
        
        for key, radius in results:
            if radius > 0:
                throat_radii_map[key] = radius

    # Build valid connections
    valid_connections = []
    for id_a, id_b in connections:
        if id_a not in id_map or id_b not in id_map:
            continue
            
        idx_a = id_map[id_a]
        idx_b = id_map[id_b]
        p1 = centers[idx_a]
        p2 = centers[idx_b]
        
        key = (id_a, id_b) if id_a < id_b else (id_b, id_a)
        r_throat = throat_radii_map.get(key, 0.0)
        
        if r_throat <= 0:
            r_throat = min(radii[idx_a], radii[idx_b]) * 0.3
        
        valid_connections.append((p1, p2, r_throat))

    if not valid_connections:
        return pv.PolyData(), np.array([])

    throat_radii = np.array([c[2] for c in valid_connections])

    # GPU batch tube generation
    backend = get_gpu_backend()
    if GPU_ENABLED and backend.available and CUPY_AVAILABLE and len(valid_connections) > 50:
        try:
            vertices, faces = _generate_tubes_batch_gpu(valid_connections, n_sides=6)
            if len(vertices) > 0:
                throats = pv.PolyData(vertices, faces)
                throats["IsPore"] = np.zeros(throats.n_points, dtype=int)
                print(f"[GPU] Generated {len(valid_connections)} tubes")
                return throats, throat_radii
        except Exception as e:
            print(f"[GPU] Tube generation failed: {e}, using CPU")
            backend.clear_memory()

    # CPU fallback - batch with PyVista
    throats = _generate_tubes_batch_cpu(valid_connections)
    throats["IsPore"] = np.zeros(throats.n_points, dtype=int)
    
    return throats, throat_radii


def _generate_tubes_batch_gpu(connections: List[Tuple], n_sides: int = 6):
    """Generate all tube meshes on GPU, transfer once at end."""
    # Pre-compute tube template (unit cylinder along Z)
    angles = np.linspace(0, 2 * np.pi, n_sides, endpoint=False)
    circle_x = np.cos(angles)
    circle_y = np.sin(angles)
    
    all_vertices = []
    all_faces = []
    vertex_offset = 0
    
    for p1, p2, radius in connections:
        p1 = np.array(p1)
        p2 = np.array(p2)
        
        # Direction and length
        direction = p2 - p1
        length = np.linalg.norm(direction)
        if length < 1e-6:
            continue
        direction = direction / length
        
        # Build rotation matrix to align Z with direction
        z_axis = np.array([0, 0, 1])
        if np.abs(np.dot(direction, z_axis)) > 0.999:
            # Nearly parallel to Z
            rotation = np.eye(3) if direction[2] > 0 else np.diag([1, -1, -1])
        else:
            v = np.cross(z_axis, direction)
            s = np.linalg.norm(v)
            c = np.dot(z_axis, direction)
            vx = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])
            rotation = np.eye(3) + vx + vx @ vx * ((1 - c) / (s * s))
        
        # Generate tube vertices
        # Bottom ring
        bottom_ring = np.column_stack([
            circle_x * radius * 0.5,
            circle_y * radius * 0.5,
            np.zeros(n_sides)
        ])
        # Top ring
        top_ring = np.column_stack([
            circle_x * radius * 0.5,
            circle_y * radius * 0.5,
            np.ones(n_sides) * length
        ])
        
        # Transform
        bottom_transformed = (rotation @ bottom_ring.T).T + p1
        top_transformed = (rotation @ top_ring.T).T + p1
        
        tube_vertices = np.vstack([bottom_transformed, top_transformed])
        all_vertices.append(tube_vertices)
        
        # Generate faces (quads as two triangles)
        tube_faces = []
        for i in range(n_sides):
            next_i = (i + 1) % n_sides
            # Bottom triangle
            tube_faces.append([3, vertex_offset + i, vertex_offset + next_i, vertex_offset + n_sides + i])
            # Top triangle
            tube_faces.append([3, vertex_offset + next_i, vertex_offset + n_sides + next_i, vertex_offset + n_sides + i])
        
        all_faces.extend(tube_faces)
        vertex_offset += 2 * n_sides
    
    if not all_vertices:
        return np.array([]), np.array([])
    
    vertices = np.vstack(all_vertices).astype(np.float32)
    faces = np.hstack([[item for sublist in all_faces for item in sublist]]).astype(np.int32)
    
    return vertices, faces


def _generate_tubes_batch_cpu(connections: List[Tuple]) -> pv.PolyData:
    """Generate tubes using PyVista (CPU fallback)."""
    tubes_list = []
    
    for p1, p2, r_throat in connections:
        try:
            line = pv.Line(p1, p2)
            tube = line.tube(radius=max(r_throat * 0.5, 0.1), n_sides=6)
            tubes_list.append(tube)
        except Exception:
            continue
    
    if not tubes_list:
        return pv.PolyData()
    
    # Merge all tubes
    result = tubes_list[0]
    for tube in tubes_list[1:]:
        result = result.merge(tube)
    
    return result


def _measure_throat_radius_fast(
    distance_map: np.ndarray,
    segmented_regions: np.ndarray, 
    pore_a: int, 
    pore_b: int, 
    spacing: Tuple[float, float, float],
    slices_cache: Dict[int, tuple] = None
) -> Tuple[float, Optional[np.ndarray]]:
    """
    Measure throat radius using bounding box optimization.
    
    Uses distance transform values at the boundary between two pores.
    
    Args:
        distance_map: Euclidean distance transform of pore space
        segmented_regions: Labeled regions from watershed
        pore_a, pore_b: Pore label IDs
        spacing: Voxel spacing
        slices_cache: Pre-computed slice objects for each pore
        
    Returns:
        Tuple of (throat_radius, None)
    """
    if slices_cache is not None:
        slice_a = slices_cache.get(pore_a)
        slice_b = slices_cache.get(pore_b)
    else:
        return 0.0, None
    
    if slice_a is None or slice_b is None:
        return 0.0, None
    
    # Compute union bounding box
    shape = segmented_regions.shape
    z_min = max(0, min(slice_a[0].start, slice_b[0].start) - 1)
    z_max = min(shape[0], max(slice_a[0].stop, slice_b[0].stop) + 1)
    y_min = max(0, min(slice_a[1].start, slice_b[1].start) - 1)
    y_max = min(shape[1], max(slice_a[1].stop, slice_b[1].stop) + 1)
    x_min = max(0, min(slice_a[2].start, slice_b[2].start) - 1)
    x_max = min(shape[2], max(slice_a[2].stop, slice_b[2].stop) + 1)
    
    # Extract local region
    local_labels = segmented_regions[z_min:z_max, y_min:y_max, x_min:x_max]
    local_dist = distance_map[z_min:z_max, y_min:y_max, x_min:x_max]
    
    mask_a = (local_labels == pore_a)
    mask_b = (local_labels == pore_b)
    
    # Find boundary using dilation
    dilated_a = ndimage.binary_dilation(mask_a)
    dilated_b = ndimage.binary_dilation(mask_b)
    
    throat_region = dilated_a & dilated_b
    
    if not np.any(throat_region):
        return 0.0, None
    
    throat_distances = local_dist[throat_region]
    
    if len(throat_distances) == 0:
        return 0.0, None
    
    throat_radius_voxels = float(np.min(throat_distances))
    avg_spacing = (spacing[0] + spacing[1] + spacing[2]) / 3.0
    throat_radius = throat_radius_voxels * avg_spacing
    
    return throat_radius, None


--- FILE: .\processors\pnm_tracker.py ---
"""
Pore Network Model Tracker for 4D CT analysis.
"""

from __future__ import annotations

import time
import warnings
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np

from core.time_series import PNMSnapshot, PoreStatus, PoreTrackingResult, TimeSeriesPNM
from config import (
    TRACKING_ASSIGN_SOLVER,
    TRACKING_CENTER_SMOOTHING,
    TRACKING_CLOSURE_MIN_VOLUME_VOXELS,
    TRACKING_CLOSURE_STRAIN_THRESHOLD,
    TRACKING_CLOSURE_VOLUME_RATIO_THRESHOLD,
    TRACKING_COMPRESSION_THRESHOLD,
    TRACKING_COST_WEIGHTS,
    TRACKING_ENABLE_MACRO_REGISTRATION,
    TRACKING_GATE_CENTER_RADIUS_FACTOR,
    TRACKING_GATE_IOU_MIN,
    TRACKING_GATE_VOLUME_RATIO_MIN_FLOOR,
    TRACKING_GATE_VOLUME_RATIO_MAX,
    TRACKING_GATE_VOLUME_RATIO_MIN,
    TRACKING_GPU_MIN_PORES,
    TRACKING_IOU_THRESHOLD,
    TRACKING_KALMAN_BRAKE_ACCEL_DECAY,
    TRACKING_KALMAN_BRAKE_VELOCITY_DECAY,
    TRACKING_KALMAN_FREEZE_AFTER_MISSES,
    TRACKING_KALMAN_MEASUREMENT_NOISE,
    TRACKING_KALMAN_PROCESS_NOISE,
    TRACKING_MACRO_REG_SMOOTHING_SIGMA,
    TRACKING_MACRO_REG_USE_GPU,
    TRACKING_MACRO_REG_GPU_MIN_MB,
    TRACKING_MACRO_REG_UPSAMPLE_FACTOR,
    TRACKING_MATCH_MODE,
    TRACKING_MAX_MISSES,
    TRACKING_USE_BATCH,
    TRACKING_USE_GPU,
    TRACKING_USE_HUNGARIAN,
    TRACKING_VOLUME_COST_GAUSSIAN_SIGMA,
    TRACKING_VOLUME_COST_MODE,
)
from processors.tracking_utils import (
    MacroRegistrationResult,
    ConstantAccelerationKalman3D,
    bounded_volume_penalty,
    estimate_macro_registration,
    extract_shifted_overlap_region,
    should_mark_closed_by_compression,
)

INVALID_COST = 1e6

# Try to import GPU backend
try:
    import cupy as cp

    HAS_GPU = True
except ImportError:
    HAS_GPU = False
    cp = None

# Try to import scipy assignment solver
try:
    from scipy.optimize import linear_sum_assignment

    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False

# Optional Jonker-Volgenant solvers
HAS_LAPJV = False
_LAPJV_SOLVER = None
try:
    from lapjv import lapjv as _LAPJV_SOLVER

    HAS_LAPJV = True
except ImportError:
    try:
        from lap import lapjv as _LAPJV_SOLVER

        HAS_LAPJV = True
    except ImportError:
        HAS_LAPJV = False
        _LAPJV_SOLVER = None


def compute_pair_cost(
    iou: float,
    center_dist: float,
    reference_volume: float,
    current_volume: float,
    dice_local: float,
    cost_weights: Tuple[float, float, float, float],
    center_gate_radius: float,
    volume_cost_mode: str = "symdiff",
    volume_cost_sigma: float = 1.0,
) -> float:
    """
    Compute TGGA pairwise matching cost.

    cost = w1*(1-IoU) + w2*norm_center_dist + w3*C_vol + w4*(1-dice_local)
    where C_vol is bounded to [0, 1].
    """
    w_iou, w_center, w_volume, w_dice = cost_weights
    norm_center_dist = center_dist / max(center_gate_radius, 1e-8)
    norm_center_dist = float(np.clip(norm_center_dist, 0.0, 1.0))
    volume_term = bounded_volume_penalty(
        reference_volume=reference_volume,
        current_volume=current_volume,
        mode=volume_cost_mode,
        gaussian_sigma=volume_cost_sigma,
    )
    dice_term = 1.0 - float(np.clip(dice_local, 0.0, 1.0))
    return (
        w_iou * (1.0 - float(np.clip(iou, 0.0, 1.0)))
        + w_center * norm_center_dist
        + w_volume * volume_term
        + w_dice * dice_term
    )


def _compute_overlap_metrics(
    local_ref_mask: np.ndarray,
    current_region: np.ndarray,
    label_id: int,
) -> Tuple[float, float, float]:
    """Compute IoU, local Dice, and local candidate volume for a label in a cropped bbox."""
    candidate_mask = current_region == int(label_id)
    intersection = int(np.sum(local_ref_mask & candidate_mask))
    ref_volume = int(np.sum(local_ref_mask))
    cand_volume = int(np.sum(candidate_mask))
    union = ref_volume + cand_volume - intersection

    iou = (intersection / union) if union > 0 else 0.0
    dice = (2.0 * intersection / (ref_volume + cand_volume)) if (ref_volume + cand_volume) > 0 else 0.0
    return float(iou), float(dice), float(cand_volume)


def build_candidates(
    reference_snapshot: PNMSnapshot,
    current_snapshot: PNMSnapshot,
    reference_masks: Dict[int, Dict],
    current_regions: np.ndarray,
    predicted_centers: Dict[int, np.ndarray],
    cost_weights: Tuple[float, float, float, float],
    gate_center_radius_factor: float,
    gate_volume_ratio_min: float,
    gate_volume_ratio_max: float,
    gate_iou_min: float = 0.02,
    gate_volume_ratio_min_floor: float = 1e-4,
    volume_cost_mode: str = "symdiff",
    volume_cost_sigma: float = 1.0,
) -> Dict[str, object]:
    """
    Build TGGA candidate graph and dense cost matrix.

    Returns:
        {
            "cost_matrix": np.ndarray (N_ref x N_curr),
            "pair_metrics": {(ref_idx, curr_idx): dict},
            "row_geom_candidate": np.ndarray(bool),
            "row_valid_candidate": np.ndarray(bool),
        }
    """
    ref_ids = np.asarray(reference_snapshot.pore_ids, dtype=np.int32)
    curr_ids = (
        np.asarray(current_snapshot.pore_ids, dtype=np.int32)
        if current_snapshot.pore_ids is not None
        else np.array([], dtype=np.int32)
    )
    curr_centers = (
        np.asarray(current_snapshot.pore_centers, dtype=np.float64)
        if current_snapshot.pore_centers is not None
        else np.zeros((len(curr_ids), 3), dtype=np.float64)
    )
    curr_volumes = (
        np.asarray(current_snapshot.pore_volumes, dtype=np.float64)
        if current_snapshot.pore_volumes is not None
        else np.zeros((len(curr_ids),), dtype=np.float64)
    )

    n_ref = len(ref_ids)
    n_curr = len(curr_ids)
    cost_matrix = np.full((n_ref, n_curr), INVALID_COST, dtype=np.float64)
    pair_metrics: Dict[Tuple[int, int], Dict[str, float]] = {}
    row_geom_candidate = np.zeros(n_ref, dtype=bool)
    row_valid_candidate = np.zeros(n_ref, dtype=bool)

    if n_ref == 0 or n_curr == 0:
        return {
            "cost_matrix": cost_matrix,
            "pair_metrics": pair_metrics,
            "row_geom_candidate": row_geom_candidate,
            "row_valid_candidate": row_valid_candidate,
        }

    ref_centers = np.asarray(reference_snapshot.pore_centers, dtype=np.float64)
    ref_radii = np.asarray(reference_snapshot.pore_radii, dtype=np.float64)
    ref_volumes = np.asarray(reference_snapshot.pore_volumes, dtype=np.float64)
    effective_ratio_min = min(float(gate_volume_ratio_min), float(gate_volume_ratio_min_floor))

    for ref_idx, ref_id in enumerate(ref_ids):
        ref_id = int(ref_id)
        mask_data = reference_masks.get(ref_id)
        if mask_data is None:
            continue

        predicted = np.asarray(predicted_centers.get(ref_id, ref_centers[ref_idx]), dtype=np.float64)
        ref_radius = float(max(ref_radii[ref_idx], 1e-6))
        center_gate_radius = float(max(gate_center_radius_factor * ref_radius, 1e-6))

        dists = np.linalg.norm(curr_centers - predicted, axis=1)
        ref_volume = float(ref_volumes[ref_idx])
        if ref_volume > 0:
            volume_ratios = curr_volumes / ref_volume
        else:
            volume_ratios = np.full_like(curr_volumes, np.inf, dtype=np.float64)

        geometric_gate = (
            (dists <= center_gate_radius)
            & (volume_ratios >= effective_ratio_min)
            & (volume_ratios <= gate_volume_ratio_max)
        )
        if np.any(geometric_gate):
            row_geom_candidate[ref_idx] = True
        else:
            continue

        mins, _maxs = mask_data["bbox"]
        local_ref_mask = mask_data["mask"]
        shifted_ref_mask, shifted_current_region = extract_shifted_overlap_region(
            current_regions=current_regions,
            local_reference_mask=local_ref_mask,
            bbox_mins=np.asarray(mins, dtype=np.int64),
            reference_center=ref_centers[ref_idx],
            expected_center=predicted,
        )
        if shifted_ref_mask.size == 0:
            continue

        gated_curr_indices = np.where(geometric_gate)[0]
        for curr_idx in gated_curr_indices:
            curr_id = int(curr_ids[curr_idx])
            iou, dice_local, _local_volume = _compute_overlap_metrics(
                shifted_ref_mask,
                shifted_current_region,
                curr_id,
            )
            volume_ratio = float(volume_ratios[curr_idx])
            adaptive_iou_gate = min(float(gate_iou_min), max(1e-4, 0.5 * min(volume_ratio, 1.0)))
            if iou < adaptive_iou_gate:
                continue

            row_valid_candidate[ref_idx] = True
            center_dist = float(dists[curr_idx])
            curr_volume = float(curr_volumes[curr_idx])
            cost = compute_pair_cost(
                iou=iou,
                center_dist=center_dist,
                reference_volume=ref_volume,
                current_volume=curr_volume,
                dice_local=dice_local,
                cost_weights=cost_weights,
                center_gate_radius=center_gate_radius,
                volume_cost_mode=volume_cost_mode,
                volume_cost_sigma=volume_cost_sigma,
            )
            cost_matrix[ref_idx, curr_idx] = min(cost_matrix[ref_idx, curr_idx], cost)
            pair_metrics[(ref_idx, curr_idx)] = {
                "iou": iou,
                "dice_local": dice_local,
                "center_dist": center_dist,
                "norm_center_dist": center_dist / center_gate_radius,
                "volume_ratio": volume_ratio,
                "volume_penalty": bounded_volume_penalty(
                    reference_volume=ref_volume,
                    current_volume=curr_volume,
                    mode=volume_cost_mode,
                    gaussian_sigma=volume_cost_sigma,
                ),
                "cost": cost,
            }

    return {
        "cost_matrix": cost_matrix,
        "pair_metrics": pair_metrics,
        "row_geom_candidate": row_geom_candidate,
        "row_valid_candidate": row_valid_candidate,
    }


def _solve_with_lapjv(
    cost_matrix: np.ndarray,
    invalid_cost: float = INVALID_COST,
) -> List[Tuple[int, int, float]]:
    """Solve assignment with lapjv/lap package, handling rectangular matrices by padding."""
    if not HAS_LAPJV or _LAPJV_SOLVER is None:
        raise RuntimeError("lapjv solver requested but lapjv/lap is unavailable")

    n_ref, n_curr = cost_matrix.shape
    if n_ref == 0 or n_curr == 0:
        return []

    n = max(n_ref, n_curr)
    padded = np.full((n, n), float(invalid_cost), dtype=np.float64)
    padded[:n_ref, :n_curr] = cost_matrix

    result = _LAPJV_SOLVER(padded)
    row_to_col: Optional[np.ndarray] = None

    if isinstance(result, tuple):
        if len(result) == 3 and np.isscalar(result[0]):
            row_to_col = np.asarray(result[1], dtype=np.int64)
        elif len(result) >= 2:
            first = np.asarray(result[0])
            second = np.asarray(result[1])
            if first.ndim == 1 and first.shape[0] == n:
                row_to_col = first.astype(np.int64)
            elif second.ndim == 1 and second.shape[0] == n:
                row_to_col = second.astype(np.int64)
    if row_to_col is None:
        raise RuntimeError("Unexpected lapjv return signature")

    matches: List[Tuple[int, int, float]] = []
    for ref_idx in range(n_ref):
        curr_idx = int(row_to_col[ref_idx])
        if curr_idx < 0 or curr_idx >= n_curr:
            continue
        cost = float(cost_matrix[ref_idx, curr_idx])
        if cost < invalid_cost * 0.5:
            matches.append((ref_idx, curr_idx, cost))
    return matches


def _solve_with_scipy(
    cost_matrix: np.ndarray,
    invalid_cost: float = INVALID_COST,
) -> List[Tuple[int, int, float]]:
    """Solve assignment with scipy's Hungarian implementation."""
    if not HAS_SCIPY:
        raise RuntimeError("scipy solver requested but scipy is unavailable")

    n_ref, n_curr = cost_matrix.shape
    if n_ref == 0 or n_curr == 0:
        return []

    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    matches: List[Tuple[int, int, float]] = []
    for ref_idx, curr_idx in zip(row_ind, col_ind):
        cost = float(cost_matrix[ref_idx, curr_idx])
        if cost < invalid_cost * 0.5:
            matches.append((int(ref_idx), int(curr_idx), cost))
    return matches


def solve_global_assignment(
    cost_matrix: np.ndarray,
    assign_solver: str = "lapjv",
    invalid_cost: float = INVALID_COST,
) -> Tuple[List[Tuple[int, int, float]], str]:
    """
    Solve one-to-one global assignment on a dense cost matrix.

    Returns:
        (matches, solver_used), where matches are (ref_idx, curr_idx, cost).
    """
    if cost_matrix.size == 0:
        return [], assign_solver

    solver = (assign_solver or "lapjv").lower()
    if solver == "lapjv":
        if HAS_LAPJV:
            try:
                return _solve_with_lapjv(cost_matrix, invalid_cost=invalid_cost), "lapjv"
            except Exception:
                if not HAS_SCIPY:
                    raise
        if HAS_SCIPY:
            return _solve_with_scipy(cost_matrix, invalid_cost=invalid_cost), "scipy"
        raise RuntimeError("No assignment solver available (lapjv and scipy are both unavailable)")

    if solver == "scipy":
        if HAS_SCIPY:
            return _solve_with_scipy(cost_matrix, invalid_cost=invalid_cost), "scipy"
        if HAS_LAPJV:
            return _solve_with_lapjv(cost_matrix, invalid_cost=invalid_cost), "lapjv"
        raise RuntimeError("No assignment solver available (scipy and lapjv are both unavailable)")

    raise ValueError(f"Unsupported assignment solver: {assign_solver}")


def update_tracks_with_hysteresis(
    tracking: PoreTrackingResult,
    reference_snapshot: PNMSnapshot,
    current_center_map: Dict[int, np.ndarray],
    match_results: Dict[int, Dict[str, object]],
    max_misses: int,
    iou_threshold: float,
    compression_threshold: float,
    center_smoothing: float,
    predicted_centers: Optional[Dict[int, np.ndarray]] = None,
    track_predictors: Optional[Dict[int, ConstantAccelerationKalman3D]] = None,
    closed_reason_token: str = "closed_by_compression",
    kalman_brake_velocity_decay: float = 0.75,
    kalman_brake_acceleration_decay: float = 0.35,
    kalman_freeze_after_misses: int = 3,
) -> Dict[int, int]:
    """Update time-series histories with consecutive-miss hysteresis."""
    id_map: Dict[int, int] = {}
    ref_ids = np.asarray(reference_snapshot.pore_ids, dtype=np.int32)
    ref_centers = np.asarray(reference_snapshot.pore_centers, dtype=np.float64)
    predicted_center_map = predicted_centers or {}
    predictor_map = track_predictors or {}

    for i, ref_id_raw in enumerate(ref_ids):
        ref_id = int(ref_id_raw)
        info = match_results.get(ref_id, {"matched_id": -1, "reason": "unmatched"})
        predictor = predictor_map.get(ref_id)

        history = tracking.center_history.setdefault(ref_id, [])
        prev_center = np.array(history[-1], dtype=np.float64) if history else ref_centers[i]

        tracking.volume_history.setdefault(ref_id, [])
        tracking.status_history.setdefault(ref_id, [])
        tracking.iou_history.setdefault(ref_id, [])
        tracking.match_confidence.setdefault(ref_id, [])
        tracking.unmatched_reason.setdefault(ref_id, [])
        tracking.miss_count.setdefault(ref_id, 0)

        matched_id = int(info.get("matched_id", -1))
        if matched_id > 0:
            iou = float(info.get("iou", 0.0))
            current_volume = float(info.get("current_volume", 0.0))
            volume_ratio = float(info.get("volume_ratio", 0.0))
            cost = float(info.get("cost", 1.0))

            tracking.miss_count[ref_id] = 0
            if iou >= iou_threshold and volume_ratio >= compression_threshold:
                status = PoreStatus.ACTIVE
            else:
                status = PoreStatus.COMPRESSED

            confidence = float(np.clip(1.0 - cost, 0.0, 1.0))
            tracking.volume_history[ref_id].append(current_volume)
            tracking.status_history[ref_id].append(status)
            tracking.iou_history[ref_id].append(iou)
            tracking.match_confidence[ref_id].append(confidence)
            tracking.unmatched_reason[ref_id].append("matched")
            id_map[ref_id] = matched_id

            if matched_id in current_center_map:
                curr_center = np.asarray(current_center_map[matched_id], dtype=np.float64)
                if predictor is not None:
                    filtered_center = predictor.update(curr_center)
                    history.append(filtered_center.tolist())
                else:
                    smoothed = prev_center + center_smoothing * (curr_center - prev_center)
                    history.append(smoothed.tolist())
            else:
                if predictor is not None:
                    history.append(predictor.current_position().tolist())
                else:
                    history.append(prev_center.tolist())
        else:
            misses = int(tracking.miss_count.get(ref_id, 0)) + 1
            tracking.miss_count[ref_id] = misses
            reason = str(info.get("reason", "unmatched"))
            if reason == closed_reason_token:
                status = PoreStatus.COMPRESSED
                carry_volume = 0.0
            else:
                status = PoreStatus.ACTIVE if misses <= int(max_misses) else PoreStatus.COMPRESSED
                prev_volume = tracking.volume_history[ref_id][-1] if tracking.volume_history[ref_id] else 0.0
                carry_volume = float(prev_volume if status == PoreStatus.ACTIVE else 0.0)
            if predictor is not None:
                predicted_center = predictor.current_position()
            else:
                predicted_center = np.asarray(predicted_center_map.get(ref_id, prev_center), dtype=np.float64)

            tracking.volume_history[ref_id].append(carry_volume)
            tracking.status_history[ref_id].append(status)
            tracking.iou_history[ref_id].append(0.0)
            tracking.match_confidence[ref_id].append(0.0)
            tracking.unmatched_reason[ref_id].append(reason)
            id_map[ref_id] = -1
            if predictor is not None:
                predictor.apply_brake(
                    miss_count=misses,
                    velocity_decay=kalman_brake_velocity_decay,
                    acceleration_decay=kalman_brake_acceleration_decay,
                    freeze_after_misses=kalman_freeze_after_misses,
                )
            history.append(predicted_center.tolist())

    return id_map


class PNMTracker:
    """
    Tracks pore network changes across 4D CT time series.

    This tracker maintains correspondence between pores in a reference
    snapshot (t=0) and all subsequent timepoints.
    """

    def __init__(
        self,
        iou_threshold: float = None,
        compression_threshold: float = None,
        use_gpu: bool = None,
        use_batch: bool = None,
        use_hungarian: bool = None,
        match_mode: str = None,
        assign_solver: str = None,
        cost_weights: Tuple[float, float, float, float] = None,
        max_misses: int = None,
        gating_params: Dict[str, float] = None,
    ):
        self.iou_threshold = iou_threshold if iou_threshold is not None else TRACKING_IOU_THRESHOLD
        self.compression_threshold = (
            compression_threshold if compression_threshold is not None else TRACKING_COMPRESSION_THRESHOLD
        )

        self.use_gpu = use_gpu if use_gpu is not None else (TRACKING_USE_GPU and HAS_GPU)
        self.use_batch = use_batch if use_batch is not None else TRACKING_USE_BATCH
        self.center_smoothing = float(TRACKING_CENTER_SMOOTHING)

        resolved_match_mode = (match_mode or TRACKING_MATCH_MODE).lower()
        if use_hungarian is not None:
            warnings.warn(
                "use_hungarian is deprecated. Use match_mode='global_iou_legacy' instead.",
                DeprecationWarning,
                stacklevel=2,
            )
            if use_hungarian:
                resolved_match_mode = "global_iou_legacy"
        else:
            if TRACKING_USE_HUNGARIAN:
                resolved_match_mode = "global_iou_legacy"

        if resolved_match_mode not in {"temporal_global", "legacy_greedy", "global_iou_legacy"}:
            warnings.warn(
                f"Unknown match_mode='{resolved_match_mode}', fallback to temporal_global",
                RuntimeWarning,
            )
            resolved_match_mode = "temporal_global"
        self.match_mode = resolved_match_mode

        self.assign_solver = (assign_solver or TRACKING_ASSIGN_SOLVER).lower()
        if self.assign_solver not in {"lapjv", "scipy"}:
            warnings.warn(
                f"Unknown assign_solver='{self.assign_solver}', fallback to lapjv",
                RuntimeWarning,
            )
            self.assign_solver = "lapjv"
        if self.assign_solver == "lapjv" and not HAS_LAPJV and HAS_SCIPY:
            warnings.warn("lapjv requested but unavailable, falling back to scipy solver")
            self.assign_solver = "scipy"
        elif self.assign_solver == "scipy" and not HAS_SCIPY and HAS_LAPJV:
            warnings.warn("scipy solver requested but unavailable, falling back to lapjv")
            self.assign_solver = "lapjv"

        if self.assign_solver == "lapjv" and not HAS_LAPJV and not HAS_SCIPY:
            raise RuntimeError("No assignment solver available (lapjv/scipy both unavailable)")
        if self.assign_solver == "scipy" and not HAS_SCIPY and not HAS_LAPJV:
            raise RuntimeError("No assignment solver available (scipy/lapjv both unavailable)")

        weights = cost_weights if cost_weights is not None else TRACKING_COST_WEIGHTS
        if len(weights) != 4:
            warnings.warn("cost_weights must contain 4 values. Falling back to default weights.")
            weights = TRACKING_COST_WEIGHTS
        self.cost_weights = tuple(float(w) for w in weights)

        self.max_misses = int(max_misses if max_misses is not None else TRACKING_MAX_MISSES)
        gates = gating_params or {}
        self.gate_center_radius_factor = float(
            gates.get("center_radius_factor", TRACKING_GATE_CENTER_RADIUS_FACTOR)
        )
        self.gate_volume_ratio_min = float(gates.get("volume_ratio_min", TRACKING_GATE_VOLUME_RATIO_MIN))
        self.gate_volume_ratio_max = float(gates.get("volume_ratio_max", TRACKING_GATE_VOLUME_RATIO_MAX))
        self.gate_iou_min = float(gates.get("iou_min", TRACKING_GATE_IOU_MIN))
        self.gate_volume_ratio_min_floor = float(
            gates.get("volume_ratio_min_floor", TRACKING_GATE_VOLUME_RATIO_MIN_FLOOR)
        )

        self.volume_cost_mode = str(gates.get("volume_cost_mode", TRACKING_VOLUME_COST_MODE))
        self.volume_cost_sigma = float(
            gates.get("volume_cost_sigma", TRACKING_VOLUME_COST_GAUSSIAN_SIGMA)
        )

        self.enable_macro_registration = bool(
            gates.get("enable_macro_registration", TRACKING_ENABLE_MACRO_REGISTRATION)
        )
        self.macro_reg_smoothing_sigma = float(
            gates.get("macro_reg_smoothing_sigma", TRACKING_MACRO_REG_SMOOTHING_SIGMA)
        )
        self.macro_reg_upsample_factor = int(
            gates.get("macro_reg_upsample_factor", TRACKING_MACRO_REG_UPSAMPLE_FACTOR)
        )
        self.macro_reg_use_gpu = bool(
            gates.get("macro_reg_use_gpu", TRACKING_MACRO_REG_USE_GPU)
        )
        self.macro_reg_gpu_min_mb = float(
            gates.get("macro_reg_gpu_min_mb", TRACKING_MACRO_REG_GPU_MIN_MB)
        )

        self.kalman_process_noise = float(
            gates.get("kalman_process_noise", TRACKING_KALMAN_PROCESS_NOISE)
        )
        self.kalman_measurement_noise = float(
            gates.get("kalman_measurement_noise", TRACKING_KALMAN_MEASUREMENT_NOISE)
        )
        self.kalman_brake_velocity_decay = float(
            gates.get("kalman_brake_velocity_decay", TRACKING_KALMAN_BRAKE_VELOCITY_DECAY)
        )
        self.kalman_brake_acceleration_decay = float(
            gates.get("kalman_brake_acceleration_decay", TRACKING_KALMAN_BRAKE_ACCEL_DECAY)
        )
        self.kalman_freeze_after_misses = int(
            gates.get("kalman_freeze_after_misses", TRACKING_KALMAN_FREEZE_AFTER_MISSES)
        )

        self.closure_volume_ratio_threshold = float(
            gates.get("closure_volume_ratio_threshold", TRACKING_CLOSURE_VOLUME_RATIO_THRESHOLD)
        )
        self.closure_min_volume_voxels = float(
            gates.get("closure_min_volume_voxels", TRACKING_CLOSURE_MIN_VOLUME_VOXELS)
        )
        self.closure_strain_threshold = float(
            gates.get("closure_strain_threshold", TRACKING_CLOSURE_STRAIN_THRESHOLD)
        )

        if self.use_gpu and not HAS_GPU:
            warnings.warn("GPU acceleration requested but CuPy not available, using CPU")
            self.use_gpu = False

        self.use_hungarian = self.match_mode == "global_iou_legacy"
        self.time_series: TimeSeriesPNM = TimeSeriesPNM()
        self._reference_masks: Dict[int, Dict] = {}
        self._track_predictors: Dict[int, ConstantAccelerationKalman3D] = {}

        algo_parts = [self.match_mode, f"solver={self.assign_solver}"]
        if self.use_gpu and self.match_mode == "legacy_greedy":
            algo_parts.append("GPU")
        if self.use_batch and self.match_mode == "legacy_greedy":
            algo_parts.append("Batch")
        print(f"[Tracker] Algorithm: {'+'.join(algo_parts)}")

    def set_reference(self, snapshot: PNMSnapshot) -> None:
        snapshot.time_index = 0
        self.time_series.reference_snapshot = snapshot
        self.time_series.snapshots = [snapshot]

        self.time_series.tracking = PoreTrackingResult(reference_ids=snapshot.pore_ids.tolist())
        self._track_predictors.clear()

        for pore_id_raw, volume in zip(snapshot.pore_ids, snapshot.pore_volumes):
            pore_id = int(pore_id_raw)
            self.time_series.tracking.volume_history[pore_id] = [float(volume)]
            self.time_series.tracking.status_history[pore_id] = [PoreStatus.ACTIVE]
            self.time_series.tracking.iou_history[pore_id] = [1.0]
            self.time_series.tracking.match_confidence[pore_id] = [1.0]
            self.time_series.tracking.miss_count[pore_id] = 0
            self.time_series.tracking.unmatched_reason[pore_id] = ["reference"]
            idx = np.where(snapshot.pore_ids == pore_id)[0]
            if len(idx) > 0:
                center = snapshot.pore_centers[int(idx[0])]
                self.time_series.tracking.center_history[pore_id] = [center.tolist()]
                self._track_predictors[pore_id] = ConstantAccelerationKalman3D(
                    initial_position=center,
                    process_noise=self.kalman_process_noise,
                    measurement_noise=self.kalman_measurement_noise,
                )

        if snapshot.segmented_regions is not None:
            self._cache_reference_masks(snapshot)

        print(f"[Tracker] Reference set: {snapshot.num_pores} pores")

    def _cache_reference_masks(self, snapshot: PNMSnapshot) -> None:
        self._reference_masks.clear()
        regions = snapshot.segmented_regions
        if regions is None:
            return

        for pore_id in snapshot.pore_ids:
            mask = regions == pore_id
            coords = np.argwhere(mask)
            if len(coords) == 0:
                continue
            mins = coords.min(axis=0)
            maxs = coords.max(axis=0) + 1
            self._reference_masks[int(pore_id)] = {
                "bbox": (mins, maxs),
                "mask": mask[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]],
            }

    def _predict_center(self, ref_id: int, fallback: np.ndarray) -> np.ndarray:
        predictor = self._track_predictors.get(ref_id)
        if predictor is not None:
            return predictor.predict()

        history = self.time_series.tracking.center_history.get(ref_id, [])
        if len(history) >= 2:
            c_prev = np.asarray(history[-2], dtype=np.float64)
            c_last = np.asarray(history[-1], dtype=np.float64)
            return c_last + (c_last - c_prev)
        if len(history) == 1:
            return np.asarray(history[-1], dtype=np.float64)
        return np.asarray(fallback, dtype=np.float64)

    def _build_current_maps(self, snapshot: PNMSnapshot) -> Dict[str, object]:
        current_center_map: Dict[int, np.ndarray] = {}
        current_volume_map: Dict[int, float] = {}
        if snapshot.pore_ids is None:
            return {"center_map": current_center_map, "volume_map": current_volume_map}

        if snapshot.pore_centers is not None:
            for i, pid in enumerate(snapshot.pore_ids):
                current_center_map[int(pid)] = np.asarray(snapshot.pore_centers[i], dtype=np.float64)
        if snapshot.pore_volumes is not None:
            for i, pid in enumerate(snapshot.pore_ids):
                current_volume_map[int(pid)] = float(snapshot.pore_volumes[i])
        return {"center_map": current_center_map, "volume_map": current_volume_map}

    def _estimate_macro_registration(
        self,
        previous_snapshot: Optional[PNMSnapshot],
        current_snapshot: PNMSnapshot,
    ) -> MacroRegistrationResult:
        if not self.enable_macro_registration:
            return MacroRegistrationResult(
                displacement=np.zeros(3, dtype=np.float64),
                method="disabled",
                confidence=0.0,
            )

        previous_regions = previous_snapshot.segmented_regions if previous_snapshot is not None else None
        current_regions = current_snapshot.segmented_regions
        return estimate_macro_registration(
            reference_regions=previous_regions,
            current_regions=current_regions,
            smoothing_sigma=self.macro_reg_smoothing_sigma,
            upsample_factor=self.macro_reg_upsample_factor,
            use_gpu=(self.use_gpu and self.macro_reg_use_gpu),
            gpu_min_size_mb=self.macro_reg_gpu_min_mb,
        )

    def _compose_expected_center(
        self,
        ref_id: int,
        motion_prediction: np.ndarray,
        macro_registration: MacroRegistrationResult,
    ) -> np.ndarray:
        """
        Compose gating center from KF prediction and macro displacement.

        To avoid drift, macro shift is weakly blended for well-tracked pores and
        only strongly applied after consecutive misses.
        """
        base = np.asarray(motion_prediction, dtype=np.float64)
        confidence = float(np.clip(macro_registration.confidence, 0.0, 1.0))
        misses = int(self.time_series.tracking.miss_count.get(ref_id, 0))
        history_len = len(self.time_series.tracking.center_history.get(ref_id, []))

        if confidence <= 0.0:
            return base

        if history_len <= 1:
            # Cold start: rely on macro field to survive first-frame large translations.
            macro_weight = 1.00
        elif misses <= 0:
            # Stable tracking: avoid injecting macro-field noise into KF trajectory.
            macro_weight = 0.00
        elif misses == 1:
            macro_weight = 0.50 * confidence
        else:
            macro_weight = 1.00 * confidence

        return base + macro_weight * np.asarray(macro_registration.displacement, dtype=np.float64)

    def _apply_closure_classification(
        self,
        reference_snapshot: PNMSnapshot,
        match_results: Dict[int, Dict[str, object]],
        predicted_centers: Dict[int, np.ndarray],
        macro_registration: MacroRegistrationResult,
    ) -> None:
        tracking = self.time_series.tracking
        ref_volume_map = {
            int(pid): float(vol)
            for pid, vol in zip(reference_snapshot.pore_ids, reference_snapshot.pore_volumes)
        }
        ref_center_map = {
            int(pid): np.asarray(center, dtype=np.float64)
            for pid, center in zip(reference_snapshot.pore_ids, reference_snapshot.pore_centers)
        }

        for ref_id_raw in reference_snapshot.pore_ids:
            ref_id = int(ref_id_raw)
            info = match_results.get(ref_id)
            if info is None:
                continue
            if int(info.get("matched_id", -1)) > 0:
                continue

            ref_volume = ref_volume_map.get(ref_id, 0.0)
            prev_series = tracking.volume_history.get(ref_id, [])
            previous_volume = float(prev_series[-1]) if prev_series else ref_volume
            predicted_center = np.asarray(
                predicted_centers.get(ref_id, ref_center_map.get(ref_id, np.zeros(3, dtype=np.float64))),
                dtype=np.float64,
            )
            local_compression = macro_registration.sample_compression(predicted_center, default=0.0)

            if should_mark_closed_by_compression(
                previous_volume=previous_volume,
                reference_volume=ref_volume,
                local_compression=local_compression,
                volume_ratio_threshold=self.closure_volume_ratio_threshold,
                min_volume_voxels=self.closure_min_volume_voxels,
                compression_threshold=self.closure_strain_threshold,
            ):
                info["reason"] = "closed_by_compression"
                info["local_compression"] = float(local_compression)
                info["matched_id"] = -1
                match_results[ref_id] = info

    def track_snapshot(
        self,
        snapshot: PNMSnapshot,
        callback: Optional[Callable[[int, str], None]] = None,
    ) -> None:
        if self.time_series.reference_snapshot is None:
            raise ValueError("Reference snapshot not set. Call set_reference first.")
        if snapshot.segmented_regions is None:
            raise ValueError("Snapshot must include segmented_regions for tracking.")
        if not self._reference_masks:
            raise ValueError("Reference masks are missing. Ensure reference snapshot includes segmented_regions.")

        start_time = time.time()
        time_index = len(self.time_series.snapshots)
        snapshot.time_index = time_index

        if self.match_mode == "legacy_greedy":
            id_map = self._track_snapshot_legacy(snapshot, callback)
            algo_desc = "legacy_greedy"
        elif self.match_mode == "global_iou_legacy":
            id_map = self._track_snapshot_global_iou(snapshot, callback)
            algo_desc = "global_iou_legacy"
        else:
            id_map = self._track_snapshot_temporal_global(snapshot, callback)
            algo_desc = "temporal_global"

        self.time_series.tracking.id_mapping[time_index] = id_map
        self.time_series.snapshots.append(snapshot)

        num_pores = len(self.time_series.reference_snapshot.pore_ids)
        num_compressed = sum(
            1
            for statuses in self.time_series.tracking.status_history.values()
            if statuses and statuses[-1] == PoreStatus.COMPRESSED
        )
        num_active = num_pores - num_compressed

        elapsed = time.time() - start_time
        print(
            f"[Tracker] t={time_index}: {num_active} active, {num_compressed} compressed "
            f"({elapsed:.2f}s, {algo_desc})"
        )
        if callback:
            callback(100, f"Tracked {num_pores} pores")

    def _track_snapshot_temporal_global(
        self,
        snapshot: PNMSnapshot,
        callback: Optional[Callable[[int, str], None]] = None,
    ) -> Dict[int, int]:
        ref_snapshot = self.time_series.reference_snapshot
        if ref_snapshot is None:
            return {}

        previous_snapshot = self.time_series.snapshots[-1] if self.time_series.snapshots else ref_snapshot
        macro_registration = self._estimate_macro_registration(previous_snapshot, snapshot)
        if callback:
            callback(20, f"Macro registration: {macro_registration.method}")

        maps = self._build_current_maps(snapshot)
        current_center_map = maps["center_map"]
        current_volume_map = maps["volume_map"]

        predicted_centers: Dict[int, np.ndarray] = {}
        for i, ref_id_raw in enumerate(ref_snapshot.pore_ids):
            ref_id = int(ref_id_raw)
            motion_prediction = self._predict_center(ref_id, ref_snapshot.pore_centers[i])
            predicted_centers[ref_id] = self._compose_expected_center(
                ref_id=ref_id,
                motion_prediction=motion_prediction,
                macro_registration=macro_registration,
            )

        if callback:
            callback(35, "Building TGGA candidates...")

        candidate_data = build_candidates(
            reference_snapshot=ref_snapshot,
            current_snapshot=snapshot,
            reference_masks=self._reference_masks,
            current_regions=snapshot.segmented_regions,
            predicted_centers=predicted_centers,
            cost_weights=self.cost_weights,
            gate_center_radius_factor=self.gate_center_radius_factor,
            gate_volume_ratio_min=self.gate_volume_ratio_min,
            gate_volume_ratio_max=self.gate_volume_ratio_max,
            gate_iou_min=self.gate_iou_min,
            gate_volume_ratio_min_floor=self.gate_volume_ratio_min_floor,
            volume_cost_mode=self.volume_cost_mode,
            volume_cost_sigma=self.volume_cost_sigma,
        )

        cost_matrix = candidate_data["cost_matrix"]
        pair_metrics = candidate_data["pair_metrics"]
        row_geom = candidate_data["row_geom_candidate"]
        row_valid = candidate_data["row_valid_candidate"]
        curr_ids = (
            np.asarray(snapshot.pore_ids, dtype=np.int32)
            if snapshot.pore_ids is not None
            else np.array([], dtype=np.int32)
        )

        if callback:
            callback(55, "Solving global assignment...")

        assignments, solver_used = solve_global_assignment(
            cost_matrix=cost_matrix,
            assign_solver=self.assign_solver,
            invalid_cost=INVALID_COST,
        )

        match_results: Dict[int, Dict[str, object]] = {}
        for ref_idx, ref_id_raw in enumerate(ref_snapshot.pore_ids):
            ref_id = int(ref_id_raw)
            if not bool(row_geom[ref_idx]):
                reason = "gate_rejected"
            elif not bool(row_valid[ref_idx]):
                reason = "iou_below_gate"
            else:
                reason = "not_selected_by_global_assignment"
            match_results[ref_id] = {"matched_id": -1, "reason": reason}

        for ref_idx, curr_idx, cost in assignments:
            metrics = pair_metrics.get((ref_idx, curr_idx))
            if metrics is None:
                continue
            ref_id = int(ref_snapshot.pore_ids[ref_idx])
            curr_id = int(curr_ids[curr_idx]) if curr_idx < len(curr_ids) else -1
            match_results[ref_id] = {
                "matched_id": curr_id,
                "iou": float(metrics["iou"]),
                "dice_local": float(metrics["dice_local"]),
                "volume_ratio": float(metrics["volume_ratio"]),
                "current_volume": float(current_volume_map.get(curr_id, 0.0)),
                "cost": float(cost),
                "reason": f"matched_{solver_used}",
            }

        self._apply_closure_classification(
            reference_snapshot=ref_snapshot,
            match_results=match_results,
            predicted_centers=predicted_centers,
            macro_registration=macro_registration,
        )

        id_map = update_tracks_with_hysteresis(
            tracking=self.time_series.tracking,
            reference_snapshot=ref_snapshot,
            current_center_map=current_center_map,
            match_results=match_results,
            max_misses=self.max_misses,
            iou_threshold=self.iou_threshold,
            compression_threshold=self.compression_threshold,
            center_smoothing=self.center_smoothing,
            predicted_centers=predicted_centers,
            track_predictors=self._track_predictors,
            kalman_brake_velocity_decay=self.kalman_brake_velocity_decay,
            kalman_brake_acceleration_decay=self.kalman_brake_acceleration_decay,
            kalman_freeze_after_misses=self.kalman_freeze_after_misses,
        )
        if callback:
            callback(85, f"TGGA matched {len(assignments)} tracks ({solver_used})")
        return id_map

    def _build_global_iou_matrix(self, snapshot: PNMSnapshot) -> Tuple[np.ndarray, Dict[int, float]]:
        ref_snapshot = self.time_series.reference_snapshot
        if ref_snapshot is None:
            return np.zeros((0, 0), dtype=np.float64), {}

        ref_ids = np.asarray(ref_snapshot.pore_ids, dtype=np.int32)
        curr_ids = (
            np.asarray(snapshot.pore_ids, dtype=np.int32)
            if snapshot.pore_ids is not None
            else np.array([], dtype=np.int32)
        )
        iou_matrix = np.zeros((len(ref_ids), len(curr_ids)), dtype=np.float64)

        curr_volume_map: Dict[int, float] = {}
        if snapshot.pore_volumes is not None and snapshot.pore_ids is not None:
            for i, pid in enumerate(snapshot.pore_ids):
                curr_volume_map[int(pid)] = float(snapshot.pore_volumes[i])

        if len(curr_ids) == 0:
            return iou_matrix, curr_volume_map

        curr_index_map = {int(pid): idx for idx, pid in enumerate(curr_ids)}
        current_regions = snapshot.segmented_regions

        for ref_idx, ref_id_raw in enumerate(ref_ids):
            ref_id = int(ref_id_raw)
            mask_data = self._reference_masks.get(ref_id)
            if mask_data is None:
                continue
            mins, maxs = mask_data["bbox"]
            local_mask = mask_data["mask"]
            current_region = current_regions[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]]
            overlaps = current_region[local_mask]
            overlaps = overlaps[overlaps > 0]
            if len(overlaps) == 0:
                continue
            unique_labels = np.unique(overlaps)
            for label in unique_labels:
                curr_idx = curr_index_map.get(int(label))
                if curr_idx is None:
                    continue
                iou, _dice, _local_vol = _compute_overlap_metrics(local_mask, current_region, int(label))
                iou_matrix[ref_idx, curr_idx] = max(iou_matrix[ref_idx, curr_idx], iou)

        return iou_matrix, curr_volume_map

    def _track_snapshot_global_iou(
        self,
        snapshot: PNMSnapshot,
        callback: Optional[Callable[[int, str], None]] = None,
    ) -> Dict[int, int]:
        ref_snapshot = self.time_series.reference_snapshot
        if ref_snapshot is None:
            return {}

        if callback:
            callback(25, "Building global IoU matrix...")

        previous_snapshot = self.time_series.snapshots[-1] if self.time_series.snapshots else ref_snapshot
        macro_registration = self._estimate_macro_registration(previous_snapshot, snapshot)
        predicted_centers: Dict[int, np.ndarray] = {}
        for i, ref_id_raw in enumerate(ref_snapshot.pore_ids):
            ref_id = int(ref_id_raw)
            motion_prediction = self._predict_center(ref_id, ref_snapshot.pore_centers[i])
            predicted_centers[ref_id] = self._compose_expected_center(
                ref_id=ref_id,
                motion_prediction=motion_prediction,
                macro_registration=macro_registration,
            )

        maps = self._build_current_maps(snapshot)
        current_center_map = maps["center_map"]

        iou_matrix, current_volume_map = self._build_global_iou_matrix(snapshot)
        if iou_matrix.size == 0:
            match_results = {
                int(ref_id): {"matched_id": -1, "reason": "empty_iou_matrix"}
                for ref_id in ref_snapshot.pore_ids
            }
            return update_tracks_with_hysteresis(
                tracking=self.time_series.tracking,
                reference_snapshot=ref_snapshot,
                current_center_map=current_center_map,
                match_results=match_results,
                max_misses=self.max_misses,
                iou_threshold=self.iou_threshold,
                compression_threshold=self.compression_threshold,
                center_smoothing=self.center_smoothing,
                predicted_centers=predicted_centers,
                track_predictors=self._track_predictors,
                kalman_brake_velocity_decay=self.kalman_brake_velocity_decay,
                kalman_brake_acceleration_decay=self.kalman_brake_acceleration_decay,
                kalman_freeze_after_misses=self.kalman_freeze_after_misses,
            )

        cost_matrix = 1.0 - iou_matrix
        cost_matrix[iou_matrix < self.iou_threshold] = INVALID_COST
        assignments, solver_used = solve_global_assignment(
            cost_matrix=cost_matrix,
            assign_solver=self.assign_solver,
            invalid_cost=INVALID_COST,
        )

        curr_ids = (
            np.asarray(snapshot.pore_ids, dtype=np.int32)
            if snapshot.pore_ids is not None
            else np.array([], dtype=np.int32)
        )
        ref_vol_map = {int(pid): float(v) for pid, v in zip(ref_snapshot.pore_ids, ref_snapshot.pore_volumes)}

        match_results: Dict[int, Dict[str, object]] = {
            int(ref_id): {"matched_id": -1, "reason": "iou_below_threshold"}
            for ref_id in ref_snapshot.pore_ids
        }
        for ref_idx, curr_idx, cost in assignments:
            if ref_idx >= iou_matrix.shape[0] or curr_idx >= iou_matrix.shape[1]:
                continue
            iou = float(iou_matrix[ref_idx, curr_idx])
            if iou < self.iou_threshold:
                continue
            ref_id = int(ref_snapshot.pore_ids[ref_idx])
            curr_id = int(curr_ids[curr_idx])
            ref_volume = ref_vol_map.get(ref_id, 0.0)
            curr_volume = float(current_volume_map.get(curr_id, 0.0))
            volume_ratio = curr_volume / ref_volume if ref_volume > 0 else 0.0
            match_results[ref_id] = {
                "matched_id": curr_id,
                "iou": iou,
                "dice_local": iou,
                "volume_ratio": volume_ratio,
                "current_volume": curr_volume,
                "cost": float(cost),
                "reason": f"matched_{solver_used}",
            }

        self._apply_closure_classification(
            reference_snapshot=ref_snapshot,
            match_results=match_results,
            predicted_centers=predicted_centers,
            macro_registration=macro_registration,
        )

        if callback:
            callback(80, f"Global IoU matched {len(assignments)} tracks ({solver_used})")
        return update_tracks_with_hysteresis(
            tracking=self.time_series.tracking,
            reference_snapshot=ref_snapshot,
            current_center_map=current_center_map,
            match_results=match_results,
            max_misses=self.max_misses,
            iou_threshold=self.iou_threshold,
            compression_threshold=self.compression_threshold,
            center_smoothing=self.center_smoothing,
            predicted_centers=predicted_centers,
            track_predictors=self._track_predictors,
            kalman_brake_velocity_decay=self.kalman_brake_velocity_decay,
            kalman_brake_acceleration_decay=self.kalman_brake_acceleration_decay,
            kalman_freeze_after_misses=self.kalman_freeze_after_misses,
        )

    def _track_snapshot_legacy(
        self,
        snapshot: PNMSnapshot,
        callback: Optional[Callable[[int, str], None]] = None,
    ) -> Dict[int, int]:
        ref_snapshot = self.time_series.reference_snapshot
        if ref_snapshot is None:
            return {}
        current_regions = snapshot.segmented_regions

        current_center_map: Dict[int, np.ndarray] = {}
        if snapshot.pore_centers is not None and snapshot.pore_ids is not None:
            for i, pid in enumerate(snapshot.pore_ids):
                current_center_map[int(pid)] = snapshot.pore_centers[i]

        id_map: Dict[int, int] = {}
        num_pores = len(ref_snapshot.pore_ids)
        use_gpu_now = self.use_gpu and num_pores >= TRACKING_GPU_MIN_PORES and HAS_GPU
        use_batch_now = self.use_batch and num_pores >= 50

        if use_batch_now:
            if callback:
                callback(30, f"Computing IoU batch (GPU={use_gpu_now})...")
            ref_masks_list = [self._reference_masks[int(pid)] for pid in ref_snapshot.pore_ids]
            if use_gpu_now:
                matched_ids, iou_scores, volumes = compute_batch_iou_gpu(
                    ref_masks_list, current_regions, ref_snapshot.pore_ids
                )
            else:
                matched_ids, iou_scores, volumes = compute_batch_iou_cpu(
                    ref_masks_list, current_regions, ref_snapshot.pore_ids
                )
            if callback:
                callback(70, f"Processing {num_pores} pore matches...")

            for i, ref_id_raw in enumerate(ref_snapshot.pore_ids):
                ref_id = int(ref_id_raw)
                matched_id = int(matched_ids[i])
                iou = float(iou_scores[i])
                current_volume = float(volumes[i])
                ref_volume = float(ref_snapshot.pore_volumes[i])
                volume_ratio = current_volume / ref_volume if ref_volume > 0 else 0.0

                if iou < self.iou_threshold or volume_ratio < self.compression_threshold:
                    status = PoreStatus.COMPRESSED
                else:
                    status = PoreStatus.ACTIVE

                tracking = self.time_series.tracking
                tracking.volume_history.setdefault(ref_id, []).append(current_volume)
                tracking.status_history.setdefault(ref_id, []).append(status)
                tracking.iou_history.setdefault(ref_id, []).append(iou)
                tracking.match_confidence.setdefault(ref_id, []).append(float(np.clip(iou, 0.0, 1.0)))
                tracking.unmatched_reason.setdefault(ref_id, []).append("matched" if matched_id > 0 else "no_overlap")
                if matched_id > 0:
                    tracking.miss_count[ref_id] = 0
                else:
                    tracking.miss_count[ref_id] = int(tracking.miss_count.get(ref_id, 0)) + 1
                id_map[ref_id] = matched_id if matched_id > 0 else -1

                history = tracking.center_history.setdefault(ref_id, [])
                prev_center = np.array(history[-1]) if history else ref_snapshot.pore_centers[i]
                if matched_id > 0 and matched_id in current_center_map:
                    curr_center = current_center_map[matched_id]
                    smoothed = prev_center + self.center_smoothing * (curr_center - prev_center)
                    history.append(smoothed.tolist())
                else:
                    history.append(prev_center.tolist())
        else:
            total_pores = len(ref_snapshot.pore_ids)
            for i, ref_id_raw in enumerate(ref_snapshot.pore_ids):
                if callback and i % 50 == 0:
                    progress = int(30 + 60 * i / max(total_pores, 1))
                    callback(progress, f"Tracking pore {i+1}/{total_pores}...")

                ref_id = int(ref_id_raw)
                ref_volume = float(ref_snapshot.pore_volumes[i])
                mask_data = self._reference_masks[ref_id]
                mins, maxs = mask_data["bbox"]
                local_mask = mask_data["mask"]
                current_region = current_regions[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]]
                matched_id, iou, current_volume = self._match_pore(local_mask, current_region)
                volume_ratio = current_volume / ref_volume if ref_volume > 0 else 0.0

                if matched_id is None or iou < self.iou_threshold:
                    status = PoreStatus.COMPRESSED
                elif volume_ratio < self.compression_threshold:
                    status = PoreStatus.COMPRESSED
                else:
                    status = PoreStatus.ACTIVE

                tracking = self.time_series.tracking
                tracking.volume_history.setdefault(ref_id, []).append(float(current_volume))
                tracking.status_history.setdefault(ref_id, []).append(status)
                tracking.iou_history.setdefault(ref_id, []).append(float(iou))
                tracking.match_confidence.setdefault(ref_id, []).append(float(np.clip(iou, 0.0, 1.0)))
                tracking.unmatched_reason.setdefault(ref_id, []).append("matched" if matched_id else "no_overlap")
                if matched_id:
                    tracking.miss_count[ref_id] = 0
                else:
                    tracking.miss_count[ref_id] = int(tracking.miss_count.get(ref_id, 0)) + 1
                id_map[ref_id] = int(matched_id) if matched_id else -1

                history = tracking.center_history.setdefault(ref_id, [])
                prev_center = np.array(history[-1]) if history else ref_snapshot.pore_centers[i]
                if matched_id and matched_id in current_center_map:
                    curr_center = current_center_map[matched_id]
                    smoothed = prev_center + self.center_smoothing * (curr_center - prev_center)
                    history.append(smoothed.tolist())
                else:
                    history.append(prev_center.tolist())

        return id_map

    def _match_pore(
        self,
        ref_mask: np.ndarray,
        current_labels: np.ndarray,
    ) -> Tuple[Optional[int], float, float]:
        overlapping_labels = current_labels[ref_mask]
        overlapping_labels = overlapping_labels[overlapping_labels > 0]
        if len(overlapping_labels) == 0:
            return None, 0.0, 0.0

        label_counts = np.bincount(overlapping_labels)
        if len(label_counts) > 1:
            best_label = int(np.argmax(label_counts[1:]) + 1)
        else:
            best_label = int(overlapping_labels[0])

        current_mask = current_labels == best_label
        intersection = np.sum(ref_mask & current_mask)
        union = np.sum(ref_mask | current_mask)
        if union == 0:
            return None, 0.0, 0.0
        iou = float(intersection / union)
        current_volume = float(np.sum(current_mask))
        return best_label, iou, current_volume

    @staticmethod
    def _safe_ratio(numerator: float, denominator: float, default: float = 0.0) -> float:
        if denominator <= 0:
            return float(default)
        return float(numerator / denominator)

    @staticmethod
    def _resolve_labels_path(volume: Any) -> Optional[str]:
        metadata = getattr(volume, "metadata", None)
        if not isinstance(metadata, dict):
            return None
        sim = metadata.get("sim_annotations")
        if not isinstance(sim, dict):
            return None
        files = sim.get("files")
        if not isinstance(files, dict):
            return None
        path = files.get("labels_npy")
        if isinstance(path, str) and path:
            return path
        return None

    def _match_instance_ids(
        self,
        iou_matrix: np.ndarray,
        iou_threshold: float,
    ) -> Tuple[List[Tuple[int, int, float]], str]:
        if iou_matrix.size == 0:
            return [], "none"

        thr = float(max(iou_threshold, 0.0))
        cost = 1.0 - iou_matrix
        cost[iou_matrix < thr] = INVALID_COST

        try:
            assignments, solver_used = solve_global_assignment(
                cost_matrix=cost,
                assign_solver=self.assign_solver,
                invalid_cost=INVALID_COST,
            )
            matched: List[Tuple[int, int, float]] = []
            for row_idx, col_idx, _cost in assignments:
                if row_idx >= iou_matrix.shape[0] or col_idx >= iou_matrix.shape[1]:
                    continue
                score = float(iou_matrix[row_idx, col_idx])
                if score >= thr:
                    matched.append((int(row_idx), int(col_idx), score))
            return matched, solver_used
        except Exception:
            pass

        pairs = np.argwhere(iou_matrix >= thr)
        scored_pairs: List[Tuple[float, int, int]] = [
            (float(iou_matrix[row_idx, col_idx]), int(row_idx), int(col_idx))
            for row_idx, col_idx in pairs
        ]
        scored_pairs.sort(key=lambda item: item[0], reverse=True)

        used_rows = set()
        used_cols = set()
        greedy_matches: List[Tuple[int, int, float]] = []
        for score, row_idx, col_idx in scored_pairs:
            if row_idx in used_rows or col_idx in used_cols:
                continue
            used_rows.add(row_idx)
            used_cols.add(col_idx)
            greedy_matches.append((row_idx, col_idx, score))
        return greedy_matches, "greedy_fallback"

    def _evaluate_step_against_gt(
        self,
        predicted_labels: np.ndarray,
        gt_labels: np.ndarray,
        instance_iou_threshold: float,
    ) -> Dict[str, Any]:
        pred = np.asarray(predicted_labels)
        gt = np.asarray(gt_labels)

        pred_binary = pred > 0
        gt_binary = gt > 0
        tp = int(np.count_nonzero(pred_binary & gt_binary))
        fp = int(np.count_nonzero(pred_binary & (~gt_binary)))
        fn = int(np.count_nonzero((~pred_binary) & gt_binary))

        voxel_precision = self._safe_ratio(tp, tp + fp, default=1.0 if (tp + fp + fn) == 0 else 0.0)
        voxel_recall = self._safe_ratio(tp, tp + fn, default=1.0 if (tp + fp + fn) == 0 else 0.0)
        voxel_iou = self._safe_ratio(tp, tp + fp + fn, default=1.0 if (tp + fp + fn) == 0 else 0.0)
        voxel_dice = self._safe_ratio(2.0 * tp, 2.0 * tp + fp + fn, default=1.0 if (tp + fp + fn) == 0 else 0.0)

        pred_ids_arr, pred_counts_arr = np.unique(pred[pred > 0], return_counts=True)
        gt_ids_arr, gt_counts_arr = np.unique(gt[gt > 0], return_counts=True)
        pred_ids = [int(v) for v in pred_ids_arr.tolist()]
        gt_ids = [int(v) for v in gt_ids_arr.tolist()]

        pred_count_map = {int(pid): int(cnt) for pid, cnt in zip(pred_ids_arr, pred_counts_arr)}
        gt_count_map = {int(gid): int(cnt) for gid, cnt in zip(gt_ids_arr, gt_counts_arr)}
        pred_index = {pid: idx for idx, pid in enumerate(pred_ids)}
        gt_index = {gid: idx for idx, gid in enumerate(gt_ids)}
        iou_matrix = np.zeros((len(pred_ids), len(gt_ids)), dtype=np.float64)

        positive_overlap = pred_binary & gt_binary
        if np.any(positive_overlap) and pred_ids and gt_ids:
            pred_overlap = pred[positive_overlap].astype(np.uint64, copy=False)
            gt_overlap = gt[positive_overlap].astype(np.uint64, copy=False)
            pair_keys = (pred_overlap << np.uint64(32)) | gt_overlap
            pair_unique, pair_counts = np.unique(pair_keys, return_counts=True)

            for pair_key, inter_count_raw in zip(pair_unique, pair_counts):
                key_int = int(pair_key)
                pred_id = int(key_int >> 32)
                gt_id = int(key_int & 0xFFFFFFFF)
                row_idx = pred_index.get(pred_id)
                col_idx = gt_index.get(gt_id)
                if row_idx is None or col_idx is None:
                    continue
                intersection = int(inter_count_raw)
                union = pred_count_map[pred_id] + gt_count_map[gt_id] - intersection
                if union > 0:
                    iou_matrix[row_idx, col_idx] = float(intersection / union)

        matches, solver_used = self._match_instance_ids(
            iou_matrix=iou_matrix,
            iou_threshold=instance_iou_threshold,
        )
        matched_ious = [float(iou) for _, _, iou in matches]

        pred_to_gt: Dict[int, int] = {}
        gt_to_pred: Dict[int, int] = {}
        for row_idx, col_idx, _iou in matches:
            if row_idx < len(pred_ids) and col_idx < len(gt_ids):
                pred_to_gt[int(pred_ids[row_idx])] = int(gt_ids[col_idx])
                gt_to_pred[int(gt_ids[col_idx])] = int(pred_ids[row_idx])

        num_pred = len(pred_ids)
        num_gt = len(gt_ids)
        num_matches = len(matches)
        if num_pred == 0 and num_gt == 0:
            inst_precision = 1.0
            inst_recall = 1.0
            inst_f1 = 1.0
        else:
            inst_precision = self._safe_ratio(num_matches, num_pred, default=0.0)
            inst_recall = self._safe_ratio(num_matches, num_gt, default=0.0)
            inst_f1 = self._safe_ratio(2.0 * inst_precision * inst_recall, inst_precision + inst_recall, default=0.0)

        return {
            "segmentation": {
                "tp_voxels": tp,
                "fp_voxels": fp,
                "fn_voxels": fn,
                "voxel_precision": voxel_precision,
                "voxel_recall": voxel_recall,
                "voxel_iou": voxel_iou,
                "voxel_dice": voxel_dice,
            },
            "instance": {
                "num_pred_instances": num_pred,
                "num_gt_instances": num_gt,
                "num_matches": num_matches,
                "precision": inst_precision,
                "recall": inst_recall,
                "f1": inst_f1,
                "mean_matched_iou": float(np.mean(matched_ious)) if matched_ious else 0.0,
                "iou_threshold": float(instance_iou_threshold),
                "assignment_solver": solver_used,
            },
            "mapping": {
                "pred_to_gt": pred_to_gt,
                "gt_to_pred": gt_to_pred,
                "gt_ids": set(gt_ids),
            },
        }

    def _evaluate_tracking_step(
        self,
        time_index: int,
        id_map: Dict[int, int],
        reference_pred_to_gt: Dict[int, int],
        current_pred_to_gt: Dict[int, int],
        current_gt_ids: set[int],
    ) -> Dict[str, Any]:
        evaluable_refs = len(reference_pred_to_gt)
        if evaluable_refs == 0:
            return {
                "available": False,
                "time_index": int(time_index),
                "reason": "no_reference_pred_to_gt_mapping",
                "per_reference": {},
            }

        expected_present = 0
        expected_absent = 0
        correct_active = 0
        correct_absent = 0
        missed = 0
        id_switched = 0
        unmatched_to_gt = 0
        false_positive_alive = 0
        per_reference: Dict[int, Dict[str, Any]] = {}

        for ref_pred_id, ref_gt_id in reference_pred_to_gt.items():
            matched_pred_id = int(id_map.get(int(ref_pred_id), -1))
            present = int(ref_gt_id) in current_gt_ids

            outcome = "unknown"
            mapped_gt = None
            is_correct = False
            if present:
                expected_present += 1
                if matched_pred_id <= 0:
                    missed += 1
                    outcome = "missed"
                else:
                    mapped_gt = current_pred_to_gt.get(matched_pred_id)
                    if mapped_gt is None:
                        unmatched_to_gt += 1
                        outcome = "unmatched_to_gt"
                    elif int(mapped_gt) == int(ref_gt_id):
                        correct_active += 1
                        outcome = "correct_active"
                        is_correct = True
                    else:
                        id_switched += 1
                        outcome = "id_switched"
            else:
                expected_absent += 1
                if matched_pred_id <= 0:
                    correct_absent += 1
                    outcome = "correct_absent"
                    is_correct = True
                else:
                    false_positive_alive += 1
                    mapped_gt = current_pred_to_gt.get(matched_pred_id)
                    outcome = "false_positive_alive"

            per_reference[int(ref_pred_id)] = {
                "ref_pred_id": int(ref_pred_id),
                "ref_gt_id": int(ref_gt_id),
                "present_in_gt": bool(present),
                "matched_pred_id": int(matched_pred_id),
                "mapped_gt_id": int(mapped_gt) if mapped_gt is not None else None,
                "outcome": outcome,
                "correct": bool(is_correct),
            }

        correct_total = correct_active + correct_absent
        return {
            "available": True,
            "time_index": int(time_index),
            "evaluable_reference_instances": int(evaluable_refs),
            "expected_present": int(expected_present),
            "expected_absent": int(expected_absent),
            "correct_active": int(correct_active),
            "correct_absent": int(correct_absent),
            "missed": int(missed),
            "id_switched": int(id_switched),
            "unmatched_to_gt": int(unmatched_to_gt),
            "false_positive_alive": int(false_positive_alive),
            "accuracy": self._safe_ratio(correct_total, evaluable_refs, default=0.0),
            "active_accuracy": self._safe_ratio(correct_active, expected_present, default=1.0 if expected_present == 0 else 0.0),
            "closure_accuracy": self._safe_ratio(correct_absent, expected_absent, default=1.0 if expected_absent == 0 else 0.0),
            "per_reference": per_reference,
        }

    def evaluate_against_sim_annotations(
        self,
        volumes: List[Any],
        instance_iou_threshold: float = 0.1,
    ) -> Dict[str, Any]:
        """
        Evaluate segmentation + tracking quality against per-step simulation labels.npy.

        The method is non-destructive: missing annotation files produce warnings
        instead of raising, and partial reports are still returned.
        """
        report: Dict[str, Any] = {
            "available": False,
            "status": "unavailable",
            "instance_iou_threshold": float(max(instance_iou_threshold, 0.0)),
            "steps": [],
            "overall": {},
            "errors": [],
            "warnings": [],
        }

        snapshots = self.time_series.snapshots
        if not snapshots:
            report["errors"].append("No snapshots available for evaluation")
            self.time_series.tracking.evaluation = report
            return report

        if not volumes:
            report["errors"].append("No loaded volumes available for evaluation")
            self.time_series.tracking.evaluation = report
            return report

        steps_to_eval = min(len(snapshots), len(volumes))
        if steps_to_eval == 0:
            report["errors"].append("No overlapping steps between snapshots and volumes")
            self.time_series.tracking.evaluation = report
            return report

        if len(snapshots) != len(volumes):
            report["warnings"].append(
                f"snapshot count ({len(snapshots)}) != volume count ({len(volumes)}); evaluating first {steps_to_eval} steps"
            )

        step_maps: Dict[int, Dict[str, Any]] = {}
        step_entries: List[Dict[str, Any]] = []

        for time_index in range(steps_to_eval):
            snapshot = snapshots[time_index]
            step_entry: Dict[str, Any] = {
                "time_index": int(time_index),
                "gt_labels_path": None,
                "evaluated": False,
                "errors": [],
                "warnings": [],
                "segmentation": {},
                "instance": {},
                "tracking": {},
                "mapping": {},
            }

            predicted_labels = snapshot.segmented_regions
            if predicted_labels is None:
                step_entry["errors"].append("snapshot has no segmented_regions")
                step_entries.append(step_entry)
                continue

            labels_path = self._resolve_labels_path(volumes[time_index])
            step_entry["gt_labels_path"] = labels_path
            if not labels_path:
                step_entry["warnings"].append("labels.npy path missing in volume metadata sim_annotations.files.labels_npy")
                step_entries.append(step_entry)
                continue

            try:
                gt_labels = np.load(labels_path, mmap_mode="r")
            except Exception as exc:
                step_entry["errors"].append(f"failed to read labels.npy: {exc}")
                step_entries.append(step_entry)
                continue

            if tuple(gt_labels.shape) != tuple(predicted_labels.shape):
                step_entry["errors"].append(
                    f"shape mismatch: gt={tuple(gt_labels.shape)} vs pred={tuple(predicted_labels.shape)}"
                )
                step_entries.append(step_entry)
                continue

            eval_step = self._evaluate_step_against_gt(
                predicted_labels=predicted_labels,
                gt_labels=gt_labels,
                instance_iou_threshold=instance_iou_threshold,
            )
            step_entry["evaluated"] = True
            step_entry["segmentation"] = eval_step["segmentation"]
            step_entry["instance"] = eval_step["instance"]
            step_entry["mapping"] = eval_step["mapping"]
            step_maps[time_index] = eval_step["mapping"]
            step_entries.append(step_entry)

        reference_map = step_maps.get(0, {}).get("pred_to_gt", {})
        if not reference_map:
            report["warnings"].append(
                "step 0 has no usable pred->GT mapping; tracking identity accuracy will be skipped"
            )

        for time_index in range(1, steps_to_eval):
            step_entry = step_entries[time_index]
            if not step_entry.get("evaluated", False):
                continue
            step_map = step_maps.get(time_index)
            if not isinstance(step_map, dict):
                continue

            id_map = self.time_series.tracking.id_mapping.get(time_index, {})
            tracking_eval = self._evaluate_tracking_step(
                time_index=time_index,
                id_map=id_map if isinstance(id_map, dict) else {},
                reference_pred_to_gt=dict(reference_map) if isinstance(reference_map, dict) else {},
                current_pred_to_gt=dict(step_map.get("pred_to_gt", {})),
                current_gt_ids=set(step_map.get("gt_ids", set())),
            )
            step_entry["tracking"] = tracking_eval

        evaluated_steps = [row for row in step_entries if row.get("evaluated", False)]
        evaluated_detection = [row for row in evaluated_steps if isinstance(row.get("instance"), dict) and row["instance"]]
        evaluated_tracking = [
            row for row in step_entries[1:]
            if isinstance(row.get("tracking"), dict) and bool(row["tracking"].get("available", False))
        ]

        overall: Dict[str, Any] = {
            "num_steps_total": len(step_entries),
            "num_steps_evaluated": len(evaluated_steps),
            "num_steps_with_tracking_eval": len(evaluated_tracking),
        }

        if evaluated_detection:
            mean_instance_precision = float(np.mean([row["instance"].get("precision", 0.0) for row in evaluated_detection]))
            mean_instance_recall = float(np.mean([row["instance"].get("recall", 0.0) for row in evaluated_detection]))
            mean_instance_f1 = float(np.mean([row["instance"].get("f1", 0.0) for row in evaluated_detection]))
            mean_voxel_iou = float(np.mean([row["segmentation"].get("voxel_iou", 0.0) for row in evaluated_detection]))
            overall.update(
                {
                    "mean_instance_precision": mean_instance_precision,
                    "mean_instance_recall": mean_instance_recall,
                    "mean_instance_f1": mean_instance_f1,
                    "mean_voxel_iou": mean_voxel_iou,
                }
            )

        if evaluated_tracking:
            mean_tracking_accuracy = float(np.mean([row["tracking"].get("accuracy", 0.0) for row in evaluated_tracking]))
            mean_active_accuracy = float(np.mean([row["tracking"].get("active_accuracy", 0.0) for row in evaluated_tracking]))
            mean_closure_accuracy = float(np.mean([row["tracking"].get("closure_accuracy", 0.0) for row in evaluated_tracking]))
            overall.update(
                {
                    "mean_tracking_accuracy": mean_tracking_accuracy,
                    "mean_active_tracking_accuracy": mean_active_accuracy,
                    "mean_closure_accuracy": mean_closure_accuracy,
                }
            )

        for step_entry in step_entries:
            for msg in step_entry.get("errors", []):
                report["errors"].append(f"t={step_entry['time_index']} {msg}")
            for msg in step_entry.get("warnings", []):
                report["warnings"].append(f"t={step_entry['time_index']} {msg}")

        report["steps"] = step_entries
        report["overall"] = overall
        report["available"] = bool(evaluated_steps)
        if report["errors"]:
            report["status"] = "partial" if report["available"] else "unavailable"
        else:
            report["status"] = "ok" if report["available"] else "unavailable"

        self.time_series.tracking.evaluation = report
        return report

    def get_results(self) -> TimeSeriesPNM:
        return self.time_series

    def get_volume_history(self, pore_id: int) -> List[float]:
        return self.time_series.tracking.get_volume_series(pore_id)

    def get_pore_status(self, pore_id: int, timepoint: int) -> PoreStatus:
        statuses = self.time_series.tracking.status_history.get(pore_id, [])
        if timepoint < len(statuses):
            return statuses[timepoint]
        return PoreStatus.UNKNOWN

    def export_volume_csv(self, filepath: str) -> None:
        import csv

        tracking = self.time_series.tracking
        num_timepoints = self.time_series.num_timepoints

        with open(filepath, "w", newline="") as f:
            writer = csv.writer(f)
            header = ["pore_id"] + [f"t{i}" for i in range(num_timepoints)] + ["final_status", "volume_retention"]
            writer.writerow(header)

            for pore_id in tracking.reference_ids:
                volumes = tracking.volume_history.get(pore_id, [])
                statuses = tracking.status_history.get(pore_id, [])
                final_status = statuses[-1].value if statuses else "unknown"
                if volumes and volumes[0] > 0:
                    retention = volumes[-1] / volumes[0]
                else:
                    retention = 0.0
                row = [pore_id] + volumes + [final_status, f"{retention:.4f}"]
                writer.writerow(row)

        print(f"[Tracker] Exported volume history to {filepath}")


# ==========================================
# Batch IoU Calculation Functions
# ==========================================

def compute_batch_iou_cpu(
    reference_masks: List[np.ndarray],
    current_labels: np.ndarray,
    ref_ids: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute IoU scores for all reference pores in batch (CPU version).
    """
    num_pores = len(reference_masks)
    matched_ids = np.zeros(num_pores, dtype=np.int32)
    iou_scores = np.zeros(num_pores, dtype=np.float32)
    volumes = np.zeros(num_pores, dtype=np.float32)

    for i, mask_data in enumerate(reference_masks):
        bbox = mask_data["bbox"]
        local_mask = mask_data["mask"]
        mins, maxs = bbox
        current_region = current_labels[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]]

        overlapping = current_region[local_mask]
        overlapping = overlapping[overlapping > 0]
        if len(overlapping) == 0:
            continue

        counts = np.bincount(overlapping)
        if len(counts) > 1:
            best_label = int(np.argmax(counts[1:]) + 1)
        else:
            best_label = int(overlapping[0])

        current_mask = current_region == best_label
        intersection = np.sum(local_mask & current_mask)
        union = np.sum(local_mask | current_mask)

        if union > 0:
            matched_ids[i] = best_label
            iou_scores[i] = intersection / union
            volumes[i] = np.sum(current_mask)

    return matched_ids, iou_scores, volumes


def compute_batch_iou_gpu(
    reference_masks: List[Dict],
    current_labels: np.ndarray,
    ref_ids: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute IoU scores for all reference pores in batch (GPU version).
    """
    if not HAS_GPU or cp is None:
        raise RuntimeError("GPU acceleration requested but CuPy not available")

    current_labels_gpu = cp.asarray(current_labels)
    num_pores = len(reference_masks)
    matched_ids = cp.zeros(num_pores, dtype=cp.int32)
    iou_scores = cp.zeros(num_pores, dtype=cp.float32)
    volumes = cp.zeros(num_pores, dtype=cp.float32)

    for i, mask_data in enumerate(reference_masks):
        bbox = mask_data["bbox"]
        local_mask_cpu = mask_data["mask"]
        mins, maxs = bbox

        current_region = current_labels_gpu[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]]
        local_mask_gpu = cp.asarray(local_mask_cpu)
        overlapping = current_region[local_mask_gpu]
        overlapping = overlapping[overlapping > 0]
        if len(overlapping) == 0:
            continue

        counts = cp.bincount(overlapping)
        if len(counts) > 1:
            best_label = int(cp.argmax(counts[1:]) + 1)
        else:
            best_label = int(overlapping[0])

        current_mask = current_region == best_label
        intersection = cp.sum(local_mask_gpu & current_mask)
        union = cp.sum(local_mask_gpu | current_mask)
        if union > 0:
            matched_ids[i] = best_label
            iou_scores[i] = intersection / union
            volumes[i] = cp.sum(current_mask)

    return cp.asnumpy(matched_ids), cp.asnumpy(iou_scores), cp.asnumpy(volumes)


def match_with_hungarian(
    iou_matrix: np.ndarray,
    iou_threshold: float = 0.1,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Perform global assignment from IoU matrix using scipy Hungarian solver.
    """
    if not HAS_SCIPY:
        raise RuntimeError("Hungarian algorithm requested but scipy not available")

    if iou_matrix.size == 0:
        return np.array([], dtype=np.int32), np.array([], dtype=np.int32)

    cost_matrix = 1.0 - iou_matrix
    cost_matrix[iou_matrix < iou_threshold] = INVALID_COST
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    valid = iou_matrix[row_ind, col_ind] >= iou_threshold
    return row_ind[valid].astype(np.int32), col_ind[valid].astype(np.int32)


--- FILE: .\processors\pore.py ---
"""
Pore extraction processor for segmenting void space from solid matrix.
Memory-optimized for large volumes using chunked processing.
"""

import numpy as np
import scipy.ndimage as ndimage
from skimage.filters import threshold_otsu
from typing import Optional, Callable, Tuple
import gc

from core import BaseProcessor, VolumeData
from processors.utils import (
    binary_fill_holes, 
    compute_histogram_gpu, 
    compute_statistics_gpu,
    threshold_otsu_gpu
)
from config import (
    PROCESS_CHUNK_THRESHOLD,
    PROCESS_CHUNK_SIZE,
    PROCESS_OTSU_SAMPLE_THRESHOLD
)

# Try to import cc3d for faster 3D labeling (10-50x speedup)
try:
    import cc3d
    HAS_CC3D = True
    print("[PoreProcessor] Using cc3d for fast 3D labeling")
except ImportError:
    HAS_CC3D = False
    print("[PoreProcessor] cc3d not found, using scipy.ndimage.label (slower)")

# Alias for backward compatibility
CHUNK_THRESHOLD = PROCESS_CHUNK_THRESHOLD


def fast_label(binary_mask: np.ndarray, connectivity: int = 26) -> Tuple[np.ndarray, int]:
    """
    Fast 3D connected component labeling.
    Uses cc3d if available (10-50x faster), otherwise falls back to scipy.
    
    Args:
        binary_mask: Binary 3D array (True = foreground)
        connectivity: 6, 18, or 26 for 3D (only used by cc3d)
        
    Returns:
        (labeled_array, num_features)
    """
    if HAS_CC3D:
        # cc3d is MUCH faster for large 3D arrays
        labeled = cc3d.connected_components(binary_mask, connectivity=connectivity)
        num_features = int(labeled.max())
        return labeled, num_features
    else:
        # Fallback to scipy (slower but always available)
        structure = np.ones((3, 3, 3)) if connectivity == 26 else None
        return ndimage.label(binary_mask, structure=structure)


class PoreExtractionProcessor(BaseProcessor):
    """
    Segments the void space (pores) from the solid matrix.
    Returns a Voxel-based VolumeData object representing "Air".
    
    Memory-optimized: uses chunked processing for large volumes.
    """

    def __init__(self, chunk_size: int = PROCESS_CHUNK_SIZE):
        """
        Args:
            chunk_size: Number of slices to process at a time for large volumes.
        """
        self.chunk_size = chunk_size

    @staticmethod
    def suggest_threshold(data: VolumeData, algorithm: str = 'auto') -> int:
        """
        Calculate optimal threshold using specified algorithm.
        
        Args:
            data: Volume data to analyze
            algorithm: One of 'auto', 'otsu', 'li', 'yen', 'triangle', 'minimum'
                - auto: Smart selection based on histogram bimodality
                - otsu: Classic bimodal thresholding
                - li: Minimum cross-entropy (good for noisy data)
                - yen: Maximum correlation
                - triangle: Good for CT 'peak+tail' histograms
                - minimum: Valley between peaks
                
        Returns:
            Suggested threshold value (int)
        """
        # Import skimage algorithms for non-Otsu methods (CPU, but operates on tiny histogram data)
        from skimage.filters import (
            threshold_li, threshold_yen, threshold_triangle, threshold_minimum
        )
        
        if data.raw_data is None:
            return -300
        
        raw = data.raw_data
        
        # Sample data for large volumes
        if raw.size > PROCESS_OTSU_SAMPLE_THRESHOLD:
            sample = raw[::4, ::4, ::4].flatten()
        else:
            sample = raw.flatten()
        
        clean_data = sample[np.isfinite(sample)]
        if clean_data.size == 0:
            return -300
        
        try:
            # Use unified GPU threshold computation (single GPU pass)
            from processors.threshold_gpu import compute_threshold_stats_gpu
            
            result = compute_threshold_stats_gpu(clean_data, nbins=256)
            
            hist, bin_edges = result['histogram']
            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
            
            # Normalize histogram
            hist_norm = hist.astype(float) / hist.sum()
            
            # Get pre-computed statistics
            stats = result['stats']
            n = stats['n']
            skewness = stats['skewness']
            kurtosis = stats['kurtosis']
            
            # Calculate bimodality coefficient
            # Values > 0.555 indicate bimodal distribution
            if kurtosis + 3 * (n - 1) ** 2 / ((n - 2) * (n - 3)) > 0:
                bimodality = (skewness ** 2 + 1) / (kurtosis + 3 * (n - 1) ** 2 / ((n - 2) * (n - 3)))
            else:
                bimodality = 0
            
            # Pre-computed Otsu from unified function
            valid_thresholds = {'otsu': result['otsu_threshold']}
            
            # CPU-based methods (operate on histogram, very fast)
            cpu_methods = {
                'li': threshold_li,
                'yen': threshold_yen,
                'triangle': threshold_triangle,
                'minimum': threshold_minimum
            }
            for name, func in cpu_methods.items():
                try:
                    valid_thresholds[name] = func(clean_data)
                except Exception:
                    pass  # Skip failed methods
            
            if not valid_thresholds:
                return -300
            
            # If specific algorithm requested, use it directly
            if algorithm != 'auto' and algorithm in valid_thresholds:
                selected_thresh = valid_thresholds[algorithm]
                print(f"[Threshold] Using {algorithm} = {int(selected_thresh)}")
                del sample, clean_data
                gc.collect()
                return int(selected_thresh)
            elif algorithm != 'auto':
                # Requested algorithm failed, fall back to auto
                print(f"[Threshold] {algorithm} failed, falling back to auto")
            
            # AUTO MODE: Select best threshold based on histogram characteristics
            best_method = 'otsu'
            
            if bimodality > 0.6:
                # Strongly bimodal: prefer minimum or otsu
                if 'minimum' in valid_thresholds:
                    best_method = 'minimum'
                elif 'otsu' in valid_thresholds:
                    best_method = 'otsu'
            elif bimodality > 0.4:
                # Moderately bimodal: prefer Li (minimum cross-entropy)
                if 'li' in valid_thresholds:
                    best_method = 'li'
                elif 'otsu' in valid_thresholds:
                    best_method = 'otsu'
            else:
                # Weak bimodality or unimodal: prefer triangle for CT
                # Triangle works well for "peak + tail" distribution common in CT
                if 'triangle' in valid_thresholds:
                    best_method = 'triangle'
                elif 'yen' in valid_thresholds:
                    best_method = 'yen'
                elif 'li' in valid_thresholds:
                    best_method = 'li'
            
            # Validate threshold is reasonable (use cached stats)
            data_min, data_max = stats['min'], stats['max']
            data_range = data_max - data_min
            
            selected_thresh = valid_thresholds[best_method]
            
            # Ensure threshold is not at extreme edges (likely failure)
            if selected_thresh < data_min + 0.05 * data_range:
                # Too low, try other methods
                for method in ['li', 'yen', 'otsu']:
                    if method in valid_thresholds:
                        alt = valid_thresholds[method]
                        if alt > data_min + 0.05 * data_range:
                            selected_thresh = alt
                            best_method = method
                            break
            
            if selected_thresh > data_max - 0.05 * data_range:
                # Too high, try other methods
                for method in ['triangle', 'li', 'otsu']:
                    if method in valid_thresholds:
                        alt = valid_thresholds[method]
                        if alt < data_max - 0.05 * data_range:
                            selected_thresh = alt
                            best_method = method
                            break
            
            print(f"[AutoThreshold] Bimodality: {bimodality:.3f}, Selected: {best_method} = {int(selected_thresh)}")
            print(f"[AutoThreshold] All: {', '.join(f'{k}={int(v)}' for k,v in valid_thresholds.items())}")
            
            del sample, clean_data
            gc.collect()
            
            return int(selected_thresh)
            
        except Exception as e:
            print(f"Advanced threshold failed: {e}, falling back to median")
            # Ultimate fallback: use median
            median_val = np.median(clean_data)
            del sample, clean_data
            gc.collect()
            return int(median_val)

    def process(self, data: VolumeData, callback: Optional[Callable[[int, str], None]] = None,
                threshold: int = -300) -> VolumeData:
        """
        Extract pores from volume data.
        
        Uses shared cache: if PNM already computed pores_mask, reuse it.
        Also stores result in cache for PNM to reuse.
        """
        if data.raw_data is None:
            raise ValueError("Input data must contain raw voxel data.")

        from data.disk_cache import get_segmentation_cache
        
        def report(p: int, msg: str):
            print(f"[Processor] {msg}")
            if callback: callback(p, msg)

        cache = get_segmentation_cache()
        volume_id = f"{id(data.raw_data)}_{threshold}"
        
        # Check if we already have cached segmentation
        cached_mask = cache.get_pores_mask(volume_id)
        if cached_mask is not None:
            report(0, "Cache hit! Reusing segmentation from previous run...")
            pores_mask = cached_mask > 0  # Convert memmap to bool if needed
            cached_meta = cache.get_metadata(volume_id) or {}
            
            report(70, "Labeling connected components (cc3d)..." if HAS_CC3D else "Labeling connected components...")
            labeled_array, num_features = fast_label(pores_mask, connectivity=26)
            del labeled_array
            gc.collect()
            
            report(90, f"Found {num_features} pores. Generating output...")
            processed_volume = np.zeros(data.raw_data.shape, dtype=np.float32)
            processed_volume[pores_mask] = 1000
            
            porosity = cached_meta.get('porosity', 0)
            
            return VolumeData(
                raw_data=processed_volume,
                spacing=data.spacing,
                origin=data.origin,
                metadata={
                    "Type": "Processed - Void Volume",
                    "Porosity": f"{porosity:.2f}%",
                    "PoreCount": int(num_features),
                    "CacheHit": True
                }
            )

        report(0, f"Starting pore detection (Threshold < {threshold})...")

        raw = data.raw_data
        volume_bytes = raw.nbytes
        
        # Check if we need chunked processing
        if volume_bytes > CHUNK_THRESHOLD:
            return self._process_chunked(data, threshold, report, cache, volume_id)
        else:
            return self._process_standard(data, threshold, report, cache, volume_id)

    def _process_standard(self, data: VolumeData, threshold: int, report, 
                          cache=None, volume_id=None) -> VolumeData:
        """Standard processing for smaller volumes."""
        raw = data.raw_data
        
        # 1. Binarization (Air vs Solid)
        solid_mask = raw > threshold
        report(20, "Binarization complete. Filling holes...")

        # 2. Morphology
        filled_volume = binary_fill_holes(solid_mask)
        pores_mask = filled_volume ^ solid_mask
        
        # Free intermediate arrays
        del solid_mask, filled_volume
        gc.collect()
        
        report(50, "Morphology operations complete. Calculating stats...")

        # Quantitative Analysis
        pore_voxels = np.sum(pores_mask)
        total_voxels = raw.size
        porosity_pct = (pore_voxels / total_voxels) * 100.0

        # Label connected components (using cc3d if available for 10-50x speedup)
        report(70, "Labeling connected components (cc3d)..." if HAS_CC3D else "Labeling connected components...")
        labeled_array, num_features = fast_label(pores_mask, connectivity=26)
        del labeled_array
        gc.collect()

        report(90, f"Found {num_features} pores. Generating output volume...")

        # Store in shared cache for PNM to reuse
        if cache is not None and volume_id is not None:
            cache.store_pores_mask(volume_id, pores_mask, metadata={'porosity': porosity_pct})

        # Create Output Volume
        processed_volume = np.zeros_like(raw)
        processed_volume[pores_mask] = 1000
        del pores_mask
        gc.collect()

        report(100, "Processing complete.")

        return VolumeData(
            raw_data=processed_volume,
            spacing=data.spacing,
            origin=data.origin,
            metadata={
                "Type": "Processed - Void Volume",
                "Porosity": f"{porosity_pct:.2f}%",
                "PoreCount": int(num_features)
            }
        )

    def _process_chunked(self, data: VolumeData, threshold: int, report,
                          cache=None, volume_id=None) -> VolumeData:
        """Chunked processing for large volumes to avoid OOM."""
        raw = data.raw_data
        n_slices = raw.shape[0]
        chunk_size = self.chunk_size
        n_chunks = (n_slices + chunk_size - 1) // chunk_size
        
        report(5, f"Large volume detected. Processing in {n_chunks} chunks...")
        
        # Pre-allocate output with same shape (will overwrite)
        processed_volume = np.zeros(raw.shape, dtype=np.float32)
        
        total_pore_voxels = 0
        total_voxels = raw.size
        
        for i in range(n_chunks):
            start = i * chunk_size
            end = min(start + chunk_size, n_slices)
            
            # Process chunk
            chunk = raw[start:end]
            
            # Binarization
            solid_mask = chunk > threshold
            
            # Fill holes (2D per slice for memory efficiency)
            pores_mask = np.zeros_like(solid_mask)
            for j in range(solid_mask.shape[0]):
                filled = binary_fill_holes(solid_mask[j])
                pores_mask[j] = filled ^ solid_mask[j]
            
            # Count pores
            total_pore_voxels += np.sum(pores_mask)
            
            # Write to output
            processed_volume[start:end][pores_mask] = 1000
            
            # Free memory
            del chunk, solid_mask, pores_mask
            gc.collect()
            
            progress = 10 + int(80 * (i + 1) / n_chunks)
            report(progress, f"Processed chunk {i+1}/{n_chunks}...")
        
        porosity_pct = (total_pore_voxels / total_voxels) * 100.0
        
        report(95, "Estimating pore count...")
        # Simplified pore counting for large volumes (sample middle chunk)
        mid_start = (n_slices // 2) - chunk_size // 2
        mid_end = mid_start + chunk_size
        mid_chunk = processed_volume[mid_start:mid_end] > 0
        _, estimated_features = fast_label(mid_chunk, connectivity=26)
        # Extrapolate
        estimated_total = int(estimated_features * n_chunks / 2)
        del mid_chunk
        gc.collect()

        report(100, "Chunked processing complete.")

        return VolumeData(
            raw_data=processed_volume,
            spacing=data.spacing,
            origin=data.origin,
            metadata={
                "Type": "Processed - Void Volume (Chunked)",
                "Porosity": f"{porosity_pct:.2f}%",
                "PoreCount": f"~{estimated_total} (estimated)"
            }
        )



--- FILE: .\processors\threshold_gpu.py ---
"""Unified GPU threshold and statistics computation."""

import numpy as np
import time
from config import GPU_ENABLED, GPU_MIN_SIZE_MB
from core.gpu_backend import get_gpu_backend, CUPY_AVAILABLE


def compute_threshold_stats_gpu(data: np.ndarray, nbins: int = 256) -> dict:
    """Unified GPU: histogram, statistics, and Otsu threshold in single pass."""
    if not GPU_ENABLED or not CUPY_AVAILABLE:
        return _compute_threshold_stats_cpu(data, nbins)
    
    backend = get_gpu_backend()
    if not backend.available or data.nbytes / (1024 * 1024) < GPU_MIN_SIZE_MB:
        return _compute_threshold_stats_cpu(data, nbins)
    
    try:
        import cupy as cp
        start = time.time()
        
        data_gpu = cp.asarray(data)
        hist_gpu, bin_edges_gpu = cp.histogram(data_gpu, bins=nbins)
        bin_centers = (bin_edges_gpu[:-1] + bin_edges_gpu[1:]) / 2
        
        # Statistics
        n = int(data_gpu.size)
        mean, std = float(cp.mean(data_gpu).item()), float(cp.std(data_gpu).item())
        data_min, data_max = float(cp.min(data_gpu).item()), float(cp.max(data_gpu).item())
        
        skewness, kurtosis = 0.0, 0.0
        if std > 0:
            z = (data_gpu - mean) / std
            skewness, kurtosis = float(cp.mean(z**3).item()), float(cp.mean(z**4).item()) - 3
        del data_gpu
        
        # Otsu threshold
        hist_norm = hist_gpu.astype(cp.float64) / hist_gpu.sum()
        w1, w2 = cp.cumsum(hist_norm), 1.0 - cp.cumsum(hist_norm)
        m1_cs = cp.cumsum(hist_norm * bin_centers)
        m1 = m1_cs / (w1 + 1e-10)
        m2 = (float((hist_norm * bin_centers).sum().item()) - m1_cs) / (w2 + 1e-10)
        otsu = float(bin_centers[cp.argmax(w1 * w2 * (m1 - m2)**2)].item())
        
        hist, bin_edges = cp.asnumpy(hist_gpu), cp.asnumpy(bin_edges_gpu)
        del hist_gpu, bin_edges_gpu, bin_centers, hist_norm, w1, w2, m1_cs, m1, m2
        backend.clear_memory(force=False)
        
        print(f"[GPU] threshold_stats: {time.time() - start:.3f}s")
        return {
            'histogram': (hist, bin_edges),
            'stats': {'mean': mean, 'std': std, 'skewness': skewness, 'kurtosis': kurtosis,
                      'min': data_min, 'max': data_max, 'n': n},
            'otsu_threshold': otsu
        }
    except Exception as e:
        print(f"[GPU] threshold_stats failed: {e}")
        backend.clear_memory(force=False)
        return _compute_threshold_stats_cpu(data, nbins)


def _compute_threshold_stats_cpu(data: np.ndarray, nbins: int = 256) -> dict:
    """CPU fallback for threshold/stats."""
    from skimage.filters import threshold_otsu
    start = time.time()
    
    hist, bin_edges = np.histogram(data, bins=nbins)
    mean, std = float(np.mean(data)), float(np.std(data))
    data_min, data_max = float(np.min(data)), float(np.max(data))
    
    skewness, kurtosis = 0.0, 0.0
    if std > 0:
        z = (data - mean) / std
        skewness, kurtosis = float(np.mean(z**3)), float(np.mean(z**4)) - 3
    
    print(f"[CPU] threshold_stats: {time.time() - start:.3f}s")
    return {
        'histogram': (hist, bin_edges),
        'stats': {'mean': mean, 'std': std, 'skewness': skewness, 'kurtosis': kurtosis,
                  'min': data_min, 'max': data_max, 'n': len(data)},
        'otsu_threshold': threshold_otsu(data)
    }


--- FILE: .\processors\tracking_utils.py ---
"""
Utility helpers for robust 4D CT TGGA tracking.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np
from scipy.ndimage import gaussian_filter, map_coordinates, shift as nd_shift
from core.gpu_backend import CUPY_AVAILABLE, get_gpu_backend

if CUPY_AVAILABLE:
    import cupy as cp
    import cupyx.scipy.ndimage as gpu_ndimage
else:
    cp = None
    gpu_ndimage = None

try:
    from skimage.registration import phase_cross_correlation

    HAS_SKIMAGE_REGISTRATION = True
except Exception:
    phase_cross_correlation = None
    HAS_SKIMAGE_REGISTRATION = False


EPS = 1e-8


def bounded_volume_penalty(
    reference_volume: float,
    current_volume: float,
    mode: str = "symdiff",
    gaussian_sigma: float = 1.0,
) -> float:
    """
    Bounded volume mismatch penalty in [0, 1].

    Modes:
      - symdiff: |Vref - Vcur| / (Vref + Vcur)
      - gaussian: 1 - exp(-(Vref - Vcur)^2 / (2*sigma^2))
    """
    v_ref = max(float(reference_volume), 0.0)
    v_cur = max(float(current_volume), 0.0)
    resolved_mode = (mode or "symdiff").lower()

    if resolved_mode in {"gaussian", "inverse_gaussian", "anti_gaussian"}:
        sigma = max(float(gaussian_sigma), EPS)
        delta = v_ref - v_cur
        penalty = 1.0 - np.exp(-(delta * delta) / (2.0 * sigma * sigma))
        return float(np.clip(penalty, 0.0, 1.0))

    denom = max(v_ref + v_cur, EPS)
    penalty = abs(v_ref - v_cur) / denom
    return float(np.clip(penalty, 0.0, 1.0))


@dataclass
class MacroRegistrationResult:
    """Macro pre-registration output for gating and closure analysis."""

    displacement: np.ndarray
    compression_field: Optional[np.ndarray] = None
    method: str = "none"
    confidence: float = 0.0

    def warp_point(self, point_zyx: np.ndarray) -> np.ndarray:
        return np.asarray(point_zyx, dtype=np.float64) + np.asarray(self.displacement, dtype=np.float64)

    def sample_compression(self, point_zyx: np.ndarray, default: float = 0.0) -> float:
        if self.compression_field is None or self.compression_field.size == 0:
            return float(default)

        p = np.asarray(point_zyx, dtype=np.float64)
        if p.size != 3:
            return float(default)

        shape = np.asarray(self.compression_field.shape, dtype=np.float64) - 1.0
        clipped = np.clip(p, 0.0, np.maximum(shape, 0.0))
        value = map_coordinates(
            self.compression_field,
            [[clipped[0]], [clipped[1]], [clipped[2]]],
            order=1,
            mode="nearest",
        )
        return float(value[0]) if value.size else float(default)


def _phase_cross_correlation_gpu(ref_density_gpu, cur_density_gpu) -> Tuple[np.ndarray, float]:
    """
    Integer-shift phase correlation on GPU.

    Returns shift that maps current -> reference and a confidence score in [0, 1].
    """
    if not CUPY_AVAILABLE or cp is None:
        raise RuntimeError("CuPy is unavailable")

    eps = np.float32(1e-12)
    ref_fft = cp.fft.fftn(ref_density_gpu)
    cur_fft = cp.fft.fftn(cur_density_gpu)
    cross_power = ref_fft * cp.conj(cur_fft)
    cross_power /= cp.maximum(cp.abs(cross_power), eps)

    corr = cp.fft.ifftn(cross_power)
    corr_abs = cp.abs(corr)
    peak_flat = int(cp.argmax(corr_abs).get())
    peak_idx = np.unravel_index(peak_flat, corr_abs.shape)

    shift = np.asarray(peak_idx, dtype=np.float64)
    shape = np.asarray(corr_abs.shape, dtype=np.float64)
    half = shape / 2.0
    shift = np.where(shift > half, shift - shape, shift)

    peak_val = float(cp.asnumpy(corr_abs.ravel()[peak_flat]))
    mean_val = float(cp.asnumpy(cp.mean(corr_abs)))
    peak_ratio = peak_val / max(mean_val, EPS)
    confidence = float(np.clip((peak_ratio - 1.0) / 10.0, 0.0, 1.0))
    return shift, confidence


def estimate_macro_registration(
    reference_regions: Optional[np.ndarray],
    current_regions: Optional[np.ndarray],
    smoothing_sigma: float = 1.5,
    upsample_factor: int = 4,
    use_gpu: bool = False,
    gpu_min_size_mb: float = 8.0,
) -> MacroRegistrationResult:
    """
    Estimate macro displacement from previous frame to current frame.

    This is a DVC-style global registration using phase cross-correlation,
    with a center-of-mass fallback.
    """
    zero = np.zeros(3, dtype=np.float64)
    if reference_regions is None or current_regions is None:
        return MacroRegistrationResult(displacement=zero, method="missing_regions", confidence=0.0)
    if reference_regions.shape != current_regions.shape:
        return MacroRegistrationResult(displacement=zero, method="shape_mismatch", confidence=0.0)

    ref_binary = (reference_regions > 0).astype(np.float32)
    cur_binary = (current_regions > 0).astype(np.float32)

    com_displacement: Optional[np.ndarray] = None
    ref_coords = np.argwhere(ref_binary > 0)
    cur_coords = np.argwhere(cur_binary > 0)
    if len(ref_coords) > 0 and len(cur_coords) > 0:
        com_displacement = np.asarray(cur_coords.mean(axis=0) - ref_coords.mean(axis=0), dtype=np.float64)

    sigma = max(float(smoothing_sigma), 0.0)
    sanity_limit = (
        max(5.0, 0.25 * np.linalg.norm(com_displacement) + 2.0)
        if com_displacement is not None
        else 5.0
    )

    if use_gpu and CUPY_AVAILABLE and cp is not None and gpu_ndimage is not None:
        backend = get_gpu_backend()
        total_mb = (ref_binary.nbytes + cur_binary.nbytes) / (1024.0 * 1024.0)
        estimated_bytes = int((ref_binary.nbytes + cur_binary.nbytes) * 10)
        if backend.available and total_mb >= float(gpu_min_size_mb) and backend.can_fit(estimated_bytes, safety_factor=0.7):
            try:
                ref_binary_gpu = cp.asarray(ref_binary)
                cur_binary_gpu = cp.asarray(cur_binary)
                if sigma > 0.0:
                    ref_density_gpu = gpu_ndimage.gaussian_filter(ref_binary_gpu, sigma=sigma)
                    cur_density_gpu = gpu_ndimage.gaussian_filter(cur_binary_gpu, sigma=sigma)
                else:
                    ref_density_gpu = ref_binary_gpu
                    cur_density_gpu = cur_binary_gpu

                shift_to_reference_gpu, gpu_confidence = _phase_cross_correlation_gpu(
                    ref_density_gpu=ref_density_gpu,
                    cur_density_gpu=cur_density_gpu,
                )
                displacement = -np.asarray(shift_to_reference_gpu, dtype=np.float64)
                method = "phase_cross_correlation_gpu"
                confidence = float(gpu_confidence)

                if com_displacement is not None and np.linalg.norm(displacement - com_displacement) > sanity_limit:
                    displacement = com_displacement
                    method = "center_of_mass_fallback"
                    confidence = max(confidence, 0.35)

                warped_reference_density_gpu = gpu_ndimage.shift(
                    ref_density_gpu,
                    shift=tuple(float(v) for v in displacement),
                    order=1,
                    mode="nearest",
                    prefilter=False,
                )
                compression_field_gpu = cp.clip(
                    (warped_reference_density_gpu - cur_density_gpu)
                    / cp.maximum(warped_reference_density_gpu, np.float32(EPS)),
                    0.0,
                    1.0,
                ).astype(cp.float32)
                compression_field = cp.asnumpy(compression_field_gpu)

                del ref_binary_gpu, cur_binary_gpu, ref_density_gpu, cur_density_gpu
                del warped_reference_density_gpu, compression_field_gpu
                backend.clear_memory(force=False)
                return MacroRegistrationResult(
                    displacement=np.asarray(displacement, dtype=np.float64),
                    compression_field=compression_field,
                    method=method,
                    confidence=confidence,
                )
            except Exception:
                backend.clear_memory(force=False)

    ref_density = gaussian_filter(ref_binary, sigma=sigma)
    cur_density = gaussian_filter(cur_binary, sigma=sigma)

    displacement = zero.copy()
    method = "zero"
    confidence = 0.0

    if HAS_SKIMAGE_REGISTRATION and np.any(ref_density) and np.any(cur_density):
        try:
            shift_to_reference, error, _ = phase_cross_correlation(
                ref_density,
                cur_density,
                upsample_factor=max(int(upsample_factor), 1),
            )
            # phase_cross_correlation returns current->reference shift.
            displacement = -np.asarray(shift_to_reference, dtype=np.float64)
            method = "phase_cross_correlation"
            confidence = float(np.clip(1.0 - float(error), 0.0, 1.0))
        except Exception:
            displacement = zero.copy()

    if method == "phase_cross_correlation" and com_displacement is not None:
        if np.linalg.norm(displacement - com_displacement) > sanity_limit:
            displacement = com_displacement
            method = "center_of_mass_fallback"
            confidence = max(confidence, 0.35)

    if method == "zero":
        if com_displacement is not None:
            displacement = com_displacement
            method = "center_of_mass"
            confidence = 0.35
        elif len(ref_coords) > 0 and len(cur_coords) == 0:
            method = "empty_current"
            confidence = 0.0
        else:
            method = "empty_reference"
            confidence = 0.0

    warped_reference_density = nd_shift(
        ref_density,
        shift=displacement,
        order=1,
        mode="nearest",
        prefilter=False,
    )
    compression_field = np.clip(
        (warped_reference_density - cur_density) / np.maximum(warped_reference_density, EPS),
        0.0,
        1.0,
    ).astype(np.float32)

    return MacroRegistrationResult(
        displacement=np.asarray(displacement, dtype=np.float64),
        compression_field=compression_field,
        method=method,
        confidence=confidence,
    )


def extract_shifted_overlap_region(
    current_regions: np.ndarray,
    local_reference_mask: np.ndarray,
    bbox_mins: np.ndarray,
    reference_center: np.ndarray,
    expected_center: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Extract aligned local region by shifting reference bbox toward expected center.

    Returns:
      (shifted_ref_mask, shifted_current_region) with identical shape.
    """
    shape = np.asarray(local_reference_mask.shape, dtype=np.int64)
    if shape.size != 3 or np.any(shape <= 0):
        return np.zeros((0, 0, 0), dtype=bool), np.zeros((0, 0, 0), dtype=current_regions.dtype)

    mins = np.asarray(bbox_mins, dtype=np.int64)
    ref_c = np.asarray(reference_center, dtype=np.float64)
    exp_c = np.asarray(expected_center, dtype=np.float64)
    shift_zyx = np.rint(exp_c - ref_c).astype(np.int64)

    target_mins = mins + shift_zyx
    target_maxs = target_mins + shape

    image_shape = np.asarray(current_regions.shape, dtype=np.int64)
    clipped_mins = np.maximum(target_mins, 0)
    clipped_maxs = np.minimum(target_maxs, image_shape)
    clipped_shape = clipped_maxs - clipped_mins
    if np.any(clipped_shape <= 0):
        return np.zeros((0, 0, 0), dtype=bool), np.zeros((0, 0, 0), dtype=current_regions.dtype)

    mask_offset_min = clipped_mins - target_mins
    mask_offset_max = mask_offset_min + clipped_shape

    ref_local = local_reference_mask[
        mask_offset_min[0]:mask_offset_max[0],
        mask_offset_min[1]:mask_offset_max[1],
        mask_offset_min[2]:mask_offset_max[2],
    ]
    curr_local = current_regions[
        clipped_mins[0]:clipped_maxs[0],
        clipped_mins[1]:clipped_maxs[1],
        clipped_mins[2]:clipped_maxs[2],
    ]
    return ref_local, curr_local


def should_mark_closed_by_compression(
    previous_volume: float,
    reference_volume: float,
    local_compression: float,
    volume_ratio_threshold: float = 0.02,
    min_volume_voxels: float = 2.0,
    compression_threshold: float = 0.6,
) -> bool:
    """Rule-based closure decision for a physically disappearing pore."""
    ref_volume_safe = max(float(reference_volume), EPS)
    prev_volume_safe = max(float(previous_volume), 0.0)
    ratio = prev_volume_safe / ref_volume_safe

    tiny_last_frame = (
        prev_volume_safe <= float(min_volume_voxels)
        or ratio <= max(float(volume_ratio_threshold), 0.0)
    )
    high_compression_zone = float(local_compression) >= float(compression_threshold)
    return bool(tiny_last_frame and high_compression_zone)


class ConstantAccelerationKalman3D:
    """Constant-acceleration Kalman filter with state [p, v, a] in 3D."""

    def __init__(
        self,
        initial_position: np.ndarray,
        dt: float = 1.0,
        process_noise: float = 0.05,
        measurement_noise: float = 1.0,
    ) -> None:
        self.dt = float(max(dt, 1e-6))
        self.process_noise = float(max(process_noise, 1e-9))
        self.measurement_noise = float(max(measurement_noise, 1e-9))

        self.state = np.zeros((9, 1), dtype=np.float64)
        self.state[0:3, 0] = np.asarray(initial_position, dtype=np.float64)

        self.covariance = np.eye(9, dtype=np.float64)
        self.covariance[3:6, 3:6] *= 5.0
        self.covariance[6:9, 6:9] *= 10.0

        self.measurement_matrix = np.zeros((3, 9), dtype=np.float64)
        self.measurement_matrix[0, 0] = 1.0
        self.measurement_matrix[1, 1] = 1.0
        self.measurement_matrix[2, 2] = 1.0
        self.measurement_covariance = np.eye(3, dtype=np.float64) * self.measurement_noise

        self.transition_matrix = self._build_transition(self.dt)
        self.process_covariance = self._build_process_covariance(self.dt, self.process_noise)

    def _build_transition(self, dt: float) -> np.ndarray:
        f = np.eye(9, dtype=np.float64)
        for axis in range(3):
            pos = axis
            vel = axis + 3
            acc = axis + 6
            f[pos, vel] = dt
            f[pos, acc] = 0.5 * dt * dt
            f[vel, acc] = dt
        return f

    def _build_process_covariance(self, dt: float, noise: float) -> np.ndarray:
        q = np.zeros((9, 9), dtype=np.float64)
        dt2 = dt * dt
        dt3 = dt2 * dt
        dt4 = dt3 * dt
        dt5 = dt4 * dt
        base = np.array(
            [
                [dt5 / 20.0, dt4 / 8.0, dt3 / 6.0],
                [dt4 / 8.0, dt3 / 3.0, dt2 / 2.0],
                [dt3 / 6.0, dt2 / 2.0, dt],
            ],
            dtype=np.float64,
        ) * noise

        for axis in range(3):
            ids = [axis, axis + 3, axis + 6]
            for r in range(3):
                for c in range(3):
                    q[ids[r], ids[c]] = base[r, c]
        return q

    def _refresh_dynamics(self, dt: float) -> None:
        self.dt = float(max(dt, 1e-6))
        self.transition_matrix = self._build_transition(self.dt)
        self.process_covariance = self._build_process_covariance(self.dt, self.process_noise)

    def predict(self, dt: Optional[float] = None) -> np.ndarray:
        if dt is not None:
            self._refresh_dynamics(float(dt))

        self.state = self.transition_matrix @ self.state
        self.covariance = (
            self.transition_matrix @ self.covariance @ self.transition_matrix.T
            + self.process_covariance
        )
        return self.current_position()

    def update(self, measurement: np.ndarray) -> np.ndarray:
        z = np.asarray(measurement, dtype=np.float64).reshape(3, 1)
        innovation = z - self.measurement_matrix @ self.state
        innovation_cov = (
            self.measurement_matrix @ self.covariance @ self.measurement_matrix.T
            + self.measurement_covariance
        )
        kalman_gain = self.covariance @ self.measurement_matrix.T @ np.linalg.inv(innovation_cov)
        self.state = self.state + kalman_gain @ innovation

        identity = np.eye(9, dtype=np.float64)
        residual = identity - kalman_gain @ self.measurement_matrix
        # Joseph form for numerical stability.
        self.covariance = (
            residual @ self.covariance @ residual.T
            + kalman_gain @ self.measurement_covariance @ kalman_gain.T
        )
        return self.current_position()

    def current_position(self) -> np.ndarray:
        return self.state[0:3, 0].copy()

    def apply_brake(
        self,
        miss_count: int,
        velocity_decay: float = 0.75,
        acceleration_decay: float = 0.35,
        freeze_after_misses: int = 3,
    ) -> np.ndarray:
        """
        Brake motion when detections are missing to prevent runaway drift.

        The filter is still allowed to coast for short occlusions, but velocity
        and acceleration are exponentially damped and fully frozen after a
        configurable number of consecutive misses.
        """
        misses = max(int(miss_count), 1)
        v_decay = float(np.clip(velocity_decay, 0.0, 1.0))
        a_decay = float(np.clip(acceleration_decay, 0.0, 1.0))
        freeze_after = max(int(freeze_after_misses), 1)

        if misses >= freeze_after:
            self.state[3:6, 0] = 0.0
            self.state[6:9, 0] = 0.0
        else:
            self.state[3:6, 0] *= v_decay
            self.state[6:9, 0] *= a_decay
        return self.current_position()

    def set_position(self, position: np.ndarray) -> None:
        self.state[0:3, 0] = np.asarray(position, dtype=np.float64)


--- FILE: .\processors\utils.py ---
"""GPU-accelerated utility functions with CPU fallback."""

import numpy as np
import scipy.ndimage as ndimage
from typing import Optional
import time

from config import GPU_ENABLED, GPU_MIN_SIZE_MB
from core.gpu_backend import get_gpu_backend, CUPY_AVAILABLE


def binary_fill_holes(binary_mask: np.ndarray) -> np.ndarray:
    """Fill holes in binary mask (GPU-accelerated)."""
    if not GPU_ENABLED:
        return ndimage.binary_fill_holes(binary_mask)
    
    backend = get_gpu_backend()
    size_mb = binary_mask.nbytes / (1024 * 1024)
    
    if backend.available and size_mb >= GPU_MIN_SIZE_MB and backend.get_free_memory_mb() > size_mb * 4:
        try:
            import cupyx.scipy.ndimage as gpu_ndimage
            start = time.time()
            mask_gpu = backend.to_gpu(binary_mask)
            result = backend.to_cpu(gpu_ndimage.binary_fill_holes(mask_gpu))
            del mask_gpu
            backend.clear_memory(force=False)
            print(f"[GPU] binary_fill_holes: {time.time() - start:.2f}s")
            return result
        except Exception as e:
            print(f"[GPU] binary_fill_holes failed: {e}")
            backend.clear_memory(force=False)
    
    start = time.time()
    result = ndimage.binary_fill_holes(binary_mask)
    print(f"[CPU] binary_fill_holes: {time.time() - start:.2f}s")
    return result


def distance_transform_edt(binary_mask: np.ndarray, 
                           sampling: Optional[tuple] = None) -> np.ndarray:
    """Euclidean Distance Transform (GPU-accelerated)."""
    if not GPU_ENABLED:
        return ndimage.distance_transform_edt(binary_mask, sampling=sampling).astype(np.float32)
    
    backend = get_gpu_backend()
    size_mb = binary_mask.nbytes / (1024 * 1024)
    required_mb = size_mb * 32
    free_mb = backend.get_free_memory_mb()
    
    if backend.available and size_mb >= GPU_MIN_SIZE_MB and required_mb < free_mb:
        try:
            import cupyx.scipy.ndimage as gpu_ndimage
            start = time.time()
            mask_gpu = backend.to_gpu(binary_mask.astype(np.uint8))
            result = backend.to_cpu(gpu_ndimage.distance_transform_edt(mask_gpu, sampling=sampling)).astype(np.float32)
            del mask_gpu
            backend.clear_memory(force=False)
            print(f"[GPU] EDT: {time.time() - start:.2f}s")
            return result
        except Exception as e:
            print(f"[GPU] EDT failed: {e}")
            backend.clear_memory(force=False)
    elif backend.available and size_mb >= GPU_MIN_SIZE_MB:
        print(f"[GPU] EDT skipped: need {required_mb:.0f}MB, have {free_mb:.0f}MB")

    start = time.time()
    result = ndimage.distance_transform_edt(binary_mask, sampling=sampling).astype(np.float32)
    print(f"[CPU] EDT: {time.time() - start:.2f}s")
    return result


def find_local_maxima(image: np.ndarray,
                      min_distance: int = 1,
                      labels: Optional[np.ndarray] = None) -> np.ndarray:
    """Find local maxima with GPU acceleration and NMS."""
    from skimage.feature import peak_local_max
    
    if not GPU_ENABLED:
        return peak_local_max(image, min_distance=min_distance, labels=labels)
    
    backend = get_gpu_backend()
    size_mb = image.nbytes / (1024 * 1024)
    required_mb = size_mb * 4
    free_mb = backend.get_free_memory_mb()
    
    if backend.available and size_mb >= GPU_MIN_SIZE_MB and required_mb < free_mb * 0.8 and CUPY_AVAILABLE:
        try:
            return _find_local_maxima_gpu_impl(image, min_distance, labels)
        except Exception as e:
            print(f"[GPU] find_local_maxima failed: {e}")
            backend.clear_memory()
    elif backend.available and size_mb >= GPU_MIN_SIZE_MB:
        print(f"[GPU] find_local_maxima skipped: need {required_mb:.0f}MB, have {free_mb:.0f}MB")
    
    start = time.time()
    result = peak_local_max(image, min_distance=min_distance, labels=labels)
    print(f"[CPU] find_local_maxima: {time.time() - start:.2f}s, {len(result)} peaks")
    return result


def _find_local_maxima_gpu_impl(image: np.ndarray,
                                 min_distance: int,
                                 labels: Optional[np.ndarray]) -> np.ndarray:
    """GPU local maxima detection with NMS."""
    import cupy as cp
    import cupyx.scipy.ndimage as gpu_ndimage
    
    backend = get_gpu_backend()
    start = time.time()
    
    image_gpu = cp.asarray(image.astype(np.float32))
    size = 2 * min_distance + 1
    max_filtered = gpu_ndimage.maximum_filter(image_gpu, size=size)
    is_peak = (image_gpu == max_filtered) & (image_gpu > 0)
    
    if labels is not None:
        is_peak &= cp.asarray(labels) > 0
    
    candidates = cp.argwhere(is_peak)
    if candidates.shape[0] == 0:
        del image_gpu, max_filtered, is_peak
        backend.clear_memory(force=False)
        return np.array([]).reshape(0, image.ndim)
    
    # Sort by value descending 鈥?single argsort, reused for both arrays
    values = image_gpu[is_peak]
    del max_filtered, is_peak
    sort_idx              = cp.argsort(-values)
    candidates_gpu_sorted = candidates[sort_idx]   # (N, 3) int64, descending intensity
    values_gpu_sorted     = values[sort_idx]       # (N,)   float32
    del candidates, values, image_gpu

    # GPU NMS via spatial hash grid  O(N)
    keep_mask  = _nms_gpu_parallel(candidates_gpu_sorted, values_gpu_sorted, min_distance)
    result_gpu = candidates_gpu_sorted[keep_mask]
    result     = backend.to_cpu(result_gpu)
    del candidates_gpu_sorted, values_gpu_sorted, sort_idx, keep_mask, result_gpu
    backend.clear_memory(force=False)
    print(f"[GPU] find_local_maxima: {time.time() - start:.2f}s, {len(result)} peaks")
    return result


# ---------------------------------------------------------------------------
# GPU Non-Maximum Suppression 鈥?O(N) via spatial hashing
# ---------------------------------------------------------------------------

def _nms_gpu_parallel(candidates_gpu, intensities_gpu, min_distance: int):
    """
    GPU-accelerated NMS with spatial hash grid.  O(N) vs. O(N虏) brute-force.

    Parameters
    ----------
    candidates_gpu  : cp.ndarray (N, 3) int32 / int64  鈥?voxel coordinates
    intensities_gpu : cp.ndarray (N,)  float32          鈥?intensity at each candidate
    min_distance    : int                               鈥?exclusion radius (voxels)

    Returns
    -------
    keep : cp.ndarray (N,) bool   鈥?True where the point IS kept
    """
    import cupy as cp

    n = int(candidates_gpu.shape[0])
    if n == 0:
        return cp.zeros(0, dtype=cp.bool_)

    cell_size   = max(float(min_distance), 1.0)
    pts_f32     = candidates_gpu.astype(cp.float32)           # (N,3)
    grid_coords = cp.floor(pts_f32 / cell_size).astype(cp.int32)  # (N,3)

    # Large coprime primes for spatial hashing
    P1 = np.int64(73_856_093)
    P2 = np.int64(19_349_663)
    P3 = np.int64(83_492_791)
    hash_table_size = max(n * 4, 1024)

    gc_i64      = grid_coords.astype(cp.int64)
    raw_hashes  = (
        gc_i64[:, 0] * int(P1) ^
        gc_i64[:, 1] * int(P2) ^
        gc_i64[:, 2] * int(P3)
    ) % hash_table_size
    raw_hashes  = raw_hashes.astype(cp.int32)

    # Sort by hash bucket to enable contiguous cell ranges
    sort_order      = cp.argsort(raw_hashes)
    sorted_hashes   = raw_hashes[sort_order]
    sorted_pts_f32  = pts_f32[sort_order]          # (N, 3) float32
    sorted_intens   = intensities_gpu[sort_order].astype(cp.float32)
    sorted_gc       = grid_coords[sort_order].astype(cp.int32)  # (N, 3)

    # Build cell_start / cell_end arrays
    cell_start = cp.full(hash_table_size, -1, dtype=cp.int32)
    cell_end   = cp.zeros(hash_table_size, dtype=cp.int32)
    is_boundary = cp.concatenate([
        cp.array([True]),
        sorted_hashes[1:] != sorted_hashes[:-1],
        cp.array([True]),
    ])
    starts        = cp.where(is_boundary[:-1])[0].astype(cp.int32)
    ends          = cp.where(is_boundary[1:])[0].astype(cp.int32)
    bucket_keys   = sorted_hashes[starts]
    cell_start[bucket_keys] = starts
    cell_end[bucket_keys]   = ends + 1

    # ------------------------------------------------------------------
    # RawKernel: each thread owns one candidate; checks 27 neighbor cells
    # ------------------------------------------------------------------
    nms_kernel_code = r'''
    extern "C" __global__
    void nms_spatial_hash_kernel(
        const float* __restrict__ points,      // (N, 3) float32, row-major
        const float* __restrict__ intensities, // (N,)   float32
        const int*   __restrict__ gc,          // (N, 3) int32  grid coords
        const int*   __restrict__ cell_start,  // (hash_table_size,)
        const int*   __restrict__ cell_end,    // (hash_table_size,)
        bool*                     keep_flag,   // (N,)   output
        int          N,
        int          hash_table_size,
        float        min_dist_sq,
        long long    P1,
        long long    P2,
        long long    P3
    ) {
        int i = blockDim.x * blockIdx.x + threadIdx.x;
        if (i >= N) return;

        float my_val = intensities[i];
        float p_z    = points[i * 3 + 0];
        float p_y    = points[i * 3 + 1];
        float p_x    = points[i * 3 + 2];
        int   g_z    = gc[i * 3 + 0];
        int   g_y    = gc[i * 3 + 1];
        int   g_x    = gc[i * 3 + 2];

        // Traverse 27 neighbouring grid cells (3x3x3 cube around owning cell)
        for (int dz = -1; dz <= 1; dz++) {
        for (int dy = -1; dy <= 1; dy++) {
        for (int dx = -1; dx <= 1; dx++) {
            int cz = g_z + dz;
            int cy = g_y + dy;
            int cx = g_x + dx;
            long long h = ((long long)cz * P1 ^ (long long)cy * P2 ^ (long long)cx * P3)
                          % (long long)hash_table_size;
            if (h < 0) h += (long long)hash_table_size;
            int bucket = (int)h;
            int cs = cell_start[bucket];
            if (cs < 0) continue;
            int ce = cell_end[bucket];
            for (int j = cs; j < ce; j++) {
                if (j == i) continue;
                // Hash-collision guard: verify exact grid-cell match
                if (gc[j*3+0] != cz || gc[j*3+1] != cy || gc[j*3+2] != cx) continue;
                float ddz  = points[j*3+0] - p_z;
                float ddy  = points[j*3+1] - p_y;
                float ddx  = points[j*3+2] - p_x;
                float dist2 = ddz*ddz + ddy*ddy + ddx*ddx;
                if (dist2 < min_dist_sq) {
                    float jval = intensities[j];
                    // Suppress self if neighbour is strictly stronger,
                    // or equally strong with a lower sorted index (tie-break).
                    if (jval > my_val || (jval == my_val && j < i)) {
                        keep_flag[i] = false;
                        return;
                    }
                }
            }
        }}}
        keep_flag[i] = true;
    }
    '''
    nms_kernel = cp.RawKernel(nms_kernel_code, 'nms_spatial_hash_kernel')

    keep_sorted = cp.ones(n, dtype=cp.bool_)
    threads     = 256
    blocks_nms  = (n + threads - 1) // threads

    nms_kernel(
        (blocks_nms,), (threads,),
        (
            sorted_pts_f32.ravel(),
            sorted_intens,
            sorted_gc.ravel(),
            cell_start,
            cell_end,
            keep_sorted,
            np.int32(n),
            np.int32(hash_table_size),
            np.float32(min_distance * min_distance),
            P1, P2, P3,
        ),
    )

    # Map keep flags from sorted-by-hash space back to original candidate order
    keep_original = cp.empty(n, dtype=cp.bool_)
    keep_original[sort_order] = keep_sorted
    return keep_original


def watershed_gpu(image: np.ndarray,
                  markers: np.ndarray,
                  mask: Optional[np.ndarray] = None,
                  max_iterations: int = 500) -> np.ndarray:
    """GPU-accelerated watershed segmentation."""
    from skimage.segmentation import watershed as cpu_watershed
    
    if not GPU_ENABLED:
        return cpu_watershed(image, markers, mask=mask)
    
    backend = get_gpu_backend()
    size_mb = image.nbytes / (1024 * 1024)
    required_mb = size_mb * 5
    free_mb = backend.get_free_memory_mb()
    
    if backend.available and size_mb >= GPU_MIN_SIZE_MB and required_mb < free_mb * 0.8 and CUPY_AVAILABLE:
        try:
            return _watershed_gpu_impl(image, markers, mask, max_iterations)
        except Exception as e:
            print(f"[GPU] watershed failed: {e}")
            backend.clear_memory()
    elif backend.available and size_mb >= GPU_MIN_SIZE_MB:
        print(f"[GPU] watershed skipped: need {required_mb:.0f}MB, have {free_mb:.0f}MB")
    
    start = time.time()
    result = cpu_watershed(image, markers, mask=mask)
    print(f"[CPU] watershed: {time.time() - start:.2f}s")
    return result


def _watershed_gpu_impl(image: np.ndarray,
                        markers: np.ndarray,
                        mask: Optional[np.ndarray],
                        max_iterations: int) -> np.ndarray:
    """
    GPU watershed 鈥?3-D shared-memory kernel with hierarchical warp/block
    reduction, plus batched host-sync to minimise PCIe stalls.

    Optimisation summary
    --------------------
    * 8x8x8 thread blocks load a (10,10,10) halo tile into __shared__ memory
      so all 6-neighbour reads hit L1 instead of L2/DRAM.
    * atomicAdd on changed_flag is replaced by:
        thread-local int  鈫? warp __shfl_down_sync  鈫? block __shared__ int
        鈫? single atomicAdd per block (27-64脳 fewer global atomic ops).
    * Host loop synchronises (calls .item()) only every CHECK_INTERVAL steps,
      keeping the GPU pipeline full between convergence checks.
    """
    import cupy as cp

    backend = get_gpu_backend()
    start   = time.time()

    watershed_kernel_code = r'''
    #define BLOCK_DIM_X 8
    #define BLOCK_DIM_Y 8
    #define BLOCK_DIM_Z 8

    extern "C" __global__
    void watershed_kernel(
        const float* __restrict__ image,      // [d*h*w] float32
        const int*   __restrict__ labels_in,  // [d*h*w] int32
        int*                      labels_out, // [d*h*w] int32
        const bool*  __restrict__ mask,       // [d*h*w] bool  (may be NULL)
        const int*   __restrict__ shape,      // [3]  = {d, h, w}
        int*                      changed_flag
    ) {
        // ---- 3-D shared memory with 1-pixel halo ---------------------------
        __shared__ int   s_labels[BLOCK_DIM_Z+2][BLOCK_DIM_Y+2][BLOCK_DIM_X+2];
        __shared__ float s_image [BLOCK_DIM_Z+2][BLOCK_DIM_Y+2][BLOCK_DIM_X+2];
        __shared__ int   s_block_changed;

        const int d = shape[0], h = shape[1], w = shape[2];

        // Global voxel this thread "owns"
        int gx = blockIdx.x * BLOCK_DIM_X + threadIdx.x;
        int gy = blockIdx.y * BLOCK_DIM_Y + threadIdx.y;
        int gz = blockIdx.z * BLOCK_DIM_Z + threadIdx.z;

        // Flat thread ID within the block (for cooperative loading & warp ID)
        int tid = threadIdx.z * (BLOCK_DIM_Y * BLOCK_DIM_X)
                + threadIdx.y * BLOCK_DIM_X
                + threadIdx.x;

        if (tid == 0) s_block_changed = 0;
        __syncthreads();

        // ---- Cooperative halo load -----------------------------------------
        // Shared tile size: (BZ+2)*(BY+2)*(BX+2) = 1000 elements
        // Block threads:    8*8*8 = 512  鈫?each thread loads 1-2 elements
        const int BX2 = BLOCK_DIM_X + 2;
        const int BY2 = BLOCK_DIM_Y + 2;
        const int BZ2 = BLOCK_DIM_Z + 2;
        const int shared_total = BX2 * BY2 * BZ2;
        const int block_threads = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z;

        // Origin of tile in global space (includes -1 halo offset)
        int bx0 = (int)blockIdx.x * BLOCK_DIM_X - 1;
        int by0 = (int)blockIdx.y * BLOCK_DIM_Y - 1;
        int bz0 = (int)blockIdx.z * BLOCK_DIM_Z - 1;

        for (int si = tid; si < shared_total; si += block_threads) {
            int sx = si % BX2;
            int sy = (si / BX2) % BY2;
            int sz = si / (BX2 * BY2);
            int rx = bx0 + sx;
            int ry = by0 + sy;
            int rz = bz0 + sz;
            if (rx >= 0 && rx < w && ry >= 0 && ry < h && rz >= 0 && rz < d) {
                int g = rz * h * w + ry * w + rx;
                s_labels[sz][sy][sx] = labels_in[g];
                s_image [sz][sy][sx] = image[g];
            } else {
                s_labels[sz][sy][sx] = 0;
                s_image [sz][sy][sx] = 1e30f;
            }
        }
        __syncthreads();

        // ---- Process owned voxel  ------------------------------------------
        int local_changed = 0;

        if (gx < w && gy < h && gz < d) {
            int g_idx   = gz * h * w + gy * w + gx;
            bool masked = mask ? mask[g_idx] : true;

            if (!masked) {
                labels_out[g_idx] = 0;
            } else {
                // Shared-memory indices (+1 for halo offset)
                int sx = threadIdx.x + 1;
                int sy = threadIdx.y + 1;
                int sz = threadIdx.z + 1;

                int cur = s_labels[sz][sy][sx];
                if (cur > 0) {
                    labels_out[g_idx] = cur;
                } else {
                    int   best_label = 0;
                    float best_val   = 1e30f;

                    #define CHECK_SHM(dz, dy, dx) {\
                        int nl = s_labels[sz+(dz)][sy+(dy)][sx+(dx)]; \
                        if (nl > 0) { \
                            float nv = s_image[sz+(dz)][sy+(dy)][sx+(dx)]; \
                            if (nv < best_val || (nv == best_val && nl < best_label)) { \
                                best_val = nv; best_label = nl; \
                            } \
                        } \
                    }
                    CHECK_SHM(-1,  0,  0)
                    CHECK_SHM(+1,  0,  0)
                    CHECK_SHM( 0, -1,  0)
                    CHECK_SHM( 0, +1,  0)
                    CHECK_SHM( 0,  0, -1)
                    CHECK_SHM( 0,  0, +1)
                    #undef CHECK_SHM

                    if (best_label > 0) {
                        labels_out[g_idx] = best_label;
                        local_changed = 1;
                    } else {
                        labels_out[g_idx] = 0;
                    }
                }
            }
        }

        // ---- Hierarchical reduction: thread 鈫?warp 鈫?block 鈫?global --------
        // Warp-level reduction with shuffle
        unsigned int full_mask = 0xffffffffu;
        for (int offset = 16; offset > 0; offset >>= 1)
            local_changed += __shfl_down_sync(full_mask, local_changed, offset);

        // Warp lane 0 accumulates into shared block counter
        if ((tid & 31) == 0)
            atomicAdd(&s_block_changed, local_changed);
        __syncthreads();

        // Block thread 0 does single global atomic per block
        if (tid == 0 && s_block_changed > 0)
            atomicAdd(changed_flag, s_block_changed);
    }
    '''
    watershed_kernel = cp.RawKernel(watershed_kernel_code, 'watershed_kernel')

    d, h, w = image.shape
    image_gpu   = cp.asarray(image, dtype=cp.float32)
    labels_curr = cp.asarray(markers, dtype=cp.int32)
    labels_next = cp.empty_like(labels_curr)
    mask_gpu    = (cp.asarray(mask, dtype=cp.bool_)
                   if mask is not None
                   else cp.ones(image.shape, dtype=cp.bool_))
    shape_gpu   = cp.array([d, h, w], dtype=cp.int32)
    changed     = cp.zeros(1, dtype=cp.int32)

    BLOCK = (8, 8, 8)
    GRID  = ((w + 7) // 8, (h + 7) // 8, (d + 7) // 8)

    # -------------------------------------------------------------------
    # Batched host-sync: run CHECK_INTERVAL kernel launches between each
    # .item() call so the GPU pipeline is never starved waiting for the
    # host to decide "keep going".
    # -------------------------------------------------------------------
    CHECK_INTERVAL = 10
    final_iter     = 0
    n_changed      = 1  # sentinel 鈥?assume not converged yet

    for outer in range(0, max_iterations, CHECK_INTERVAL):
        changed.fill(0)  # async clear 鈥?no host stall
        for inner in range(CHECK_INTERVAL):
            current_iter = outer + inner
            if current_iter >= max_iterations:
                break
            watershed_kernel(
                GRID, BLOCK,
                (image_gpu, labels_curr, labels_next, mask_gpu, shape_gpu, changed),
            )
            labels_curr, labels_next = labels_next, labels_curr
            final_iter = current_iter

        # Single sync per batch 鈥?amortises PCIe round-trip over CHECK_INTERVAL iters
        n_changed = int(changed.item())
        if n_changed == 0:
            break

    print(f"[GPU] watershed: {time.time() - start:.2f}s, {final_iter + 1} iters")
    result = backend.to_cpu(labels_curr)
    del image_gpu, labels_curr, labels_next, mask_gpu, shape_gpu, changed
    backend.clear_memory(force=False)
    return result


# =============================================================================
# GPU-Accelerated Threshold Computation Functions
# =============================================================================

def compute_histogram_gpu(data: np.ndarray, bins: int = 256, 
                          range_: Optional[tuple] = None) -> tuple:
    """GPU-accelerated histogram computation."""
    if not GPU_ENABLED:
        return np.histogram(data, bins=bins, range=range_)
    
    backend = get_gpu_backend()
    size_mb = data.nbytes / (1024 * 1024)
    
    if backend.available and size_mb >= GPU_MIN_SIZE_MB and CUPY_AVAILABLE:
        try:
            import cupy as cp
            start = time.time()
            data_gpu = cp.asarray(data)
            hist_gpu, bin_edges_gpu = cp.histogram(data_gpu, bins=bins, range=range_)
            hist, bin_edges = cp.asnumpy(hist_gpu), cp.asnumpy(bin_edges_gpu)
            del data_gpu, hist_gpu, bin_edges_gpu
            backend.clear_memory(force=False)
            print(f"[GPU] histogram: {time.time() - start:.3f}s")
            return hist, bin_edges
        except Exception as e:
            print(f"[GPU] histogram failed: {e}")
            backend.clear_memory(force=False)
    
    start = time.time()
    result = np.histogram(data, bins=bins, range=range_)
    print(f"[CPU] histogram: {time.time() - start:.3f}s")
    return result


def compute_statistics_gpu(data: np.ndarray) -> dict:
    """GPU-accelerated statistics: mean, std, skewness, kurtosis."""
    if not GPU_ENABLED:
        return _compute_statistics_cpu(data)
    
    backend = get_gpu_backend()
    if not (backend.available and data.nbytes / (1024 * 1024) >= GPU_MIN_SIZE_MB and CUPY_AVAILABLE):
        return _compute_statistics_cpu(data)
    
    try:
        import cupy as cp
        start = time.time()
        data_gpu = cp.asarray(data)
        
        mean, std = float(cp.mean(data_gpu).item()), float(cp.std(data_gpu).item())
        data_min, data_max = float(cp.min(data_gpu).item()), float(cp.max(data_gpu).item())
        
        skewness, kurtosis = 0.0, 0.0
        if std > 0:
            z = (data_gpu - mean) / std
            skewness, kurtosis = float(cp.mean(z**3).item()), float(cp.mean(z**4).item()) - 3
        
        del data_gpu
        backend.clear_memory(force=False)
        print(f"[GPU] statistics: {time.time() - start:.3f}s")
        
        return {'mean': mean, 'std': std, 'skewness': skewness, 'kurtosis': kurtosis,
                'min': data_min, 'max': data_max, 'n': len(data)}
    except Exception as e:
        print(f"[GPU] statistics failed: {e}")
        backend.clear_memory(force=False)
        return _compute_statistics_cpu(data)


def _compute_statistics_cpu(data: np.ndarray) -> dict:
    """CPU statistics computation."""
    start = time.time()
    mean, std = float(np.mean(data)), float(np.std(data))
    data_min, data_max = float(np.min(data)), float(np.max(data))
    
    skewness, kurtosis = 0.0, 0.0
    if std > 0:
        z = (data - mean) / std
        skewness, kurtosis = float(np.mean(z**3)), float(np.mean(z**4)) - 3
    
    print(f"[CPU] statistics: {time.time() - start:.3f}s")
    return {'mean': mean, 'std': std, 'skewness': skewness, 'kurtosis': kurtosis,
            'min': data_min, 'max': data_max, 'n': len(data)}


def threshold_otsu_gpu(data: np.ndarray, nbins: int = 256) -> float:
    """GPU-accelerated Otsu threshold (delegates to unified function)."""
    try:
        from processors.threshold_gpu import compute_threshold_stats_gpu
        return compute_threshold_stats_gpu(data, nbins)['otsu_threshold']
    except Exception:
        pass
    
    from skimage.filters import threshold_otsu
    if not GPU_ENABLED:
        return threshold_otsu(data)
    
    backend = get_gpu_backend()
    if not (backend.available and data.nbytes / (1024 * 1024) >= GPU_MIN_SIZE_MB and CUPY_AVAILABLE):
        return threshold_otsu(data)
    
    try:
        import cupy as cp
        start = time.time()
        data_gpu = cp.asarray(data)
        hist, bin_edges = cp.histogram(data_gpu, bins=nbins)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        
        hist_norm = hist.astype(cp.float64) / hist.sum()
        w1, w2 = cp.cumsum(hist_norm), 1.0 - cp.cumsum(hist_norm)
        m1_cs = cp.cumsum(hist_norm * bin_centers)
        m1 = m1_cs / (w1 + 1e-10)
        m2 = (float((hist_norm * bin_centers).sum().item()) - m1_cs) / (w2 + 1e-10)
        
        threshold = float(bin_centers[cp.argmax(w1 * w2 * (m1 - m2)**2)].item())
        del data_gpu, hist, bin_edges, bin_centers, hist_norm, w1, w2, m1_cs, m1, m2
        backend.clear_memory(force=False)
        print(f"[GPU] Otsu: {time.time() - start:.3f}s, value={threshold:.1f}")
        return threshold
    except Exception as e:
        print(f"[GPU] Otsu failed: {e}")
        backend.clear_memory(force=False)
        return threshold_otsu(data)


--- FILE: .\processors\__init__.py ---
"""
Data processors package for volumetric analysis.

Modules:
- pore: Pore extraction from CT data
- pnm: Pore Network Modeling (main processor)
- pnm_adjacency: Pore adjacency detection algorithms
- pnm_throat: Throat mesh generation algorithms
- pnm_tracker: 4D CT pore tracking
- utils: GPU-accelerated utility functions
"""

from processors.pore import PoreExtractionProcessor
from processors.pnm import PoreToSphereProcessor
from processors.pnm_adjacency import find_adjacency
from processors.pnm_throat import create_throat_mesh
from processors.pnm_tracker import PNMTracker

__all__ = [
    'PoreExtractionProcessor', 
    'PoreToSphereProcessor',
    'find_adjacency',
    'create_throat_mesh',
    'PNMTracker',
]



--- FILE: .\rendering\clip_handler.py ---
"""
Clip plane handler for volume visualization.
"""

from typing import Optional, Dict, Any
import pyvista as pv


class ClipHandler:
    """
    Handles clip plane logic for visualization.
    Designed for composition with GUI/rendering classes.
    """

    def __init__(self, plotter, clip_panel, render_engine=None):
        """
        Initialize clip handler.
        
        Args:
            plotter: BackgroundPlotter instance
            clip_panel: ClipPlanePanel for UI controls
            render_engine: Optional RenderEngine for rendering callbacks
        """
        self.plotter = plotter
        self.clip_panel = clip_panel
        self.render_engine = render_engine

    def update_clip_panel_state(self, active_mode: str):
        """Enable/Disable clip panel based on active mode."""
        if not self.clip_panel:
            return

        supported_modes = ['volume', 'mesh', 'iso']
        is_supported = active_mode in supported_modes

        self.clip_panel.setEnabled(is_supported)
        self.clip_panel.enable_checkbox.blockSignals(True)
        self.clip_panel.enable_checkbox.setChecked(False)
        self.clip_panel.enable_checkbox.blockSignals(False)
        self.clip_panel._enabled = False
        self.clip_panel._update_slider_state()

    def on_clip_toggled(self, enabled: bool):
        """Handle clip plane enable/disable toggle."""
        if enabled:
            self.apply_clip_planes()
        else:
            # Force full re-render to restore unclipped data
            if self.render_engine:
                mode = self.render_engine.active_view_mode
                if mode == 'volume':
                    self.render_engine.render_volume(reset_view=True)
                elif mode == 'slices':
                    self.render_engine.render_slices(reset_view=True)
                elif mode == 'iso':
                    self.render_engine.render_isosurface_auto(reset_view=True)
                elif mode == 'mesh':
                    self.render_engine.render_mesh(reset_view=True)

    def apply_clip_planes(self):
        """Apply clip planes to current visualization."""
        if not self.clip_panel:
            return

        clip_vals = self.clip_panel.get_clip_values()
        if not clip_vals['enabled']:
            return

        EPS = 0.005
        for axis in ['x', 'y', 'z']:
            if not clip_vals[f'invert_{axis}']:
                clip_vals[axis] = max(EPS, clip_vals[axis])
            else:
                clip_vals[axis] = min(1.0 - EPS, clip_vals[axis])

        try:
            if self.render_engine is None:
                return

            data_source = None
            mode = self.render_engine.active_view_mode

            if mode == 'volume':
                data_source = self.render_engine.grid
            elif mode == 'mesh':
                data_source = self.render_engine.mesh
            elif mode == 'iso':
                params = self.render_engine.params_panel.get_current_values() if self.render_engine.params_panel else {}
                thresh = params.get('threshold', 300)
                if thresh in self.render_engine._iso_cache:
                    data_source = self.render_engine._iso_cache[thresh]
                elif self.render_engine.grid:
                    data_source = self.render_engine.grid.cell_data_to_point_data().contour([thresh])

            if data_source is None:
                return

            bounds = data_source.bounds

            x_min = bounds[0]
            x_max = bounds[0] + (bounds[1] - bounds[0]) * clip_vals['x']
            y_min = bounds[2]
            y_max = bounds[2] + (bounds[3] - bounds[2]) * clip_vals['y']
            z_min = bounds[4]
            z_max = bounds[4] + (bounds[5] - bounds[4]) * clip_vals['z']

            if clip_vals['invert_x']:
                x_min, x_max = x_max, bounds[1]
            if clip_vals['invert_y']:
                y_min, y_max = y_max, bounds[3]
            if clip_vals['invert_z']:
                z_min, z_max = z_max, bounds[5]

            clip_bounds = [x_min, x_max, y_min, y_max, z_min, z_max]

            self.plotter.clear()
            self.plotter.add_axes()
            params = self.render_engine.params_panel.get_current_values() if self.render_engine.params_panel else {}

            new_iso_actor = None

            if mode == 'volume' and self.render_engine.grid is not None:
                clipped = self.render_engine.grid.clip_box(clip_bounds, invert=False)
                if clipped.n_cells > 0:
                    self.plotter.add_mesh(
                        clipped,
                        scalars="values",
                        cmap=params.get('colormap', 'bone'),
                        clim=params.get('clim', [0, 1000]),
                        show_scalar_bar=True,
                        opacity=0.5
                    )

            elif mode == 'mesh' and self.render_engine.mesh is not None:
                clipped = self.render_engine.mesh.clip_box(clip_bounds, invert=False)
                if clipped.n_points > 0:
                    self.plotter.add_mesh(
                        clipped,
                        scalars="IsPore" if "IsPore" in clipped.array_names else None,
                        cmap=["gray", "red"] if "IsPore" in clipped.array_names else params.get('colormap', 'viridis'),
                        show_scalar_bar=False,
                        smooth_shading=True
                    )

            elif mode == 'iso':
                clipped = data_source.clip_box(clip_bounds, invert=False)
                mesh_kwargs = self.render_engine.get_iso_mesh_kwargs(params=params, sync_mode=True)

                coloring = params.get('coloring_mode', 'Solid Color')
                if coloring == 'Solid Color':
                    new_iso_actor = self.plotter.add_mesh(clipped, color=params.get('solid_color', 'ivory'), **mesh_kwargs)
                elif coloring == 'Depth (Z-Axis)':
                    clipped["Elevation"] = clipped.points[:, 2]
                    new_iso_actor = self.plotter.add_mesh(clipped, scalars="Elevation", cmap=params.get('colormap', 'viridis'), **mesh_kwargs)
                else:
                    new_iso_actor = self.plotter.add_mesh(clipped, color='white', **mesh_kwargs)

            if self.render_engine:
                self.render_engine.iso_actor = new_iso_actor if mode == 'iso' else None

            self.render_engine._apply_custom_lighting(params)
            self.plotter.render()

        except Exception as e:
            print(f"[ClipHandler] Error: {e}")
            self.on_clip_toggled(False)


--- FILE: .\rendering\lod_manager.py ---
"""
Level of Detail (LOD) manager for efficient volume rendering.
Provides multi-resolution pyramid and GPU capability detection.
"""

from typing import Optional, Dict, List, Tuple
import numpy as np
import pyvista as pv

# Try to import VTK for GPU detection
try:
    import vtk
    VTK_AVAILABLE = True
except ImportError:
    VTK_AVAILABLE = False


def check_gpu_volume_rendering() -> Tuple[bool, str]:
    """
    Check if GPU volume rendering is available.
    
    Returns:
        Tuple of (is_available, description)
    """
    if not VTK_AVAILABLE:
        return False, "VTK not available"
    
    try:
        render_window = vtk.vtkRenderWindow()
        render_window.SetOffScreenRendering(1)
        render_window.Render()
        
        mapper = vtk.vtkGPUVolumeRayCastMapper()
        vol_property = vtk.vtkVolumeProperty()
        
        renderer = vtk.vtkRenderer()
        render_window.AddRenderer(renderer)
        
        if mapper.IsRenderSupported(render_window, vol_property):
            return True, "GPU volume rendering supported"
        else:
            return False, "GPU volume rendering not supported by hardware"
    except Exception as e:
        return False, f"GPU check failed: {e}"


class LODPyramid:
    """
    Multi-resolution pyramid for efficient rendering.
    Creates downsampled versions of volume data at multiple levels.
    """
    
    def __init__(self, grid: pv.ImageData, levels: int = 3):
        """
        Create LOD pyramid from a PyVista ImageData grid.
        
        Args:
            grid: Original full-resolution grid.
            levels: Number of LOD levels to create (including original).
        """
        self.levels: List[pv.ImageData] = []
        self.level_info: List[Dict] = []
        
        self._build_pyramid(grid, levels)
    
    def _build_pyramid(self, grid: pv.ImageData, levels: int):
        """Build the LOD pyramid."""
        current = grid
        
        for level in range(levels):
            self.levels.append(current)
            self.level_info.append({
                'level': level,
                'n_cells': current.n_cells,
                'dimensions': current.dimensions,
                'memory_mb': current.n_cells * 4 / (1024 * 1024)  # Approximate float32
            })
            
            if level < levels - 1:
                # Create downsampled version
                dims = current.dimensions
                if min(dims) > 10:  # Ensure we have enough resolution
                    try:
                        current = current.extract_subset(
                            voi=(0, dims[0]-1, 0, dims[1]-1, 0, dims[2]-1),
                            rate=(2, 2, 2)
                        )
                    except (ValueError, RuntimeError):
                        # Can't downsample further
                        break
                else:
                    break
    
    def get_level(self, level: int) -> Optional[pv.ImageData]:
        """Get grid at specified LOD level."""
        if 0 <= level < len(self.levels):
            return self.levels[level]
        return self.levels[-1] if self.levels else None
    
    def get_for_distance(self, camera_distance: float, 
                         far_threshold: float = 1000,
                         mid_threshold: float = 500) -> pv.ImageData:
        """
        Get appropriate LOD based on camera distance.
        
        Args:
            camera_distance: Distance from camera to object.
            far_threshold: Distance above which to use lowest LOD.
            mid_threshold: Distance above which to use medium LOD.
            
        Returns:
            Grid at appropriate LOD level.
        """
        if camera_distance > far_threshold and len(self.levels) > 2:
            return self.levels[2]
        elif camera_distance > mid_threshold and len(self.levels) > 1:
            return self.levels[1]
        return self.levels[0]
    
    def get_for_memory(self, max_memory_mb: float = 400) -> pv.ImageData:
        """
        Get highest resolution LOD that fits in memory budget.
        
        Args:
            max_memory_mb: Maximum memory budget in MB.
            
        Returns:
            Grid that fits in memory budget.
        """
        for i, info in enumerate(self.level_info):
            if info['memory_mb'] <= max_memory_mb:
                return self.levels[i]
        return self.levels[-1] if self.levels else None
    
    @property
    def num_levels(self) -> int:
        return len(self.levels)
    
    def __repr__(self):
        info_str = ", ".join([f"L{i['level']}:{i['n_cells']:,}" for i in self.level_info])
        return f"LODPyramid({info_str})"


class LODRenderManager:
    """
    Manages LOD-based rendering with automatic level selection.
    """
    
    def __init__(self, plotter, status_callback=None):
        """
        Args:
            plotter: PyVista plotter instance.
            status_callback: Optional status update callback.
        """
        self.plotter = plotter
        self._status_callback = status_callback
        self.pyramid: Optional[LODPyramid] = None
        self.current_level: int = 0
        
        # Check GPU capability once
        self.gpu_available, self.gpu_info = check_gpu_volume_rendering()
        if status_callback:
            status_callback(f"GPU rendering: {self.gpu_info}")
    
    def set_grid(self, grid: pv.ImageData, levels: int = 3):
        """
        Create LOD pyramid from grid.
        
        Args:
            grid: Original grid.
            levels: Number of LOD levels.
        """
        self.pyramid = LODPyramid(grid, levels)
        if self._status_callback:
            self._status_callback(f"Created LOD pyramid: {self.pyramid}")
    
    def get_render_grid(self, max_memory_mb: float = 400) -> Optional[pv.ImageData]:
        """Get grid suitable for rendering within memory budget."""
        if self.pyramid is None:
            return None
        return self.pyramid.get_for_memory(max_memory_mb)
    
    def update_lod_for_camera(self):
        """Update LOD based on current camera position."""
        if self.pyramid is None or len(self.pyramid.levels) == 0:
            return
        
        # Get camera distance
        camera = self.plotter.camera
        if camera and hasattr(camera, 'distance'):
            distance = camera.distance
            grid = self.pyramid.get_for_distance(distance)
            # Here you would trigger a re-render with the new LOD
            # Implementation depends on how rendering is structured


--- FILE: .\rendering\render_engine.py ---
"""
Core rendering engine for volumetric data visualization.
Provides reusable rendering methods independent of GUI framework.
Includes LOD (Level of Detail) support for large volume handling.
"""

from typing import Optional, Dict, Any, Tuple
import numpy as np
import pyvista as pv
from core import VolumeData
from core.dto import RenderParamsDTO
from rendering.lod_manager import LODPyramid, LODRenderManager, check_gpu_volume_rendering
from config import (
    RENDER_MAX_VOXELS_VOLUME,
    RENDER_MAX_VOXELS_ISO,
    RENDER_MAX_MEMORY_MB
)

# Aliases for backward compatibility
MAX_VOXELS_FOR_VOLUME = RENDER_MAX_VOXELS_VOLUME
MAX_VOXELS_FOR_ISO = RENDER_MAX_VOXELS_ISO
MAX_MEMORY_MB_RENDER = RENDER_MAX_MEMORY_MB


class RenderEngine:
    """
    Handles PyVista rendering logic.
    Designed for composition with GUI classes.
    """

    def __init__(self, plotter, params_panel, info_panel=None, clip_panel=None, status_callback=None):
        """
        Initialize engine with renderer dependencies.
        
        Args:
            plotter: BackgroundPlotter instance
            params_panel: RenderingParametersPanel for current parameters
            info_panel: Optional InfoPanel for data display
            clip_panel: Optional ClipPlanePanel for clipping controls
            status_callback: Optional callback for status updates
        """
        self.plotter = plotter
        self.params_panel = params_panel
        self.info_panel = info_panel
        self.clip_panel = clip_panel
        self._status_callback = status_callback
        
        # Data state
        self.data: Optional[VolumeData] = None
        self.grid: Optional[pv.ImageData] = None
        self.mesh: Optional[pv.PolyData] = None
        self.active_view_mode: Optional[str] = None
        
        # Caches
        self._iso_cache: Dict[int, pv.PolyData] = {}
        self._cached_vol_grid: Optional[pv.PolyData] = None
        self._cached_vol_grid_source: Optional[int] = None
        self._lod_pyramid: Optional[LODPyramid] = None
        
        # PNM color range cache (for consistent colors across timepoints)
        self._pnm_radius_clim: Optional[Tuple[float, float]] = None
        self._pnm_compression_clim: Optional[Tuple[float, float]] = None
        self._highlight_pore_id: Optional[int] = None
        
        # GPU capability
        self.gpu_available, self.gpu_info = check_gpu_volume_rendering()

        # Actors
        self.volume_actor = None
        self.iso_actor = None
        self._current_iso_threshold: Optional[int] = None

        # Centralized render-style state (single source of truth for isosurface style)
        self._current_render_mode = 'surface'
        self._current_render_show_edges = False
        self._current_render_mode_label = 'Surface'

        # Injected params DTO (used when running headless without a params_panel)
        self._injected_params: Optional[RenderParamsDTO] = None

    # ------------------------------------------------------------------
    # Headless / DTO interface
    # ------------------------------------------------------------------

    def inject_params(self, dto: RenderParamsDTO) -> None:
        """
        Push a RenderParamsDTO directly into the engine.

        When set, ``_get_render_params()`` will return values from this DTO
        instead of querying the live params_panel.  Call with ``None`` to
        restore live-panel mode.
        """
        self._injected_params = dto

    def _get_render_params(self) -> Dict[str, Any]:
        """
        Return current render parameters as a plain dict.

        Priority:
        1. Injected DTO (headless / CLI mode)
        2. Live params_panel.get_current_values() (GUI mode)
        3. Hard-coded safe defaults (neither present)
        """
        if self._injected_params is not None:
            return self._injected_params.to_dict()
        if self.params_panel is not None:
            return self.params_panel.get_current_values()
        return RenderParamsDTO().to_dict()

    # ------------------------------------------------------------------

    def update_status(self, message: str):
        """Update status via callback."""
        if self._status_callback:
            self._status_callback(message)
        else:
            print(f"[RenderEngine] {message}")

    def set_data(self, data: VolumeData):
        """Set volume data and prepare grid. Clears previous data first."""
        # Explicitly release previous references 鈥?no gc.collect() needed;
        # Python's reference counting handles deallocation as soon as the
        # last reference drops.
        if self.data is not None or self.grid is not None:
            self.update_status("Clearing previous data...")

        self.data            = None
        self.grid            = None
        self.mesh            = None
        self.volume_actor    = None
        self.iso_actor       = None
        self._current_iso_threshold = None
        self._iso_cache      = {}
        self._cached_vol_grid = None
        self._lod_pyramid    = None

        # Now set new data
        self.data = data

        if data.has_mesh:
            self.mesh = data.mesh
        elif data.raw_data is not None:
            self._create_pyvista_grid()
            # Create LOD pyramid for large volumes
            if self.grid and self.grid.n_cells > MAX_VOXELS_FOR_VOLUME:
                self._lod_pyramid = LODPyramid(self.grid, levels=3)
                self.update_status(f"Created LOD pyramid: {self._lod_pyramid}")

    def _create_pyvista_grid(self):
        """Create PyVista ImageData grid from volume data."""
        if not self.data or self.data.raw_data is None:
            return
        grid = pv.ImageData()
        grid.dimensions = np.array(self.data.raw_data.shape) + 1
        grid.origin = self.data.origin
        grid.spacing = self.data.spacing
        grid.cell_data["values"] = self.data.raw_data.flatten(order="F")
        self.grid = grid

    def clear_view(self):
        """Clear all actors from plotter."""
        self.plotter.clear()
        self.plotter.add_axes()
        self.active_view_mode = None
        self.volume_actor = None
        self.iso_actor = None
        self._current_iso_threshold = None
        if self.params_panel:
            self.params_panel.set_mode(None)

    def set_highlight_pore(self, pore_id: Optional[int]):
        """Set the pore ID to highlight in yellow. Pass None to clear highlight."""
        self._highlight_pore_id = pore_id
    
    def clear_pnm_color_cache(self):
        """Clear PNM color range cache (call when loading new dataset)."""
        self._pnm_radius_clim = None
        self._highlight_pore_id = None

    def reset_camera(self):
        """Reset camera to isometric view."""
        self.plotter.reset_camera()
        self.plotter.view_isometric()

    def update_clim_fast(self, clim: Tuple[float, float]) -> bool:
        """Fast update of scalar range without re-rendering volume.
        
        Args:
            clim: Tuple of (min, max) scalar values.
            
        Returns:
            True if update was successful, False if full re-render is needed.
        """
        if self.volume_actor is None:
            return False
        
        try:
            # Try to update mapper scalar range directly
            if hasattr(self.volume_actor, 'mapper') and self.volume_actor.mapper:
                self.volume_actor.mapper.scalar_range = clim
                self.plotter.render()
                return True
        except Exception as e:
            print(f"[RenderEngine] Fast clim update failed: {e}")
        
        return False

    def _get_downsampled_grid(self, max_voxels: int) -> Optional[pv.ImageData]:
        """Get grid with automatic downsampling if exceeds voxel threshold.
        
        Args:
            max_voxels: Maximum allowed voxels before downsampling.
            
        Returns:
            Original or downsampled grid.
        """
        if not self.grid:
            return None
        
        n_cells = self.grid.n_cells
        if n_cells <= max_voxels:
            return self.grid
        
        # Calculate downsampling step (cubic root to reduce in all dimensions)
        step = int(np.ceil((n_cells / max_voxels) ** (1/3)))
        step = max(2, step)  # Minimum step of 2
        dims = self.grid.dimensions
        
        try:
            # Extract every Nth cell in each dimension using keyword args
            downsampled = self.grid.extract_subset(
                voi=(0, dims[0]-1, 0, dims[1]-1, 0, dims[2]-1),
                rate=(step, step, step)
            )
            self.update_status(f"Auto-downsampled for memory: {n_cells:,} -> {downsampled.n_cells:,} cells (step={step})")
            return downsampled
        except Exception as e:
            print(f"[RenderEngine] Downsampling failed: {e}")
            return self.grid

    def _get_grid_for_isosurface(self) -> Optional[pv.ImageData]:
        """Get grid suitable for isosurface rendering."""
        # Use LOD pyramid if available
        if self._lod_pyramid and self._lod_pyramid.num_levels > 1:
            return self._lod_pyramid.get_for_memory(MAX_MEMORY_MB_RENDER)
        return self._get_downsampled_grid(MAX_VOXELS_FOR_ISO)
    
    def _get_grid_for_volume(self) -> Optional[pv.ImageData]:
        """Get grid suitable for volume rendering using LOD if available."""
        # Use LOD pyramid if available (pre-computed levels)
        if self._lod_pyramid and self._lod_pyramid.num_levels > 1:
            grid = self._lod_pyramid.get_for_memory(MAX_MEMORY_MB_RENDER)
            if grid:
                self.update_status(f"Using LOD level: {grid.n_cells:,} cells")
                return grid
        return self._get_downsampled_grid(MAX_VOXELS_FOR_VOLUME)

    def _update_clip_panel_state(self):
        """Enable/Disable clip panel based on active mode."""
        if not self.clip_panel:
            return

        supported_modes = ['volume', 'mesh', 'iso']
        is_supported = self.active_view_mode in supported_modes
        self.clip_panel.setEnabled(is_supported)

        self.clip_panel.enable_checkbox.blockSignals(True)
        self.clip_panel.enable_checkbox.setChecked(False)
        self.clip_panel.enable_checkbox.blockSignals(False)
        self.clip_panel._enabled = False
        self.clip_panel._update_slider_state()

    @staticmethod
    def normalize_render_mode(render_style: Optional[str]) -> Tuple[str, bool, str]:
        """Normalize UI render style text into VTK style + edge-visibility flags."""
        style_map = {
            'surface': ('surface', False, 'Surface'),
            'wireframe': ('wireframe', False, 'Wireframe'),
            'wireframe + surface': ('surface', True, 'Wireframe + Surface'),
            'points': ('points', False, 'Points'),
        }
        key = str(render_style or 'Surface').strip().lower()
        return style_map.get(key, ('surface', False, 'Surface'))

    def get_current_render_mode_label(self) -> str:
        """Return current UI label for isosurface render style."""
        return self._current_render_mode_label

    def get_iso_mesh_kwargs(self, params: Optional[Dict[str, Any]] = None, sync_mode: bool = True) -> Dict[str, Any]:
        """
        Build shared mesh kwargs for isosurface-like rendering.

        This centralizes style mapping to keep render paths DRY.
        """
        render_style = self._current_render_mode_label
        if isinstance(params, dict):
            render_style = params.get('render_style', render_style)

        style, show_edges, canonical = self.normalize_render_mode(render_style)
        if sync_mode:
            self._current_render_mode = style
            self._current_render_show_edges = show_edges
            self._current_render_mode_label = canonical

        return {
            'style': style,
            'show_edges': show_edges,
            'smooth_shading': True,
            'specular': 0.4,
            'diffuse': 0.7,
            'ambient': 0.15,
            'lighting': True
        }

    def _apply_actor_render_style(self, actor, style: str, show_edges: bool) -> bool:
        """Apply style in-place to an existing actor without rebuilding scene objects."""
        prop = getattr(actor, 'prop', None)
        if prop is None:
            try:
                prop = actor.GetProperty()
            except Exception:
                prop = None
        if prop is None:
            return False

        changed = False
        try:
            # PyVista actor property helper
            prop.style = style
            changed = True
        except Exception:
            try:
                # VTK fallback
                if style == 'wireframe':
                    prop.SetRepresentationToWireframe()
                elif style == 'points':
                    prop.SetRepresentationToPoints()
                else:
                    prop.SetRepresentationToSurface()
                changed = True
            except Exception:
                pass

        try:
            if hasattr(prop, 'show_edges'):
                prop.show_edges = bool(show_edges)
                changed = True
            elif hasattr(prop, 'SetEdgeVisibility'):
                prop.SetEdgeVisibility(1 if show_edges else 0)
                changed = True
        except Exception:
            pass

        return changed

    def request_render_mode_change(self, mode_label: str) -> str:
        """
        Request an isosurface render-style change.

        Returns:
            'unchanged': requested mode equals current state
            'applied': changed and applied in-place to current iso actor
            'state_only': state changed but no live iso actor to mutate
        """
        style, show_edges, canonical = self.normalize_render_mode(mode_label)
        if style == self._current_render_mode and show_edges == self._current_render_show_edges:
            return 'unchanged'

        self._current_render_mode = style
        self._current_render_show_edges = show_edges
        self._current_render_mode_label = canonical

        if self.active_view_mode == 'iso' and self.iso_actor is not None:
            if self._apply_actor_render_style(self.iso_actor, style, show_edges):
                self.plotter.render()
                return 'applied'
        return 'state_only'

    def render_mesh(self, reset_view=True):
        """Render PNM mesh with PoreRadius colormap."""
        if not self.mesh:
            return
        
        # Save camera state before clearing (for smooth time step transitions)
        saved_camera_position = None
        saved_camera_focal_point = None
        saved_camera_view_up = None
        saved_camera_view_angle = None
        
        if not reset_view and self.active_view_mode == 'mesh':
            try:
                # Save detailed camera state
                saved_camera_position = self.plotter.camera.position
                saved_camera_focal_point = self.plotter.camera.focal_point
                saved_camera_view_up = self.plotter.camera.up
                saved_camera_view_angle = self.plotter.camera.view_angle
            except Exception:
                pass
            
        # Clean up previous actors before adding new ones
        self.plotter.clear()
        self.plotter.add_axes()
        self.volume_actor = None
        self.iso_actor = None
        self._current_iso_threshold = None
        
        self.active_view_mode = 'mesh'
        self._update_clip_panel_state()
        if self.params_panel:
            self.params_panel.set_mode('mesh')
        self.plotter.enable_lightkit()

        params = self._get_render_params()

        # Priority 1: CompressionRatio (cached range) with optional highlight
        if "CompressionRatio" in self.mesh.point_data:
            comp = self.mesh.point_data["CompressionRatio"]
            if self._pnm_compression_clim is None:
                self._pnm_compression_clim = (float(comp.min()), float(comp.max()))
            clim_comp = self._pnm_compression_clim

            # Invert so smaller ratios look darker
            comp_display = 1.0 - comp
            clim_display = (1.0 - clim_comp[1], 1.0 - clim_comp[0])

            if self._highlight_pore_id is not None and "ID" in self.mesh.point_data:
                pore_ids = self.mesh.point_data.get("ID", np.array([]))
                highlight_mask = (pore_ids == self._highlight_pore_id)
                if highlight_mask.any():
                    display_scalar = comp_display.copy()
                    # set highlighted to max+10% to map to yellow
                    display_scalar[highlight_mask] = clim_display[1] * 1.1
                    self.plotter.add_mesh(
                        self.mesh,
                        scalars=display_scalar,
                        cmap=['black', 'dimgray', 'lightgray', 'yellow'],
                        clim=[clim_display[0], clim_display[1] * 1.1],
                        show_scalar_bar=True,
                        scalar_bar_args={'title': 'Compression (dark=compressed, yellow=selected)'},
                        smooth_shading=True,
                        specular=0.5
                    )
                    return

            # No highlight
            self.plotter.add_mesh(
                self.mesh,
                scalars=comp_display,
                cmap='Greys',
                clim=clim_display,
                show_scalar_bar=True,
                scalar_bar_args={'title': 'Compression (dark=compressed)'},
                smooth_shading=True,
                specular=0.5
            )

        # Priority 2: PoreRadius (cached range) with optional highlight
        elif "PoreRadius" in self.mesh.point_data:
            pore_radii = self.mesh.point_data["PoreRadius"]
            if self._pnm_radius_clim is None:
                self._pnm_radius_clim = (float(pore_radii.min()), float(pore_radii.max()))

            if self._highlight_pore_id is not None and "ID" in self.mesh.point_data:
                pore_ids = self.mesh.point_data.get("ID", np.array([]))
                highlight_mask = (pore_ids == self._highlight_pore_id)
                if highlight_mask.any():
                    display_scalar = pore_radii.copy()
                    max_radius = self._pnm_radius_clim[1]
                    display_scalar[highlight_mask] = max_radius * 1.5
                    self.plotter.add_mesh(
                        self.mesh,
                        scalars=display_scalar,
                        cmap=['darkblue', 'blue', 'cyan', 'lime', 'yellow'],
                        clim=[self._pnm_radius_clim[0], max_radius * 1.5],
                        show_scalar_bar=True,
                        scalar_bar_args={'title': 'Pore Radius (mm) - Yellow=Selected'},
                        smooth_shading=True,
                        specular=0.5
                    )
                    return

            # Normal rendering with cached color range
            self.plotter.add_mesh(
                self.mesh,
                scalars="PoreRadius",
                cmap=params.get('colormap', 'Oranges'),
                clim=self._pnm_radius_clim,
                show_scalar_bar=True,
                scalar_bar_args={'title': 'Pore Radius (mm)'},
                smooth_shading=True,
                specular=0.5
            )
        elif "IsPore" in self.mesh.array_names:
            self.plotter.add_mesh(
                self.mesh,
                scalars="IsPore",
                cmap=["gray", "red"],
                categories=True,
                show_scalar_bar=False,
                smooth_shading=True,
                specular=0.5
            )
        else:
            self.plotter.add_mesh(self.mesh, color='gold', smooth_shading=True, specular=0.5)

        if reset_view:
            self.reset_camera()
        elif saved_camera_position is not None:
            # Restore previous camera state with detailed parameters
            try:
                self.plotter.camera.position = saved_camera_position
                self.plotter.camera.focal_point = saved_camera_focal_point
                self.plotter.camera.up = saved_camera_view_up
                self.plotter.camera.view_angle = saved_camera_view_angle
                self.plotter.render()
            except Exception:
                pass

    def render_volume(self, reset_view=True):
        """
        Optimized Volume Rendering.
        Updates Opacity/Color in-place when possible.
        """
        if not self.grid:
            return

        # Save camera state before potential view change
        saved_camera_position = None
        saved_camera_focal_point = None
        saved_camera_view_up = None
        saved_camera_view_angle = None
        
        if not reset_view and self.active_view_mode in ['volume', 'mesh', 'slices', 'iso']:
            try:
                # Save detailed camera state
                saved_camera_position = self.plotter.camera.position
                saved_camera_focal_point = self.plotter.camera.focal_point
                saved_camera_view_up = self.plotter.camera.up
                saved_camera_view_angle = self.plotter.camera.view_angle
            except Exception:
                pass

        if reset_view or self.active_view_mode != 'volume' or self.volume_actor is None:
            self.update_status("Rendering volume (New)...")
            self.clear_view()
            self.active_view_mode = 'volume'
            self._update_clip_panel_state()
            if self.params_panel:
                self.params_panel.set_mode('volume')
            self.plotter.enable_lightkit()

            # Get potentially downsampled grid for memory safety
            safe_grid = self._get_grid_for_volume()
            if safe_grid is None:
                self.update_status("No grid available for volume rendering")
                return
            
            # Cache the point data grid
            grid_id = id(safe_grid)
            if self._cached_vol_grid is None or self._cached_vol_grid_source != grid_id:
                self._cached_vol_grid = safe_grid.cell_data_to_point_data()
                self._cached_vol_grid_source = grid_id

            vol_grid = self._cached_vol_grid
            params = self._get_render_params()

            self.volume_actor = self.plotter.add_volume(
                vol_grid,
                cmap=params.get('colormap', 'bone'),
                opacity=params.get('opacity', 'sigmoid'),
                clim=params.get('clim', [0, 1000]),
                shade=False
            )
            self.plotter.add_axes()
            if reset_view:
                self.reset_camera()
            elif saved_camera_position is not None:
                # Restore previous camera state when switching from other modes
                try:
                    self.plotter.camera.position = saved_camera_position
                    self.plotter.camera.focal_point = saved_camera_focal_point
                    self.plotter.camera.up = saved_camera_view_up
                    self.plotter.camera.view_angle = saved_camera_view_angle
                    self.plotter.render()
                except Exception:
                    pass
        else:
            # Fast update without clearing
            self.update_status("Updating volume properties...")
            params   = self._get_render_params()
            vol_grid = self._cached_vol_grid or self.grid.cell_data_to_point_data()

            old_actor = self.volume_actor
            new_actor = self.plotter.add_volume(
                vol_grid,
                cmap=params.get('colormap', 'bone'),
                opacity=params.get('opacity', 'sigmoid'),
                clim=params.get('clim', [0, 1000]),
                shade=False,
                render=False
            )

            if new_actor.mapper:
                new_actor.mapper.scalar_range = params.get('clim', [0, 1000])

            if old_actor:
                self.plotter.remove_actor(old_actor, render=False)

            self.volume_actor = new_actor
            self.plotter.render()

    def render_slices(self, reset_view=True):
        """Render orthogonal slices."""
        if not self.grid:
            return

        # Save camera state before potential view change
        saved_camera_position = None
        saved_camera_focal_point = None
        saved_camera_view_up = None
        saved_camera_view_angle = None
        
        if not reset_view and self.active_view_mode in ['volume', 'mesh', 'slices', 'iso']:
            try:
                # Save detailed camera state
                saved_camera_position = self.plotter.camera.position
                saved_camera_focal_point = self.plotter.camera.focal_point
                saved_camera_view_up = self.plotter.camera.up
                saved_camera_view_angle = self.plotter.camera.view_angle
            except Exception:
                pass

        if reset_view or self.active_view_mode != 'slices':
            self.clear_view()
            self.active_view_mode = 'slices'
            self._update_clip_panel_state()
            if self.params_panel:
                self.params_panel.set_mode('slices')
            self.plotter.enable_lightkit()
            self.plotter.show_grid()

        params = self._get_render_params()
        ox, oy, oz = self.grid.origin
        dx, dy, dz = self.grid.spacing
        x = ox + params.get('slice_x', 0) * dx
        y = oy + params.get('slice_y', 0) * dy
        z = oz + params.get('slice_z', 0) * dz

        self.plotter.clear_actors()
        slices = self.grid.slice_orthogonal(x=x, y=y, z=z)
        self.plotter.add_mesh(
            slices,
            cmap=params.get('colormap', 'bone'),
            clim=params.get('clim', [0, 1000]),
            show_scalar_bar=False
        )
        self.plotter.add_axes()
        if reset_view:
            self.reset_camera()
        elif saved_camera_position is not None:
            # Restore previous camera state when switching from other modes
            try:
                self.plotter.camera.position = saved_camera_position
                self.plotter.camera.focal_point = saved_camera_focal_point
                self.plotter.camera.up = saved_camera_view_up
                self.plotter.camera.view_angle = saved_camera_view_angle
                self.plotter.render()
            except Exception:
                pass

    def render_isosurface(self, threshold=300, reset_view=True):
        """Render isosurface at specified threshold using optimized VTK algorithm."""
        if not self.grid:
            return

        # Save camera state before potential view change
        saved_camera_position = None
        saved_camera_focal_point = None
        saved_camera_view_up = None
        saved_camera_view_angle = None
        
        if not reset_view and self.active_view_mode in ['volume', 'mesh', 'slices', 'iso']:
            try:
                # Save detailed camera state
                saved_camera_position = self.plotter.camera.position
                saved_camera_focal_point = self.plotter.camera.focal_point
                saved_camera_view_up = self.plotter.camera.up
                saved_camera_view_angle = self.plotter.camera.view_angle
            except Exception:
                pass

        self.update_status(f"Generating isosurface ({threshold})...")
        self.clear_view()
        self.active_view_mode = 'iso'
        self._update_clip_panel_state()
        if self.params_panel:
            self.params_panel.set_mode('iso')
        self.plotter.enable_lightkit()
        params = self._get_render_params()
        mesh_kwargs = self.get_iso_mesh_kwargs(params=params, sync_mode=True)

        try:
            # Check cache
            if threshold in self._iso_cache:
                contours = self._iso_cache[threshold]
                self.update_status("Using cached isosurface")
            else:
                import time
                start = time.time()
                
                # Use potentially downsampled grid for large volumes
                safe_grid = self._get_grid_for_isosurface()
                if safe_grid is None:
                    self.update_status("No grid available for isosurface")
                    return
                
                grid_points = safe_grid.cell_data_to_point_data()
                
                # Use VTK Flying Edges algorithm (faster than marching cubes)
                # flying_edges is 2-10x faster and is the default in VTK 9.0+
                try:
                    # Flying Edges via VTK directly
                    import vtk
                    contour_filter = vtk.vtkFlyingEdges3D()
                    contour_filter.SetInputData(grid_points)
                    contour_filter.SetValue(0, threshold)
                    contour_filter.ComputeNormalsOn()
                    contour_filter.Update()
                    contours = pv.wrap(contour_filter.GetOutput())
                    elapsed = time.time() - start
                    print(f"[Render] Isosurface (FlyingEdges): {elapsed:.2f}s, {contours.n_points} points")
                except Exception as e:
                    # Fallback to PyVista contour
                    print(f"[Render] FlyingEdges failed ({e}), using PyVista contour")
                    contours = grid_points.contour(isosurfaces=[threshold])
                    contours.compute_normals(inplace=True)
                    elapsed = time.time() - start
                    print(f"[Render] Isosurface (contour): {elapsed:.2f}s, {contours.n_points} points")
                
                self._iso_cache[threshold] = contours

            mode = params.get('coloring_mode', 'Solid Color')
            actor = None
            if mode == 'Solid Color':
                actor = self.plotter.add_mesh(contours, color=params.get('solid_color', 'ivory'), **mesh_kwargs)
            elif mode == 'Depth (Z-Axis)':
                contours["Elevation"] = contours.points[:, 2]
                actor = self.plotter.add_mesh(contours, scalars="Elevation", cmap=params.get('colormap', 'viridis'), **mesh_kwargs)
            elif mode == 'Radial (Center Dist)':
                dist = np.linalg.norm(contours.points - contours.center, axis=1)
                contours["RadialDistance"] = dist
                actor = self.plotter.add_mesh(contours, scalars="RadialDistance", cmap=params.get('colormap', 'viridis'), **mesh_kwargs)

            self.iso_actor = actor
            self._current_iso_threshold = threshold

            self._apply_custom_lighting(params)
            self.plotter.add_axes()
            if reset_view:
                self.reset_camera()
            elif saved_camera_position is not None:
                # Restore previous camera state when switching from other modes
                try:
                    self.plotter.camera.position = saved_camera_position
                    self.plotter.camera.focal_point = saved_camera_focal_point
                    self.plotter.camera.up = saved_camera_view_up
                    self.plotter.camera.view_angle = saved_camera_view_angle
                    self.plotter.render()
                except Exception:
                    pass
        except Exception as e:
            print(f"Isosurface error: {e}")
            self.iso_actor = None

    def render_isosurface_auto(self, reset_view=True):
        """Render isosurface with current threshold parameter."""
        params = self._get_render_params()
        self.render_isosurface(threshold=params.get('threshold', 300), reset_view=reset_view)

    def _apply_custom_lighting(self, params):
        """Apply custom lighting configuration based on parameters."""
        if 'light_angle' in params and params['light_angle'] is not None:
            import math
            angle = params['light_angle']
            rad = math.radians(angle)
            light_pos = [10 * math.cos(rad), 10 * math.sin(rad), 10]
            self.plotter.remove_all_lights()
            self.plotter.add_light(pv.Light(position=light_pos, intensity=1.0))


--- FILE: .\rendering\roi_extractor.py ---
"""
ROI shape extraction algorithms for volumetric data.
Provides box, ellipsoid, and cylinder extraction from 3D arrays.
"""

from typing import Optional, Tuple
import numpy as np

from core import VolumeData


def bounds_to_voxel_indices(
    bounds: tuple, 
    raw_shape: tuple, 
    spacing: tuple, 
    origin: tuple
) -> Tuple[int, int, int, int, int, int]:
    """
    Convert world coordinate bounds to voxel array indices.
    
    Args:
        bounds: World coordinate bounds (x_min, x_max, y_min, y_max, z_min, z_max)
        raw_shape: Shape of the raw data array (z, y, x)
        spacing: Voxel spacing (x, y, z)
        origin: World coordinate origin (x, y, z)
        
    Returns:
        Tuple of (i_start, i_end, j_start, j_end, k_start, k_end)
    """
    i_start = max(0, int((bounds[0] - origin[0]) / spacing[0]))
    i_end = min(raw_shape[0], int((bounds[1] - origin[0]) / spacing[0]))
    j_start = max(0, int((bounds[2] - origin[1]) / spacing[1]))
    j_end = min(raw_shape[1], int((bounds[3] - origin[1]) / spacing[1]))
    k_start = max(0, int((bounds[4] - origin[2]) / spacing[2]))
    k_end = min(raw_shape[2], int((bounds[5] - origin[2]) / spacing[2]))
    
    return i_start, i_end, j_start, j_end, k_start, k_end


def _calculate_new_origin(
    origin: tuple, 
    spacing: tuple, 
    i_start: int, 
    j_start: int, 
    k_start: int
) -> Tuple[float, float, float]:
    """Calculate origin for extracted sub-volume."""
    return (
        origin[0] + i_start * spacing[0],
        origin[1] + j_start * spacing[1],
        origin[2] + k_start * spacing[2]
    )


def _build_result(
    sub_data: np.ndarray,
    spacing: tuple,
    new_origin: tuple,
    base_metadata: dict,
    type_label: str,
    bounds: tuple,
    extra_metadata: dict = None
) -> Optional[VolumeData]:
    """Build VolumeData result with metadata."""
    if sub_data.size == 0:
        return None
    
    metadata = dict(base_metadata)
    metadata['Type'] = type_label
    metadata['ROI_Bounds'] = bounds
    if extra_metadata:
        metadata.update(extra_metadata)
    
    return VolumeData(raw_data=sub_data, spacing=spacing, origin=new_origin, metadata=metadata)


def extract_box(
    data: VolumeData, 
    bounds: tuple
) -> Optional[VolumeData]:
    """
    Extract box-shaped sub-volume from data.
    
    Args:
        data: Input VolumeData
        bounds: World coordinate bounds
        
    Returns:
        Extracted VolumeData with new origin, or None if extraction fails
    """
    if data is None or data.raw_data is None:
        return None

    raw = data.raw_data
    spacing = data.spacing
    origin = data.origin
    
    i_start, i_end, j_start, j_end, k_start, k_end = bounds_to_voxel_indices(
        bounds, raw.shape, spacing, origin
    )
    sub_data = raw[i_start:i_end, j_start:j_end, k_start:k_end].copy()
    
    if sub_data.size == 0:
        return None

    new_origin = _calculate_new_origin(origin, spacing, i_start, j_start, k_start)
    return _build_result(
        sub_data, spacing, new_origin, data.metadata,
        f"ROI Box ({sub_data.shape})", bounds
    )


def extract_ellipsoid(
    data: VolumeData, 
    bounds: tuple
) -> Optional[VolumeData]:
    """
    Extract ellipsoid sub-volume inscribed in box bounds.
    
    Areas outside the ellipsoid are set to the minimum value.
    
    Args:
        data: Input VolumeData
        bounds: World coordinate bounds
        
    Returns:
        Extracted VolumeData with ellipsoid mask applied
    """
    if data is None or data.raw_data is None:
        return None

    raw = data.raw_data
    spacing = data.spacing
    origin = data.origin
    
    i_start, i_end, j_start, j_end, k_start, k_end = bounds_to_voxel_indices(
        bounds, raw.shape, spacing, origin
    )
    
    # Calculate center and radii
    rx = (bounds[1] - bounds[0]) / 2
    ry = (bounds[3] - bounds[2]) / 2
    rz = (bounds[5] - bounds[4]) / 2
    
    ci = ((bounds[0] + bounds[1]) / 2 - origin[0]) / spacing[0]
    cj = ((bounds[2] + bounds[3]) / 2 - origin[1]) / spacing[1]
    ck = ((bounds[4] + bounds[5]) / 2 - origin[2]) / spacing[2]
    
    # Create ellipsoid mask
    ii, jj, kk = np.meshgrid(
        np.arange(i_start, i_end),
        np.arange(j_start, j_end),
        np.arange(k_start, k_end),
        indexing='ij'
    )
    
    dist = np.sqrt(
        ((ii - ci) * spacing[0] / rx) ** 2 +
        ((jj - cj) * spacing[1] / ry) ** 2 +
        ((kk - ck) * spacing[2] / rz) ** 2
    )
    mask = dist <= 1.0
    
    sub_data = raw[i_start:i_end, j_start:j_end, k_start:k_end].copy()
    sub_data[~mask] = sub_data.min()
    
    if sub_data.size == 0:
        return None

    new_origin = _calculate_new_origin(origin, spacing, i_start, j_start, k_start)
    return _build_result(
        sub_data, spacing, new_origin, data.metadata,
        f"ROI Ellipsoid ({sub_data.shape})", bounds,
        {'ROI_Radii': (rx, ry, rz)}
    )


def extract_cylinder(
    data: VolumeData, 
    bounds: tuple,
    current_size: Optional[Tuple[float, float, float]] = None,
    transform: Optional[np.ndarray] = None
) -> Optional[VolumeData]:
    """
    Extract elliptical cylinder inscribed in box bounds.
    
    Supports rotation via transform matrix.
    
    Args:
        data: Input VolumeData
        bounds: World coordinate bounds (AABB)
        current_size: Pre-rotation size (x, y, z) if available
        transform: 4x4 transformation matrix containing rotation
        
    Returns:
        Extracted VolumeData with cylinder mask applied
    """
    if data is None or data.raw_data is None:
        return None

    raw = data.raw_data
    spacing = data.spacing
    origin = data.origin
    
    i_start, i_end, j_start, j_end, k_start, k_end = bounds_to_voxel_indices(
        bounds, raw.shape, spacing, origin
    )
    
    # Use stored size if available (rotation-invariant)
    if current_size:
        size_x, size_y, size_z = current_size
    else:
        size_x = bounds[1] - bounds[0]
        size_y = bounds[3] - bounds[2]
        size_z = bounds[5] - bounds[4]
    
    # Radii for elliptical cross-section
    rx = size_x / 2  # Cylinder length axis
    ry = size_y / 2
    rz = size_z / 2
    
    # Center in world coordinates (from AABB)
    center = np.array([
        (bounds[0] + bounds[1]) / 2,
        (bounds[2] + bounds[3]) / 2,
        (bounds[4] + bounds[5]) / 2
    ])
    
    # Create voxel coordinate grids
    ii, jj, kk = np.meshgrid(
        np.arange(i_start, i_end),
        np.arange(j_start, j_end),
        np.arange(k_start, k_end),
        indexing='ij'
    )
    
    # Convert to world coordinates
    world_x = origin[0] + ii * spacing[0]
    world_y = origin[1] + jj * spacing[1]
    world_z = origin[2] + kk * spacing[2]
    
    # Get relative position to center
    rel_x = world_x - center[0]
    rel_y = world_y - center[1]
    rel_z = world_z - center[2]
    
    # Apply inverse rotation if transform exists
    if transform is not None:
        # Extract rotation matrix and normalize (remove scale)
        rotation = transform[:3, :3].copy()
        for i in range(3):
            col_norm = np.linalg.norm(rotation[:, i])
            if col_norm > 1e-6:
                rotation[:, i] /= col_norm
        
        # Inverse rotation = transpose for orthogonal matrix
        inv_rotation = rotation.T
        
        # Transform relative coordinates to local space
        local_x = inv_rotation[0, 0] * rel_x + inv_rotation[0, 1] * rel_y + inv_rotation[0, 2] * rel_z
        local_y = inv_rotation[1, 0] * rel_x + inv_rotation[1, 1] * rel_y + inv_rotation[1, 2] * rel_z
        local_z = inv_rotation[2, 0] * rel_x + inv_rotation[2, 1] * rel_y + inv_rotation[2, 2] * rel_z
    else:
        local_x = rel_x
        local_y = rel_y
        local_z = rel_z
    
    # Cylinder mask: inside if (y/ry)^2 + (z/rz)^2 <= 1 AND abs(x) <= rx
    dist_2d = np.sqrt((local_y / ry) ** 2 + (local_z / rz) ** 2)
    mask = (dist_2d <= 1.0) & (np.abs(local_x) <= rx)
    
    sub_data = raw[i_start:i_end, j_start:j_end, k_start:k_end].copy()
    sub_data[~mask] = sub_data.min()
    
    if sub_data.size == 0:
        return None

    new_origin = _calculate_new_origin(origin, spacing, i_start, j_start, k_start)
    extra = {'ROI_Radii_YZ': (ry, rz)}
    if transform is not None:
        extra['ROI_Rotated'] = True
    
    return _build_result(
        sub_data, spacing, new_origin, data.metadata,
        f"ROI Elliptical Cylinder ({sub_data.shape})", bounds, extra
    )



--- FILE: .\rendering\roi_handler.py ---
"""
ROI (Region of Interest) handler for volume visualization.
Handles all ROI selection, preview rendering, and extraction logic.
"""

from typing import Optional, Callable, Tuple
import numpy as np
import pyvista as pv
from core import VolumeData
from rendering.roi_extractor import extract_box, extract_ellipsoid, extract_cylinder


class ROIHandler:
    """
    Handles ROI selection, preview rendering, and extraction logic.
    Designed for composition with GUI/rendering classes.
    """

    # Shape color mapping
    SHAPE_COLORS = {'box': 'cyan', 'sphere': 'magenta', 'cylinder': 'yellow'}

    def __init__(self, plotter, roi_panel, data_manager=None, status_callback=None):
        """
        Initialize ROI handler.
        
        Args:
            plotter: BackgroundPlotter instance
            roi_panel: ROIPanel for UI controls
            data_manager: Optional DataManager for data flow
            status_callback: Optional callback for status updates
        """
        self.plotter = plotter
        self.roi_panel = roi_panel
        self._data_manager = data_manager
        self._status_callback = status_callback
        
        # Data references (set externally)
        self.data: Optional[VolumeData] = None
        self.grid = None
        
        # Widget state
        self._box_widget = None
        self._transform: Optional[np.ndarray] = None
        self._base_size: Optional[Tuple[float, float, float]] = None
        self._current_size: Optional[Tuple[float, float, float]] = None
        
        # Preview rendering state
        self._shape_preview = None
        self._preview_renderer = None

    def update_status(self, message: str):
        """Update status via callback."""
        if self._status_callback:
            self._status_callback(message)

    def set_data(self, data: VolumeData, grid):
        """Set data reference for ROI extraction."""
        self.data = data
        self.grid = grid

    # ==========================================
    # Widget Management
    # ==========================================

    def on_shape_changed(self, shape: str):
        """Handle shape change - refresh widget with new color and update preview."""
        if not self.roi_panel.enable_checkbox.isChecked():
            self.update_status(f"ROI shape set to: {shape}")
            return
        
        # Preserve current bounds before clearing
        current_bounds = self.roi_panel.get_bounds()
        if current_bounds is None and self.grid is not None:
            current_bounds = self.grid.bounds
        
        self.clear_widgets()
        
        if current_bounds is None or self.grid is None:
            return
        
        # Re-create widget with new shape's color
        self._create_box_widget(current_bounds, shape)
        self._update_preview(current_bounds)
        
        status_map = {
            'box': "ROI Box: Drag to resize, rotate with edges",
            'sphere': "ROI Ellipsoid: Inscribed in box, inherits rotation",
            'cylinder': "ROI Cylinder: Inscribed in box, inherits rotation"
        }
        self.update_status(status_map.get(shape, f"ROI shape: {shape}"))
        self.plotter.render()

    def on_toggled(self, enabled: bool):
        """Handle ROI mode toggle."""
        if not enabled:
            self.clear_widgets()
            self.roi_panel.update_bounds(None)
            self.update_status("ROI mode disabled")
            return

        if self.grid is None:
            self.roi_panel.enable_checkbox.setChecked(False)
            return

        shape = self.roi_panel.get_shape()
        bounds = self.grid.bounds
        
        self._create_box_widget(bounds, shape)
        
        status_map = {
            'box': "ROI Box: Drag to resize, rotate with edges",
            'sphere': "ROI Ellipsoid: Drag box - ellipsoid fills box, can rotate",
            'cylinder': "ROI Cylinder: Drag box - elliptical cylinder, can rotate"
        }
        self.update_status(status_map.get(shape, "ROI: Drag to select region"))

    def _create_box_widget(self, bounds: tuple, shape: str):
        """Create box widget for ROI selection."""
        color = self.SHAPE_COLORS.get(shape, 'cyan')
        
        self.plotter.add_box_widget(
            callback=self._on_bounds_changed,
            bounds=bounds,
            factor=1.0,
            rotation_enabled=True,
            color=color,
            use_planes=False
        )
        
        # Store references
        if self.plotter.box_widgets:
            self._box_widget = self.plotter.box_widgets[-1]
        else:
            self._box_widget = None
        
        # Store initial size for rotation tracking
        initial_size = (
            bounds[1] - bounds[0],
            bounds[3] - bounds[2],
            bounds[5] - bounds[4]
        )
        self._base_size = initial_size
        self._current_size = initial_size

    def _on_bounds_changed(self, bounds):
        """Callback when box widget is moved/rotated/scaled."""
        if hasattr(bounds, 'bounds'):
            actual_bounds = bounds.bounds
        else:
            actual_bounds = bounds
        
        # Capture transform matrix
        self._transform = None
        if self._box_widget:
            try:
                import vtk
                transform = vtk.vtkTransform()
                self._box_widget.GetTransform(transform)
                
                matrix = transform.GetMatrix()
                self._transform = np.array([
                    [matrix.GetElement(i, j) for j in range(4)]
                    for i in range(4)
                ])
                
                # Update size based on scale in transform
                scale_x = np.linalg.norm(self._transform[:3, 0])
                scale_y = np.linalg.norm(self._transform[:3, 1])
                scale_z = np.linalg.norm(self._transform[:3, 2])
                
                if self._base_size:
                    self._current_size = (
                        self._base_size[0] * scale_x,
                        self._base_size[1] * scale_y,
                        self._base_size[2] * scale_z
                    )
            except Exception as e:
                print(f"[ROI] Transform extraction failed: {e}")
        
        self.roi_panel.update_bounds(actual_bounds)
        self._update_preview(actual_bounds)

    def clear_widgets(self):
        """Clear all ROI-related widgets and preview shapes."""
        self.plotter.clear_box_widgets()
        self.plotter.clear_sphere_widgets()
        self._box_widget = None
        
        # Remove preview
        if self._shape_preview:
            if self._preview_renderer:
                self._preview_renderer.RemoveActor(self._shape_preview)
            self.plotter.remove_actor(self._shape_preview)
            self._shape_preview = None

    # ==========================================
    # Preview Rendering
    # ==========================================

    def _setup_preview_renderer(self):
        """Setup foreground renderer layer for ROI shape preview."""
        if self._preview_renderer:
            return self._preview_renderer
        
        import vtk
        
        self._preview_renderer = vtk.vtkRenderer()
        self._preview_renderer.SetLayer(1)
        self._preview_renderer.InteractiveOff()
        self._preview_renderer.SetBackground(0, 0, 0)
        self._preview_renderer.SetBackgroundAlpha(0.0)
        
        render_window = self.plotter.ren_win
        render_window.SetNumberOfLayers(2)
        render_window.AddRenderer(self._preview_renderer)
        
        main_camera = self.plotter.renderer.GetActiveCamera()
        self._preview_renderer.SetActiveCamera(main_camera)
        
        return self._preview_renderer

    def _update_preview(self, bounds):
        """Update inscribed shape preview with box widget rotation."""
        if bounds is None:
            return
        
        shape = self.roi_panel.get_shape()
        
        # Remove old preview
        if self._shape_preview:
            if self._preview_renderer:
                self._preview_renderer.RemoveActor(self._shape_preview)
            self.plotter.remove_actor(self._shape_preview)
            self._shape_preview = None
        
        if shape == 'box':
            return
        
        # Use stored size (not AABB which expands on rotation)
        if self._current_size:
            size_x, size_y, size_z = self._current_size
        else:
            size_x = bounds[1] - bounds[0]
            size_y = bounds[3] - bounds[2]
            size_z = bounds[5] - bounds[4]
        
        # Create mesh at origin
        if shape == 'sphere':
            mesh = pv.ParametricEllipsoid(size_x / 2, size_y / 2, size_z / 2)
            preview_color = 'magenta'
        elif shape == 'cylinder':
            mesh = self._create_cylinder_mesh(size_x, size_y, size_z)
            preview_color = 'yellow'
        else:
            return
        
        # Apply pure rotation + translation
        # Calculate center from AABB bounds (this is always the visual center)
        center = (
            (bounds[0] + bounds[1]) / 2,
            (bounds[2] + bounds[3]) / 2,
            (bounds[4] + bounds[5]) / 2
        )
        
        if self._transform is not None:
            # Extract pure rotation (normalize columns to remove scale)
            rotation = self._transform[:3, :3].copy()
            for i in range(3):
                col_norm = np.linalg.norm(rotation[:, i])
                if col_norm > 1e-6:
                    rotation[:, i] /= col_norm
            
            # Apply rotation first, then translate to AABB center
            pure_transform = np.eye(4)
            pure_transform[:3, :3] = rotation
            mesh.transform(pure_transform, inplace=True)
            mesh.translate(center, inplace=True)
        else:
            mesh.translate(center, inplace=True)
        
        # Add preview mesh
        self._shape_preview = self.plotter.add_mesh(
            mesh,
            color=preview_color,
            style='wireframe',
            line_width=2,
            opacity=1.0,
            name='roi_shape_preview'
        )
        
        # Move to foreground layer
        if self._shape_preview:
            renderer = self._setup_preview_renderer()
            if self.plotter.renderer.HasViewProp(self._shape_preview):
                self.plotter.renderer.RemoveActor(self._shape_preview)
            renderer.AddActor(self._shape_preview)

    def _create_cylinder_mesh(self, size_x: float, size_y: float, size_z: float):
        """Create elliptical cylinder mesh centered at origin."""
        ry, rz = size_y / 2, size_z / 2
        n_pts = 32
        angles = np.linspace(0, 2 * np.pi, n_pts, endpoint=False)
        
        points_bottom = np.column_stack([
            np.full(n_pts, -size_x / 2),
            ry * np.cos(angles),
            rz * np.sin(angles)
        ])
        points_top = np.column_stack([
            np.full(n_pts, size_x / 2),
            ry * np.cos(angles),
            rz * np.sin(angles)
        ])
        
        all_points = np.vstack([points_bottom, points_top])
        
        faces = []
        for i in range(n_pts):
            next_i = (i + 1) % n_pts
            faces.append([3, i, next_i, n_pts + i])
            faces.append([3, next_i, n_pts + next_i, n_pts + i])
        faces_flat = np.hstack([[item for f in faces for item in f]])
        
        return pv.PolyData(all_points, faces_flat)

    # ==========================================
    # ROI Application
    # ==========================================

    def on_apply(self, set_data_callback: Callable[[VolumeData], None]):
        """Apply ROI extraction and update data."""
        if self.data is None:
            return

        shape = self.roi_panel.get_shape()
        roi_bounds = self.roi_panel.get_bounds()
        
        if roi_bounds is None:
            return
        
        try:
            if shape == 'box':
                extracted = self._extract_box(roi_bounds)
            elif shape == 'sphere':
                extracted = self._extract_ellipsoid(roi_bounds)
            elif shape == 'cylinder':
                extracted = self._extract_cylinder(roi_bounds)
            else:
                extracted = None
            
            if extracted is not None:
                if self._data_manager:
                    self._data_manager.set_roi_data(extracted)
                set_data_callback(extracted)
                self.update_status(f"ROI applied ({shape}): {extracted.raw_data.shape}")
                self.roi_panel.enable_checkbox.setChecked(False)
                self.clear_widgets()
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.update_status(f"ROI extraction failed: {e}")

    def on_reset(self, set_data_callback: Callable[[VolumeData], None]):
        """Reset ROI and restore original data."""
        self.clear_widgets()
        if self._data_manager:
            self._data_manager.clear_roi()
            if self._data_manager.raw_ct_data:
                set_data_callback(self._data_manager.raw_ct_data)
        self.update_status("ROI reset")

    # ==========================================
    # Extraction Methods
    # ==========================================

    def _bounds_to_voxel_indices(self, bounds: tuple) -> tuple:
        """Convert world bounds to voxel indices."""
        raw = self.data.raw_data
        spacing = self.data.spacing
        origin = self.data.origin
        
        i_start = max(0, int((bounds[0] - origin[0]) / spacing[0]))
        i_end = min(raw.shape[0], int((bounds[1] - origin[0]) / spacing[0]))
        j_start = max(0, int((bounds[2] - origin[1]) / spacing[1]))
        j_end = min(raw.shape[1], int((bounds[3] - origin[1]) / spacing[1]))
        k_start = max(0, int((bounds[4] - origin[2]) / spacing[2]))
        k_end = min(raw.shape[2], int((bounds[5] - origin[2]) / spacing[2]))
        
        return i_start, i_end, j_start, j_end, k_start, k_end

    def _extract_box(self, bounds: tuple) -> Optional[VolumeData]:
        """Extract box-shaped sub-volume."""
        return extract_box(self.data, bounds)

    def _extract_ellipsoid(self, bounds: tuple) -> Optional[VolumeData]:
        """Extract ellipsoid sub-volume inscribed in box bounds."""
        return extract_ellipsoid(self.data, bounds)

    def _extract_cylinder(self, bounds: tuple) -> Optional[VolumeData]:
        """Extract elliptical cylinder inscribed in box bounds (supports rotation)."""
        return extract_cylinder(self.data, bounds, self._current_size, self._transform)


--- FILE: .\rendering\__init__.py ---
"""
Rendering package for modular visualization logic.
"""

from rendering.render_engine import RenderEngine
from rendering.clip_handler import ClipHandler
from rendering.roi_handler import ROIHandler
from rendering.lod_manager import LODPyramid, LODRenderManager, check_gpu_volume_rendering

__all__ = [
    'RenderEngine',
    'ClipHandler',
    'ROIHandler',
    'LODPyramid',
    'LODRenderManager',
    'check_gpu_volume_rendering'
]


--- FILE: .\tests\benchmark_watershed.py ---

import numpy as np
import time
from processors.utils import watershed_gpu
from skimage.segmentation import watershed as cpu_watershed
from scipy.ndimage import distance_transform_edt

def test_watershed():
    print("Creating synthetic data...")
    shape = (128, 128, 128)
    # Create two blobs
    z, y, x = np.ogrid[:shape[0], :shape[1], :shape[2]]
    center1 = (40, 64, 64)
    center2 = (88, 64, 64)
    mask1 = ((z-center1[0])**2 + (y-center1[1])**2 + (x-center1[2])**2) < 30**2
    mask2 = ((z-center2[0])**2 + (y-center2[1])**2 + (x-center2[2])**2) < 30**2
    image = np.zeros(shape, dtype=np.float32)
    image[mask1] = 1.0
    image[mask2] = 1.0
    
    # Distance transform (inverse for watershed)
    dist = distance_transform_edt(image).astype(np.float32)
    image_input = -dist
    
    # Markers
    markers = np.zeros(shape, dtype=np.int32)
    markers[center1] = 1
    markers[center2] = 2
    
    print("Running CPU watershed...")
    t0 = time.time()
    res_cpu = cpu_watershed(image_input, markers, mask=image>0)
    print(f"CPU Time: {time.time()-t0:.4f}s")
    
    print("Running GPU watershed...")
    try:
        t0 = time.time()
        res_gpu = watershed_gpu(image_input, markers, mask=image>0)
        print(f"GPU Time: {time.time()-t0:.4f}s")
        
        # Verify
        match = np.sum(res_cpu == res_gpu) / np.prod(shape)
        print(f"Match Similarity: {match*100:.2f}%")
        
        if match > 0.99:
            print("SUCCESS: GPU result matches CPU.")
        else:
            print("WARNING: Low similarity.")
            
    except Exception as e:
        print(f"GPU Failed: {e}")

if __name__ == "__main__":
    test_watershed()


--- FILE: .\tests\debug_loader.py ---
"""
Debug script for DICOM loader testing.
Run this to diagnose loading issues.
"""
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from loaders.dicom import DicomSeriesLoader, FastDicomLoader

def debug_load(folder_path: str):
    """Test DICOM loading with debug output."""
    print("=" * 60)
    print(f"Testing folder: {folder_path}")
    print("=" * 60)
    
    # Test with standard loader
    print("\n[TEST 1] DicomSeriesLoader (default, use_header_sort=True)")
    try:
        loader = DicomSeriesLoader()
        data = loader.load(folder_path)
        print(f"  Shape: {data.raw_data.shape}")
        print(f"  Spacing: {data.spacing}")
        print(f"  Value range: {np.nanmin(data.raw_data):.1f} to {np.nanmax(data.raw_data):.1f}")
        print(f"  Metadata: {data.metadata}")
        
        # Check if data has variation (not just a solid block)
        std = np.nanstd(data.raw_data)
        print(f"  Std deviation: {std:.2f}")
        if std < 1.0:
            print("  鈿狅笍 WARNING: Very low variation - might be solid block!")
        else:
            print("  鉁?Data has good variation")
            
    except Exception as e:
        print(f"  鉂?Error: {e}")
    
    # Test with fast loader for comparison
    print("\n[TEST 2] FastDicomLoader (step=2)")
    try:
        loader = FastDicomLoader(step=2)
        data = loader.load(folder_path)
        print(f"  Shape: {data.raw_data.shape}")
        print(f"  Value range: {np.nanmin(data.raw_data):.1f} to {np.nanmax(data.raw_data):.1f}")
        
        std = np.nanstd(data.raw_data)
        print(f"  Std deviation: {std:.2f}")
        if std < 1.0:
            print("  鈿狅笍 WARNING: Very low variation - might be solid block!")
        else:
            print("  鉁?Data has good variation")
            
    except Exception as e:
        print(f"  鉂?Error: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python debug_loader.py <path_to_dicom_folder>")
        sys.exit(1)
    
    folder = sys.argv[1]
    debug_load(folder)


--- FILE: .\tests\test_core.py ---
import unittest
import numpy as np
from core import VolumeData

class TestVolumeData(unittest.TestCase):
    def test_initialization(self):
        data = np.zeros((10, 10, 10))
        vol = VolumeData(raw_data=data, spacing=(1.0, 1.0, 1.0))
        self.assertIsNotNone(vol.raw_data)
        self.assertEqual(vol.dimensions, (10, 10, 10))
        self.assertFalse(vol.has_mesh)

    def test_metadata(self):
        vol = VolumeData(metadata={"Type": "Test"})
        self.assertEqual(vol.metadata["Type"], "Test")
        self.assertEqual(vol.dimensions, (0, 0, 0))

if __name__ == '__main__':
    unittest.main()


--- FILE: .\tests\test_gpu_backend.py ---
"""
Regression & performance tests for the optimised GPU pipeline.

Covers
------
* watershed_kernel  鈥?shared-memory / hierarchical-reduction version
* _nms_gpu_parallel 鈥?spatial-hash O(N) NMS

Run
---
    cd "d:/Projects/6.Food CT/Porous"
    python -m pytest tests/test_gpu_backend.py -v

Or standalone:
    python tests/test_gpu_backend.py
"""

import sys
import time
import numpy as np
import scipy.ndimage as ndimage
from scipy.spatial import cKDTree

# ---------------------------------------------------------------------------
# Minimal bootstrap so tests run from any cwd
# ---------------------------------------------------------------------------
import os
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

# ---------------------------------------------------------------------------
# GPU availability guard
# ---------------------------------------------------------------------------
try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False


# ===========================================================================
# Helpers
# ===========================================================================

def _make_two_blob_volume(shape=(64, 64, 64)):
    """Create a synthetic 3-D volume with two spherical blobs."""
    d, h, w = shape
    z, y, x = np.ogrid[:d, :h, :w]
    c1 = (d // 4,     h // 2, w // 2)
    c2 = (3 * d // 4, h // 2, w // 2)
    r  = min(d, h, w) // 5
    mask = (
        ((z - c1[0])**2 + (y - c1[1])**2 + (x - c1[2])**2 < r**2) |
        ((z - c2[0])**2 + (y - c2[1])**2 + (x - c2[2])**2 < r**2)
    )
    image = np.zeros(shape, dtype=np.float32)
    image[mask] = 1.0
    dist   = ndimage.distance_transform_edt(image).astype(np.float32)
    markers = np.zeros(shape, dtype=np.int32)
    markers[c1] = 1
    markers[c2] = 2
    return -dist, markers, mask.astype(bool)


def _make_random_peaks(n=200, shape=(64, 64, 64), min_sep=5, seed=42):
    """Create N random 3-D points with random float intensities."""
    rng = np.random.default_rng(seed)
    pts = rng.integers(0, [s - 1 for s in shape], size=(n, 3))
    intensities = rng.random(n).astype(np.float32)
    return pts, intensities


def _kdtree_nms_reference(pts, intensities, min_distance):
    """Pure-CPU NMS via cKDTree 鈥?used as ground truth."""
    n = len(pts)
    order     = np.argsort(-intensities)
    pts_s     = pts[order]
    intens_s  = intensities[order]
    tree      = cKDTree(pts_s)
    suppressed = np.zeros(n, dtype=bool)
    selected   = []
    for i in range(n):
        if suppressed[i]:
            continue
        selected.append(pts_s[i])
        for j in tree.query_ball_point(pts_s[i], r=min_distance, p=2):
            if j > i:
                suppressed[j] = True
    return np.array(selected) if selected else np.empty((0, 3), dtype=pts.dtype)


# ===========================================================================
# Assertion 1 鈥?Watershed label correctness
# ===========================================================================

class TestWatershedCorrectness:
    """GPU watershed must reproduce CPU watershed labels (>99 % agreement)."""

    def test_small_volume_label_match(self):
        from processors.utils import watershed_gpu
        from skimage.segmentation import watershed as cpu_watershed

        image, markers, mask = _make_two_blob_volume(shape=(48, 48, 48))

        labels_cpu = cpu_watershed(image, markers, mask=mask)
        labels_gpu = watershed_gpu(image, markers, mask=mask)

        total   = int(mask.sum())
        match   = int(np.sum((labels_cpu == labels_gpu) & mask))
        pct     = match / total if total > 0 else 1.0
        print(f"\n  Watershed label match: {pct*100:.2f}% ({match}/{total})")

        assert pct >= 0.99, (
            f"GPU watershed accuracy {pct*100:.2f}% < 99 % 鈥?"
            "shared-memory kernel may have off-by-one or indexing bug"
        )

    def test_label_values_bounded(self):
        """No label value should exceed the number of seeds."""
        from processors.utils import watershed_gpu

        image, markers, mask = _make_two_blob_volume(shape=(32, 32, 32))
        labels = watershed_gpu(image, markers, mask=mask)
        n_seeds = int(markers.max())
        assert int(labels.max()) <= n_seeds, "Unexpected label > n_seeds"
        assert int(labels.min()) >= 0, "Negative label in output"

    @staticmethod
    def run():
        t = TestWatershedCorrectness()
        t.test_small_volume_label_match()
        t.test_label_values_bounded()
        print("  [PASS] Watershed correctness")


# ===========================================================================
# Assertion 2 鈥?NMS output equality with cKDTree reference
# ===========================================================================

class TestNmsCorrectness:
    """_nms_gpu_parallel peaks must match cKDTree NMS (set equality)."""

    def _pts_to_set(self, pts):
        return {tuple(p) for p in pts.tolist()}

    def test_nms_matches_kdtree(self):
        if not GPU_AVAILABLE:
            print("  [SKIP] CuPy not available 鈥?skipping GPU NMS test")
            return

        from processors.utils import _nms_gpu_parallel

        pts, intensities = _make_random_peaks(n=300, min_sep=5)
        min_dist = 5

        # ---- GPU result ----
        pts_gpu    = cp.asarray(pts.astype(np.int32))
        intens_gpu = cp.asarray(intensities)
        sort_idx   = cp.argsort(-intens_gpu)
        keep_gpu   = _nms_gpu_parallel(pts_gpu[sort_idx], intens_gpu[sort_idx], min_dist)
        sel_gpu    = cp.asnumpy(pts_gpu[sort_idx][keep_gpu])

        # ---- CPU reference ----
        sel_cpu    = _kdtree_nms_reference(pts, intensities, min_dist)

        gpu_set = self._pts_to_set(sel_gpu)
        cpu_set = self._pts_to_set(sel_cpu)

        # Points in CPU result but not GPU (false negatives)
        missed    = cpu_set - gpu_set
        # Points in GPU result but not CPU (false positives 鈥?should not happen)
        extra     = gpu_set - cpu_set

        print(
            f"\n  NMS 鈥?CPU kept: {len(cpu_set)}, GPU kept: {len(gpu_set)}, "
            f"missed: {len(missed)}, extra: {len(extra)}"
        )

        assert len(extra)  == 0, f"GPU NMS kept {len(extra)} extra points not in CPU reference"
        assert len(missed) == 0, f"GPU NMS missed {len(missed)} points present in CPU reference"

    def test_nms_empty_input(self):
        if not GPU_AVAILABLE:
            return
        from processors.utils import _nms_gpu_parallel
        empty_pts    = cp.empty((0, 3), dtype=cp.int32)
        empty_intens = cp.empty((0,),   dtype=cp.float32)
        keep         = _nms_gpu_parallel(empty_pts, empty_intens, min_distance=5)
        assert keep.shape == (0,), "Empty input should return empty keep array"

    def test_nms_single_point(self):
        if not GPU_AVAILABLE:
            return
        from processors.utils import _nms_gpu_parallel
        pts    = cp.array([[10, 20, 30]], dtype=cp.int32)
        intens = cp.array([0.9],         dtype=cp.float32)
        keep   = _nms_gpu_parallel(pts, intens, min_distance=5)
        assert bool(keep[0]) is True, "Single point must always be kept"

    @staticmethod
    def run():
        t = TestNmsCorrectness()
        t.test_nms_matches_kdtree()
        t.test_nms_empty_input()
        t.test_nms_single_point()
        print("  [PASS] NMS correctness")


# ===========================================================================
# Performance benchmark
# ===========================================================================

class BenchmarkGPUPipeline:
    """
    Throughput comparison on larger volumes.
    Prints timing; does NOT assert speed (hardware-dependent).
    """

    def benchmark_watershed(self, shape=(128, 128, 128)):
        from processors.utils import watershed_gpu
        from skimage.segmentation import watershed as cpu_watershed

        image, markers, mask = _make_two_blob_volume(shape=shape)

        t0 = time.perf_counter()
        cpu_watershed(image, markers, mask=mask)
        cpu_time = time.perf_counter() - t0

        t0 = time.perf_counter()
        watershed_gpu(image, markers, mask=mask)
        gpu_time = time.perf_counter() - t0

        speedup = cpu_time / gpu_time if gpu_time > 0 else float("inf")
        print(
            f"\n  Watershed {shape}  "
            f"CPU {cpu_time:.3f}s | GPU {gpu_time:.3f}s | "
            f"speedup {speedup:.1f}x"
        )
        return speedup

    def benchmark_nms(self, n_points=50_000, min_dist=5):
        if not GPU_AVAILABLE:
            print("  [SKIP] CuPy not available 鈥?skipping GPU NMS benchmark")
            return

        from processors.utils import _nms_gpu_parallel

        rng       = np.random.default_rng(0)
        pts       = rng.integers(0, 512, size=(n_points, 3)).astype(np.int32)
        intensities = rng.random(n_points).astype(np.float32)

        # CPU KDTree NMS
        t0 = time.perf_counter()
        _kdtree_nms_reference(pts, intensities, min_dist)
        cpu_time = time.perf_counter() - t0

        # GPU spatial-hash NMS
        pts_gpu    = cp.asarray(pts)
        intens_gpu = cp.asarray(intensities)
        sort_idx   = cp.argsort(-intens_gpu)
        cp.cuda.Stream.null.synchronize()  # warm up

        t0 = time.perf_counter()
        keep = _nms_gpu_parallel(pts_gpu[sort_idx], intens_gpu[sort_idx], min_dist)
        cp.cuda.Stream.null.synchronize()
        gpu_time = time.perf_counter() - t0

        speedup = cpu_time / gpu_time if gpu_time > 0 else float("inf")
        print(
            f"\n  NMS N={n_points} min_dist={min_dist}  "
            f"CPU {cpu_time:.3f}s | GPU {gpu_time:.3f}s | "
            f"speedup {speedup:.1f}x  "
            f"kept {int(keep.sum())}/{n_points}"
        )

    @staticmethod
    def run():
        b = BenchmarkGPUPipeline()
        b.benchmark_watershed(shape=(64,  64,  64))
        b.benchmark_watershed(shape=(128, 128, 128))
        b.benchmark_nms(n_points=10_000)
        b.benchmark_nms(n_points=50_000)
        print("  [PASS] Benchmark complete (see timings above)")


# ===========================================================================
# Pytest entry points (picked up by pytest auto-discovery)
# ===========================================================================

def test_watershed_label_match():
    TestWatershedCorrectness().test_small_volume_label_match()

def test_watershed_label_bounds():
    TestWatershedCorrectness().test_label_values_bounded()

def test_nms_matches_kdtree():
    TestNmsCorrectness().test_nms_matches_kdtree()

def test_nms_empty_input():
    TestNmsCorrectness().test_nms_empty_input()

def test_nms_single_point():
    TestNmsCorrectness().test_nms_single_point()


# ===========================================================================
# Standalone runner
# ===========================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("GPU Backend Regression & Benchmark Suite")
    print(f"CuPy available: {GPU_AVAILABLE}")
    print("=" * 60)

    print("\n--- Assertion 1: Watershed correctness ---")
    TestWatershedCorrectness.run()

    print("\n--- Assertion 2: NMS correctness ---")
    TestNmsCorrectness.run()

    print("\n--- Performance benchmarks ---")
    BenchmarkGPUPipeline.run()

    print("\n" + "=" * 60)
    print("All checks passed.")


--- FILE: .\tests\test_loaders.py ---
"""
Unit tests for DICOM loaders.
"""

import unittest
import os
import sys

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from loaders.dicom import _natural_sort_key


class TestNaturalSortKey(unittest.TestCase):
    """Test natural sorting function for filenames."""
    
    def test_numeric_sorting(self):
        """Test that numeric parts are sorted numerically, not lexicographically."""
        files = ['img_1.dcm', 'img_10.dcm', 'img_2.dcm', 'img_20.dcm', 'img_3.dcm']
        sorted_files = sorted(files, key=_natural_sort_key)
        expected = ['img_1.dcm', 'img_2.dcm', 'img_3.dcm', 'img_10.dcm', 'img_20.dcm']
        self.assertEqual(sorted_files, expected)
    
    def test_case_insensitive(self):
        """Test that sorting is case-insensitive."""
        files = ['IMG_1.dcm', 'img_2.dcm', 'Img_3.dcm']
        sorted_files = sorted(files, key=_natural_sort_key)
        expected = ['IMG_1.dcm', 'img_2.dcm', 'Img_3.dcm']
        self.assertEqual(sorted_files, expected)
    
    def test_mixed_format(self):
        """Test various filename formats."""
        files = ['slice001.dcm', 'slice010.dcm', 'slice002.dcm']
        sorted_files = sorted(files, key=_natural_sort_key)
        expected = ['slice001.dcm', 'slice002.dcm', 'slice010.dcm']
        self.assertEqual(sorted_files, expected)
    
    def test_pure_numbers(self):
        """Test files named with just numbers."""
        files = ['1', '10', '2', '20', '100']
        sorted_files = sorted(files, key=_natural_sort_key)
        expected = ['1', '2', '10', '20', '100']
        self.assertEqual(sorted_files, expected)


class TestLoaderImports(unittest.TestCase):
    """Test that loader classes can be imported correctly."""
    
    def test_import_loaders(self):
        """Test that all loader classes can be imported."""
        from loaders.dicom import DicomSeriesLoader, FastDicomLoader
        from loaders.dicom import MemoryMappedDicomLoader, ChunkedDicomLoader
        
        self.assertIsNotNone(DicomSeriesLoader)
        self.assertIsNotNone(FastDicomLoader)
        self.assertIsNotNone(MemoryMappedDicomLoader)
        self.assertIsNotNone(ChunkedDicomLoader)
    
    def test_loader_initialization(self):
        """Test that loaders can be instantiated with default args."""
        from loaders.dicom import DicomSeriesLoader, FastDicomLoader
        
        loader1 = DicomSeriesLoader()
        self.assertEqual(loader1.use_header_sort, True)  # Default is now True
        self.assertEqual(loader1.max_workers, 4)
        
        loader2 = DicomSeriesLoader(use_header_sort=False, max_workers=8)
        self.assertEqual(loader2.use_header_sort, False)
        self.assertEqual(loader2.max_workers, 8)
        
        fast_loader = FastDicomLoader(step=4)
        self.assertEqual(fast_loader.step, 4)


if __name__ == '__main__':
    unittest.main()


--- FILE: .\tests\test_processors.py ---
import unittest
import numpy as np
from core import VolumeData
from processors import PoreExtractionProcessor, PoreToSphereProcessor

class TestProcessors(unittest.TestCase):
    def setUp(self):
        # Create a 50x50x50 volume with a central "pore" (cube of 0s in a field of 1s)
        self.size = 50
        self.data = np.ones((self.size, self.size, self.size), dtype=np.float32) * 1000 # Solid
        
        # Create a 10x10x10 void in the center
        center = self.size // 2
        start, end = center - 5, center + 5
        self.data[start:end, start:end, start:end] = -1000 # Air
        
        self.vol_data = VolumeData(raw_data=self.data, spacing=(1.0, 1.0, 1.0))

    def test_pore_extraction(self):
        processor = PoreExtractionProcessor()
        # Threshold: > 0 is Solid, < 0 is Air. 
        # Processor finds "Pores" (Air). Default threshold check might depend on logic.
        # Logic says: solid_mask = data > threshold. 
        # If threshold is -300: 
        #   1000 > -300 -> True (Solid)
        #   -1000 > -300 -> False (Air)
        # So pores are where False.
        
        result = processor.process(self.vol_data, threshold=0)
        
        # Metadata should contain porosity
        # 10x10x10 = 1000 voxels. Total = 125000. 
        # Porosity = 1000/125000 = 0.8%
        
        self.assertIn("Porosity", result.metadata)
        self.assertIn("PoreCount", result.metadata)
        
        # Check actual values (allow some float tolerance)
        p_count = result.metadata["PoreCount"]
        # It should find 1 connected component
        self.assertEqual(int(p_count), 1)

    def test_pnm_generation_caching(self):
        processor = PoreToSphereProcessor()
        
        # First Run
        result1 = processor.process(self.vol_data, threshold=0)
        self.assertTrue(result1.has_mesh)
        
        # Second Run (Should hit cache)
        # We can mock print or define a callback to verify, but here we just ensure it runs and returns same result
        result2 = processor.process(self.vol_data, threshold=0)
        
        self.assertEqual(result1.metadata["PoreCount"], result2.metadata["PoreCount"])
        self.assertEqual(result1.mesh.n_points, result2.mesh.n_points)

if __name__ == '__main__':
    unittest.main()


--- FILE: .\tests\test_sim_annotation_evaluation.py ---
import numpy as np
import pytest

from core import VolumeData
from core.time_series import PNMSnapshot
from processors.pnm_tracker import PNMTracker


def _make_snapshot(regions: np.ndarray, time_index: int) -> PNMSnapshot:
    labels = np.unique(regions)
    labels = labels[labels > 0]
    centers = []
    radii = []
    volumes = []

    for label in labels:
        coords = np.argwhere(regions == label)
        volumes.append(float(len(coords)))
        centers.append(coords.mean(axis=0).astype(np.float64))
        r_vox = (3.0 * len(coords) / (4.0 * np.pi)) ** (1.0 / 3.0)
        radii.append(float(r_vox))

    return PNMSnapshot(
        time_index=time_index,
        pore_centers=np.asarray(centers, dtype=np.float64) if centers else np.zeros((0, 3), dtype=np.float64),
        pore_radii=np.asarray(radii, dtype=np.float64) if radii else np.zeros((0,), dtype=np.float64),
        pore_ids=np.asarray(labels, dtype=np.int32),
        pore_volumes=np.asarray(volumes, dtype=np.float64) if volumes else np.zeros((0,), dtype=np.float64),
        connections=[],
        segmented_regions=regions,
        spacing=(1.0, 1.0, 1.0),
        origin=(0.0, 0.0, 0.0),
        metadata={},
    )


def test_sim_annotation_evaluation_reports_tracking_accuracy(tmp_path):
    shape = (20, 20, 20)

    pred_t0 = np.zeros(shape, dtype=np.int32)
    pred_t0[2:6, 2:6, 2:6] = 1
    pred_t0[10:14, 10:14, 10:14] = 2

    pred_t1 = np.zeros(shape, dtype=np.int32)
    pred_t1[2:6, 2:6, 2:6] = 11
    pred_t1[10:14, 10:14, 10:14] = 22

    gt_t0 = np.zeros(shape, dtype=np.int16)
    gt_t0[2:6, 2:6, 2:6] = 101
    gt_t0[10:14, 10:14, 10:14] = 202

    gt_t1 = np.zeros(shape, dtype=np.int16)
    gt_t1[2:6, 2:6, 2:6] = 101
    gt_t1[10:14, 10:14, 10:14] = 202

    gt_path_t0 = tmp_path / "Step_00_labels.npy"
    gt_path_t1 = tmp_path / "Step_01_labels.npy"
    np.save(gt_path_t0, gt_t0)
    np.save(gt_path_t1, gt_t1)

    volumes = [
        VolumeData(
            raw_data=np.zeros(shape, dtype=np.int16),
            metadata={"sim_annotations": {"files": {"labels_npy": str(gt_path_t0)}}},
        ),
        VolumeData(
            raw_data=np.zeros(shape, dtype=np.int16),
            metadata={"sim_annotations": {"files": {"labels_npy": str(gt_path_t1)}}},
        ),
    ]

    tracker = PNMTracker(
        match_mode="temporal_global",
        assign_solver="scipy",
        use_gpu=False,
        use_batch=False,
        max_misses=0,
    )
    tracker.set_reference(_make_snapshot(pred_t0, time_index=0))
    tracker.track_snapshot(_make_snapshot(pred_t1, time_index=1))

    report = tracker.evaluate_against_sim_annotations(volumes, instance_iou_threshold=0.1)
    assert report["available"] is True
    assert report["status"] == "ok"
    assert report["overall"]["num_steps_evaluated"] == 2
    assert report["steps"][0]["instance"]["f1"] == pytest.approx(1.0)
    assert report["steps"][1]["tracking"]["accuracy"] == pytest.approx(1.0)
    per_ref = report["steps"][1]["tracking"]["per_reference"]
    assert per_ref[1]["outcome"] == "correct_active"
    assert per_ref[2]["outcome"] == "correct_active"
    assert per_ref[1]["correct"] is True
    assert tracker.get_results().tracking.evaluation["available"] is True


def test_sim_annotation_evaluation_handles_missing_label_path_gracefully():
    shape = (12, 12, 12)
    pred_t0 = np.zeros(shape, dtype=np.int32)
    pred_t0[2:5, 2:5, 2:5] = 1

    tracker = PNMTracker(
        match_mode="temporal_global",
        assign_solver="scipy",
        use_gpu=False,
        use_batch=False,
    )
    tracker.set_reference(_make_snapshot(pred_t0, time_index=0))

    volumes = [VolumeData(raw_data=np.zeros(shape, dtype=np.int16), metadata={})]
    report = tracker.evaluate_against_sim_annotations(volumes, instance_iou_threshold=0.1)

    assert report["available"] is False
    assert report["status"] == "unavailable"
    assert any("labels.npy path missing" in msg for msg in report["warnings"])


--- FILE: .\tests\test_tgga_tracking.py ---
import numpy as np
import pytest

from core.time_series import PNMSnapshot, PoreStatus
from processors.pnm_tracker import (
    HAS_LAPJV,
    HAS_SCIPY,
    PNMTracker,
    build_candidates,
    solve_global_assignment,
)
from processors.tracking_utils import estimate_macro_registration


def _make_snapshot(regions: np.ndarray, time_index: int = 0) -> PNMSnapshot:
    labels = np.unique(regions)
    labels = labels[labels > 0]
    centers = []
    radii = []
    volumes = []
    for label in labels:
        coords = np.argwhere(regions == label)
        volumes.append(float(len(coords)))
        centers.append(coords.mean(axis=0).astype(np.float64))
        r_vox = (3.0 * len(coords) / (4.0 * np.pi)) ** (1.0 / 3.0)
        radii.append(float(r_vox))

    if centers:
        centers_arr = np.asarray(centers, dtype=np.float64)
    else:
        centers_arr = np.zeros((0, 3), dtype=np.float64)
    radii_arr = np.asarray(radii, dtype=np.float64) if radii else np.zeros((0,), dtype=np.float64)
    volumes_arr = np.asarray(volumes, dtype=np.float64) if volumes else np.zeros((0,), dtype=np.float64)

    return PNMSnapshot(
        time_index=time_index,
        pore_centers=centers_arr,
        pore_radii=radii_arr,
        pore_ids=np.asarray(labels, dtype=np.int32),
        pore_volumes=volumes_arr,
        connections=[],
        segmented_regions=regions,
        spacing=(1.0, 1.0, 1.0),
        origin=(0.0, 0.0, 0.0),
        metadata={},
    )


def _make_reference_and_current():
    ref = np.zeros((24, 24, 24), dtype=np.int32)
    ref[2:6, 2:6, 2:6] = 1
    ref[14:18, 14:18, 14:18] = 2

    cur = np.zeros((24, 24, 24), dtype=np.int32)
    cur[3:7, 3:7, 3:7] = 11
    cur[13:17, 13:17, 13:17] = 22
    return _make_snapshot(ref, 0), _make_snapshot(cur, 1)


def _paint_cube(volume: np.ndarray, label: int, center_zyx: tuple[int, int, int], size: int) -> None:
    half = size // 2
    z, y, x = center_zyx
    volume[z - half:z - half + size, y - half:y - half + size, x - half:x - half + size] = label


def test_tgga_one_to_one_mapping():
    ref_snap, cur_snap = _make_reference_and_current()
    tracker = PNMTracker(
        match_mode="temporal_global",
        assign_solver="scipy",
        use_gpu=False,
        use_batch=False,
        max_misses=0,
    )
    tracker.set_reference(ref_snap)
    tracker.track_snapshot(cur_snap)

    mapping = tracker.get_results().tracking.id_mapping[1]
    matched_current = [cid for cid in mapping.values() if cid != -1]
    assert len(matched_current) == len(set(matched_current))


def test_candidate_gating_rejects_far_pairs():
    ref = np.zeros((20, 20, 20), dtype=np.int32)
    ref[2:6, 2:6, 2:6] = 1
    ref_snap = _make_snapshot(ref, 0)

    cur = np.zeros((20, 20, 20), dtype=np.int32)
    cur[14:18, 14:18, 14:18] = 9
    cur_snap = _make_snapshot(cur, 1)

    tracker = PNMTracker(match_mode="temporal_global", assign_solver="scipy", use_gpu=False, use_batch=False)
    tracker.set_reference(ref_snap)

    data = build_candidates(
        reference_snapshot=ref_snap,
        current_snapshot=cur_snap,
        reference_masks=tracker._reference_masks,
        current_regions=cur_snap.segmented_regions,
        predicted_centers={1: ref_snap.pore_centers[0]},
        cost_weights=(0.45, 0.30, 0.20, 0.05),
        gate_center_radius_factor=2.5,
        gate_volume_ratio_min=0.2,
        gate_volume_ratio_max=5.0,
        gate_iou_min=0.02,
    )
    assert data["row_geom_candidate"].shape[0] == 1
    assert not bool(data["row_geom_candidate"][0])
    assert np.all(data["cost_matrix"] >= 1e6)


@pytest.mark.skipif(not (HAS_LAPJV and HAS_SCIPY), reason="lapjv and scipy both required")
def test_solver_consistency_lapjv_vs_scipy():
    rng = np.random.default_rng(7)
    cost = rng.uniform(0.0, 1.0, size=(7, 9))

    m_lap, _ = solve_global_assignment(cost.copy(), assign_solver="lapjv")
    m_sci, _ = solve_global_assignment(cost.copy(), assign_solver="scipy")

    c_lap = sum(v for _, _, v in m_lap)
    c_sci = sum(v for _, _, v in m_sci)
    assert abs(c_lap - c_sci) < 1e-6


def test_legacy_greedy_basic_compatibility():
    ref_snap, cur_snap = _make_reference_and_current()
    tracker = PNMTracker(
        match_mode="legacy_greedy",
        use_gpu=False,
        use_batch=False,
    )
    tracker.set_reference(ref_snap)
    tracker.track_snapshot(cur_snap)

    tracking = tracker.get_results().tracking
    mapping = tracking.id_mapping[1]
    assert set(mapping.keys()) == {1, 2}
    assert tracking.status_history[1][-1] in {PoreStatus.ACTIVE, PoreStatus.COMPRESSED}
    assert tracking.status_history[2][-1] in {PoreStatus.ACTIVE, PoreStatus.COMPRESSED}


def test_extreme_compression_keeps_finite_cost_and_valid_assignment():
    ref = np.zeros((64, 64, 64), dtype=np.int32)
    ref[8:18, 8:18, 8:18] = 1  # 1000 voxels
    cur = np.zeros((64, 64, 64), dtype=np.int32)
    cur[30, 30, 30] = 9  # 1 voxel => ratio 0.001

    ref_snap = _make_snapshot(ref, 0)
    cur_snap = _make_snapshot(cur, 1)

    tracker = PNMTracker(match_mode="temporal_global", assign_solver="scipy", use_gpu=False, use_batch=False)
    tracker.set_reference(ref_snap)

    data = build_candidates(
        reference_snapshot=ref_snap,
        current_snapshot=cur_snap,
        reference_masks=tracker._reference_masks,
        current_regions=cur_snap.segmented_regions,
        predicted_centers={1: cur_snap.pore_centers[0]},
        cost_weights=(0.45, 0.30, 0.20, 0.05),
        gate_center_radius_factor=8.0,
        gate_volume_ratio_min=0.2,
        gate_volume_ratio_max=5.0,
        gate_iou_min=0.02,
    )
    metrics = data["pair_metrics"][(0, 0)]
    assert np.isfinite(metrics["cost"])
    assert metrics["cost"] < 1e6
    assert 0.0005 <= metrics["volume_ratio"] <= 0.0015

    tracker.track_snapshot(cur_snap)
    mapping = tracker.get_results().tracking.id_mapping[1]
    assert mapping[1] == 9


def test_macro_registration_recovers_large_translation():
    ref = np.zeros((128, 128, 128), dtype=np.int32)
    _paint_cube(ref, 1, (24, 24, 24), size=8)
    cur = np.zeros((128, 128, 128), dtype=np.int32)
    _paint_cube(cur, 7, (74, 74, 74), size=8)  # +50 voxels translation

    ref_snap = _make_snapshot(ref, 0)
    cur_snap = _make_snapshot(cur, 1)

    tracker = PNMTracker(
        match_mode="temporal_global",
        assign_solver="scipy",
        use_gpu=False,
        use_batch=False,
        max_misses=0,
    )
    tracker.set_reference(ref_snap)
    tracker.track_snapshot(cur_snap)

    mapping = tracker.get_results().tracking.id_mapping[1]
    assert mapping[1] == 7


def test_macro_registration_gpu_flag_falls_back_cleanly():
    ref = np.zeros((48, 48, 48), dtype=np.int32)
    _paint_cube(ref, 1, (12, 12, 12), size=6)
    cur = np.zeros((48, 48, 48), dtype=np.int32)
    _paint_cube(cur, 2, (20, 20, 20), size=6)

    result = estimate_macro_registration(
        reference_regions=ref,
        current_regions=cur,
        smoothing_sigma=1.5,
        upsample_factor=2,
        use_gpu=True,
        gpu_min_size_mb=0.0,
    )
    assert result.displacement.shape == (3,)
    assert np.linalg.norm(result.displacement) > 0.0


def test_kalman_predicts_nonlinear_motion_through_two_occluded_frames():
    shape = (80, 80, 80)

    t0 = np.zeros(shape, dtype=np.int32)
    _paint_cube(t0, 1, (30, 30, 20), size=5)
    t1 = np.zeros(shape, dtype=np.int32)
    _paint_cube(t1, 11, (30, 30, 23), size=5)
    t2 = np.zeros(shape, dtype=np.int32)
    _paint_cube(t2, 12, (30, 30, 27), size=5)
    t3 = np.zeros(shape, dtype=np.int32)  # occlusion
    t4 = np.zeros(shape, dtype=np.int32)  # occlusion

    tracker = PNMTracker(
        match_mode="temporal_global",
        assign_solver="scipy",
        use_gpu=False,
        use_batch=False,
        max_misses=5,
    )
    tracker.set_reference(_make_snapshot(t0, 0))
    tracker.track_snapshot(_make_snapshot(t1, 1))
    tracker.track_snapshot(_make_snapshot(t2, 2))
    tracker.track_snapshot(_make_snapshot(t3, 3))
    tracker.track_snapshot(_make_snapshot(t4, 4))

    tracking = tracker.get_results().tracking
    history = np.asarray(tracking.center_history[1], dtype=np.float64)
    pred_t3_x = float(history[3, 2])
    pred_t4_x = float(history[4, 2])

    # Quadratic-like target path: x = [20, 23, 27, 32, 38]
    assert abs(pred_t3_x - 32.0) <= 4.0
    assert abs(pred_t4_x - 38.0) <= 6.0
    assert tracking.id_mapping[3][1] == -1
    assert tracking.id_mapping[4][1] == -1


def test_pore_closure_event_is_labeled_closed_by_compression():
    shape = (48, 48, 48)

    t0 = np.zeros(shape, dtype=np.int32)
    t0[10:12, 10:12, 10:12] = 1  # tiny pore, volume=8
    t0[30:36, 30:36, 30:36] = 2  # stable large pore

    t1 = np.zeros(shape, dtype=np.int32)
    t1[11, 11, 11] = 11  # tiny pore shrinks near segmentation limit
    t1[30:36, 30:36, 30:36] = 22

    t2 = np.zeros(shape, dtype=np.int32)
    t2[30:36, 30:36, 30:36] = 33  # tiny pore disappears

    tracker = PNMTracker(
        match_mode="temporal_global",
        assign_solver="scipy",
        use_gpu=False,
        use_batch=False,
        max_misses=3,
    )
    tracker.set_reference(_make_snapshot(t0, 0))
    tracker.track_snapshot(_make_snapshot(t1, 1))
    tracker.track_snapshot(_make_snapshot(t2, 2))

    tracking = tracker.get_results().tracking
    assert tracking.unmatched_reason[1][-1] == "closed_by_compression"
    assert tracking.status_history[1][-1] == PoreStatus.COMPRESSED
    assert tracking.volume_history[1][-1] == 0.0


def test_kalman_brakes_after_consecutive_tracking_loss():
    shape = (80, 80, 80)

    t0 = np.zeros(shape, dtype=np.int32)
    _paint_cube(t0, 1, (30, 30, 20), size=5)
    t1 = np.zeros(shape, dtype=np.int32)
    _paint_cube(t1, 11, (30, 30, 24), size=5)
    t2 = np.zeros(shape, dtype=np.int32)
    _paint_cube(t2, 12, (30, 30, 29), size=5)

    tracker = PNMTracker(
        match_mode="temporal_global",
        assign_solver="scipy",
        use_gpu=False,
        use_batch=False,
        max_misses=10,
        gating_params={
            "kalman_brake_velocity_decay": 0.4,
            "kalman_brake_acceleration_decay": 0.1,
            "kalman_freeze_after_misses": 3,
        },
    )
    tracker.set_reference(_make_snapshot(t0, 0))
    tracker.track_snapshot(_make_snapshot(t1, 1))
    tracker.track_snapshot(_make_snapshot(t2, 2))
    tracker.track_snapshot(_make_snapshot(np.zeros(shape, dtype=np.int32), 3))
    tracker.track_snapshot(_make_snapshot(np.zeros(shape, dtype=np.int32), 4))
    tracker.track_snapshot(_make_snapshot(np.zeros(shape, dtype=np.int32), 5))
    tracker.track_snapshot(_make_snapshot(np.zeros(shape, dtype=np.int32), 6))

    history = np.asarray(tracker.get_results().tracking.center_history[1], dtype=np.float64)
    x = history[:, 2]
    d1 = abs(float(x[3] - x[2]))
    d2 = abs(float(x[4] - x[3]))
    d3 = abs(float(x[5] - x[4]))
    d4 = abs(float(x[6] - x[5]))

    assert d2 <= d1 + 1e-6
    assert d3 <= d2 + 1e-6
    assert d4 <= 1e-6


--- FILE: .\tests\test_timeseries_annotations.py ---
import json
from pathlib import Path

import numpy as np

from core import VolumeData
from loaders.time_series import TimeSeriesDicomLoader


class _DummyVolumeLoader:
    def __init__(self, shape_by_folder):
        self._shape_by_folder = dict(shape_by_folder)

    def load(self, folder_path, callback=None):
        folder_name = Path(folder_path).name
        shape = self._shape_by_folder[folder_name]
        return VolumeData(
            raw_data=np.zeros(shape, dtype=np.int16),
            spacing=(1.0, 1.0, 1.0),
            origin=(0.0, 0.0, 0.0),
            metadata={"Type": "Dummy"},
        )


def _write_step_assets(step_dir: Path, step_index: int, volume_shape, labels_shape=None):
    step_dir.mkdir(parents=True, exist_ok=True)
    (step_dir / "CT_0000.dcm").write_bytes(b"DICM")

    labels_shape = labels_shape or volume_shape
    labels = np.zeros(labels_shape, dtype=np.int16)
    labels[(0,) * len(labels_shape)] = 1
    np.save(step_dir / "labels.npy", labels)

    annotations = {
        "step_index": step_index,
        "compression_ratio": float(step_index) * 0.1,
        "voxel_size": 0.5,
        "volume_shape": list(volume_shape),
        "origin": [0.0, 0.0, 0.0],
        "num_voids": 1,
        "voids": [
            {
                "id": 1,
                "shape": "sphere",
                "center_mm": [1.0, 1.0, 1.0],
                "radius_mm": 0.8,
                "volume_mm3": 2.1,
                "bbox_voxel_min": [0, 0, 0],
                "bbox_voxel_max": [1, 1, 1],
                "center_voxel": [0.0, 0.0, 0.0],
            }
        ],
    }
    (step_dir / "annotations.json").write_text(json.dumps(annotations), encoding="utf-8")

    coco = {
        "images": [{"id": 1, "file_name": "slice_0000.png", "width": volume_shape[2], "height": volume_shape[1]}],
        "annotations": [
            {
                "id": 1,
                "image_id": 1,
                "category_id": 1,
                "bbox": [0, 0, 1, 1],
                "area": 1,
                "iscrowd": 0,
                "void_id": 1,
                "void_shape": "sphere",
            }
        ],
        "categories": [{"id": 1, "name": "void", "supercategory": "defect"}],
    }
    (step_dir / "coco_annotations.json").write_text(json.dumps(coco), encoding="utf-8")


def _write_summary(parent: Path, num_steps: int):
    summary = {
        "num_steps": num_steps,
        "config": {"total_compression": 0.2, "num_steps": max(0, num_steps - 1), "axis": "Z"},
        "steps": [
            {"step_index": i, "compression_ratio": i * 0.1, "num_voids": 1}
            for i in range(num_steps)
        ],
    }
    (parent / "annotations_summary.json").write_text(json.dumps(summary), encoding="utf-8")


def test_timeseries_loader_reads_sim_annotations(tmp_path):
    shape = (12, 10, 8)
    _write_step_assets(tmp_path / "Step_00", step_index=0, volume_shape=shape)
    _write_step_assets(tmp_path / "Step_01", step_index=1, volume_shape=shape)
    _write_summary(tmp_path, num_steps=2)

    loader = TimeSeriesDicomLoader(
        loader=_DummyVolumeLoader({"Step_00": shape, "Step_01": shape}),
        read_sim_annotations=True,
    )
    volumes = loader.load_series(str(tmp_path), sort_mode="alphabetical")

    assert len(volumes) == 2
    for i, volume in enumerate(volumes):
        assert "sim_annotations" in volume.metadata
        sim = volume.metadata["sim_annotations"]
        assert sim["annotations"]["step_index"] == i
        assert sim["labels"]["shape"] == list(shape)
        assert sim["validation"]["ok"] is True

        series = volume.metadata["sim_annotation_series"]
        assert series["summary"]["available"] is True
        assert series["validation"]["error_count"] == 0


def test_timeseries_loader_reports_label_shape_mismatch(tmp_path):
    shape = (10, 10, 10)
    _write_step_assets(tmp_path / "Step_00", step_index=0, volume_shape=shape, labels_shape=(6, 6, 6))
    _write_summary(tmp_path, num_steps=1)

    loader = TimeSeriesDicomLoader(
        loader=_DummyVolumeLoader({"Step_00": shape}),
        read_sim_annotations=True,
    )
    volumes = loader.load_series(str(tmp_path), sort_mode="alphabetical")

    sim = volumes[0].metadata["sim_annotations"]
    assert sim["validation"]["ok"] is False
    assert any("labels.npy shape" in msg for msg in sim["validation"]["errors"])
    assert volumes[0].metadata["sim_annotation_series"]["validation"]["error_count"] >= 1


--- FILE: .\tests\test_timeseries_cache.py ---
"""
Test time series PNM caching functionality.
"""

import numpy as np
from core import VolumeData
from core.time_series import PNMSnapshot, TimeSeriesPNM, PoreTrackingResult
from data import get_timeseries_pnm_cache, clear_timeseries_pnm_cache


def create_dummy_volume(shape=(50, 50, 50), folder_name="t0"):
    """Create a dummy volume for testing."""
    data = np.random.randint(-1000, 1000, shape, dtype=np.int16)
    volume = VolumeData()
    volume.raw_data = data
    volume.spacing = (1.0, 1.0, 1.0)
    volume.origin = (0.0, 0.0, 0.0)
    volume.metadata = {'folder_name': folder_name}
    return volume


def create_dummy_snapshot(time_index=0, num_pores=10):
    """Create a dummy PNM snapshot for testing."""
    pore_centers = np.random.rand(num_pores, 3) * 50
    pore_radii = np.random.rand(num_pores) * 2 + 1
    pore_ids = np.arange(1, num_pores + 1)
    pore_volumes = np.random.randint(100, 1000, num_pores)
    connections = [(i, i+1) for i in range(1, num_pores)]
    
    return PNMSnapshot(
        time_index=time_index,
        pore_centers=pore_centers,
        pore_radii=pore_radii,
        pore_ids=pore_ids,
        pore_volumes=pore_volumes,
        connections=connections,
        spacing=(1.0, 1.0, 1.0),
        origin=(0.0, 0.0, 0.0),
        metadata={'threshold': -300}
    )


def create_dummy_timeseries_pnm(num_timepoints=3, num_pores=10):
    """Create a dummy TimeSeriesPNM for testing."""
    ts_pnm = TimeSeriesPNM()
    
    # Create reference snapshot
    reference = create_dummy_snapshot(0, num_pores)
    ts_pnm.reference_snapshot = reference
    ts_pnm.snapshots.append(reference)
    
    # Create tracking result
    ts_pnm.tracking = PoreTrackingResult(
        reference_ids=reference.pore_ids.tolist()
    )
    
    # Initialize tracking data
    for pore_id, volume in zip(reference.pore_ids, reference.pore_volumes):
        ts_pnm.tracking.volume_history[int(pore_id)] = [float(volume)]
    
    # Add more timepoints
    for t in range(1, num_timepoints):
        snapshot = create_dummy_snapshot(t, num_pores)
        ts_pnm.snapshots.append(snapshot)
        
        # Update tracking
        for pore_id, volume in zip(snapshot.pore_ids, snapshot.pore_volumes):
            ts_pnm.tracking.volume_history[int(pore_id)].append(float(volume))
    
    return ts_pnm


def test_cache_store_and_retrieve():
    """Test storing and retrieving time series PNM from cache."""
    print("\n=== Test: Cache Store and Retrieve ===")
    
    # Clear cache first
    clear_timeseries_pnm_cache()
    
    # Create test data
    volumes = [create_dummy_volume(folder_name=f"t{i}") for i in range(3)]
    threshold = -300
    
    ts_pnm = create_dummy_timeseries_pnm(num_timepoints=3)
    reference_mesh = create_dummy_volume(folder_name="mesh")
    reference_snapshot = ts_pnm.reference_snapshot
    
    # Get cache and generate key
    cache = get_timeseries_pnm_cache()
    cache_key = cache.generate_key(volumes, threshold)
    
    print(f"Cache key: {cache_key}")
    
    # Store data
    cache.store(cache_key, ts_pnm, reference_mesh, reference_snapshot)
    print("鉁?Data stored to cache")
    
    # Verify cache exists
    assert cache.has_cache(cache_key), "Cache should exist after storing"
    print("鉁?Cache exists")
    
    # Retrieve data
    cached_data = cache.get(cache_key)
    assert cached_data is not None, "Should retrieve cached data"
    
    cached_ts_pnm, cached_mesh, cached_snapshot = cached_data
    print("鉁?Data retrieved from cache")
    
    # Verify data integrity
    assert cached_ts_pnm.num_timepoints == ts_pnm.num_timepoints
    assert cached_ts_pnm.num_reference_pores == ts_pnm.num_reference_pores
    print(f"鉁?Data integrity verified: {cached_ts_pnm.num_timepoints} timepoints, "
          f"{cached_ts_pnm.num_reference_pores} pores")
    
    print("\n鉁?Test passed!\n")


def test_cache_key_uniqueness():
    """Test that different configurations generate different keys."""
    print("\n=== Test: Cache Key Uniqueness ===")
    
    cache = get_timeseries_pnm_cache()
    
    # Same volumes, same threshold
    volumes1 = [create_dummy_volume(folder_name=f"t{i}") for i in range(3)]
    key1a = cache.generate_key(volumes1, -300)
    key1b = cache.generate_key(volumes1, -300)
    
    assert key1a == key1b, "Same volumes and threshold should generate same key"
    print(f"鉁?Same config generates same key: {key1a[:8]}...")
    
    # Same volumes, different threshold
    key2 = cache.generate_key(volumes1, -400)
    assert key1a != key2, "Different threshold should generate different key"
    print(f"鉁?Different threshold generates different key: {key2[:8]}...")
    
    # Different volumes, same threshold
    volumes2 = [create_dummy_volume(folder_name=f"t{i}") for i in range(3)]
    key3 = cache.generate_key(volumes2, -300)
    assert key1a != key3, "Different volumes should generate different key"
    print(f"鉁?Different volumes generate different key: {key3[:8]}...")
    
    print("\n鉁?Test passed!\n")


def test_cache_clear():
    """Test cache clearing functionality."""
    print("\n=== Test: Cache Clear ===")
    
    cache = get_timeseries_pnm_cache()
    clear_timeseries_pnm_cache()
    
    # Create and store test data
    volumes = [create_dummy_volume(folder_name=f"t{i}") for i in range(2)]
    cache_key = cache.generate_key(volumes, -300)
    
    ts_pnm = create_dummy_timeseries_pnm(num_timepoints=2)
    reference_mesh = create_dummy_volume()
    
    cache.store(cache_key, ts_pnm, reference_mesh, ts_pnm.reference_snapshot)
    print("鉁?Data stored")
    
    assert cache.has_cache(cache_key), "Cache should exist"
    
    # Clear specific cache
    cache.clear(cache_key)
    assert not cache.has_cache(cache_key), "Cache should be cleared"
    print("鉁?Specific cache cleared")
    
    # Store again and clear all
    cache.store(cache_key, ts_pnm, reference_mesh, ts_pnm.reference_snapshot)
    clear_timeseries_pnm_cache()
    assert not cache.has_cache(cache_key), "All cache should be cleared"
    print("鉁?All cache cleared")
    
    print("\n鉁?Test passed!\n")


if __name__ == '__main__':
    print("\n" + "="*60)
    print("Time Series PNM Cache Tests")
    print("="*60)
    
    try:
        test_cache_store_and_retrieve()
        test_cache_key_uniqueness()
        test_cache_clear()
        
        print("\n" + "="*60)
        print("鉁?All tests passed!")
        print("="*60 + "\n")
    except Exception as e:
        print(f"\n鉂?Test failed with error: {e}")
        import traceback
        traceback.print_exc()


